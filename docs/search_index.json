[["index.html", "Quality R analysis with large language models Chapter 1 Quality R analysis with large language models 1.1 Summary 1.2 About Chris 1.3 Course outline 1.4 Software you’ll need for this workshop 1.5 Software licenses 1.6 R packages you’ll need for this workshop 1.7 Data", " Quality R analysis with large language models CJ Brown (c.j.brown@utas.edu.au) 2025-05-18 Chapter 1 Quality R analysis with large language models 1.1 Summary If you are using R you are probably using language models (e.g. ChatGPT) to help you write code, but are you using them in the most effective way? Language models have different biases to humans and so make different types of errors. This 1-day workshop will cover how to use language models to learn R and conduct reliable environmental analyses. We will cover: Pros and cons of different tools from the simple interfaces like ChatGPT to advanced tools that can run and test code by themselves and keep going until the analysis is complete (and even written up). Best practice prompting techniques that can dramatically improve model performance for complex statistical applications Applying language models to common environmental applications such as GLMs, multivariate statistics and Bayesian statistics Copyright and ethical issues We’ll finish up with a discussion of what large language models mean for analysis and the scientific process. Requirements for interactive workshop: Laptop with R, Rstudio and VScode installed. Please see software instructions below. 1.1.0.1 Who should take this workshop? The workshop is for: anyone who currently uses R, from intermittent users to experienced professionals. The workshop is not suitable for those that need an introduction to R and I’ll assume students know at least what R does and are able to do tasks like read in data and create plots. 1.2 About Chris I’m an Associate Professor of Fisheries Science at University of Tasmania and an Australian Research Council Future Fellow. I specialise in data analysis and modelling, skills I use to better inform environmental decision makers. R takes me many places and I’ve worked with marine ecosystems from tuna fisheries to mangrove forests. I’m an experienced teacher of R. I have taught R to 100s people over the years, from the basics to sophisticated modelling and for everyone from undergraduates to my own supervisors. 1.3 Course outline Introduction to LLMs for R 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. Part 1 LLM fundamentals 10-10:30am Tea break: 10:30-10:45 Continue Part 1 LLM fundamentals Ethics and copyright 11:30-12:00 Lunch break: 12:00-1:00pm Part 2 Github copilot for R 1:00pm-2:45pm Tea break 2:45-3:00pm Part 3 Advanced LLM agents 3:00-3:30pm Conclusion and discussion of what it means for science 3:30pm-4:00pm 1.4 Software you’ll need for this workshop Save some time for set-up, there is a bit to it. You may also need ITs help if your computer is locked down. R of course, (4.2.2 or later) VScode It takes a bit of time and IT knowledge and setting up to connect VScode to R. So don’t leave that to the last minute See my installation instructions Once you have VScode, make sure you get these extensions (which can be found by clicking the four squares in the left hand menu bar): GitHub Copilot, GitHub Copilot Chat If you can’t get VScode to work with R you are still welcome to join. Most of the morning session can be done in Rstudio. In the afternoon you’ll need VScode if you want to try what I am teaching. 1.4.1 Optional software RStudio. You can do some of this workshop in Rstudio. But you’ll get more out of it if you use VScode. If you are using Rstudio, make sure you get a Github Copilot account and connect it to Rstudio. 1.4.1.1 Optional VScode extensions Web Search for Copilot (once installed, follow the instructions to set up your API key, I use Tavily because it has a free tier). Roo code extension for VScode. Markdown preview enhanced Let’s you view markdown files in a pane with cmd(cntrl)-shift-V Radian terminal I also recommend installing radian terminal. This makes your terminal for R much cleaner, has autocomplete and seems to help with some common issues. 1.5 Software licenses Github copilot Go to their page to sign-up. The free tier is fine. You can also get free Pro access as a student or professor (requires application). API key and account with LLM provider You will need API (application programming interface) access to an LLM to do all the examples in this workshop. This will allow us to interact with LLMs directly via R code. API access is on a pay per token basis. You will need to create an account with one of the below providers and then buy some credits (USD10 should be sufficient). Here are some popular choices: OpenRouter (recommended as gives you flexible access to lots of models) Anthropic OpenAI Once you have your API key, keep it secret. It’s a password. Be careful not to push it to aa github repo accidently. 1.6 R packages you’ll need for this workshop install.packages(c(\"vegan\", \"ellmer\",\"tidyverse\") INLA for Bayesian computation. Use the link, its not on cran. 1.7 Data Not yet complete… We’ll work with some benthic cover data, direct links to csv files are here: text files for 2 papers. time-series data? "],["introduction-to-llms-for-r.html", "Chapter 2 Introduction to LLMs for R", " Chapter 2 Introduction to LLMs for R Time: 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. This chapter provides an overview of: How Large Language Models (LLMs) function and their capabilities Best practices for prompt engineering when working with R Software options available for R users to interact with LLMs Practical applications of LLMs for R programming and data analysis Ethical considerations when using LLMs for scientific work We’ll explore how LLMs can enhance your R workflow, from code generation to data analysis assistance, while maintaining scientific rigor and reproducibility. "],["llm-prompting-fundamentals.html", "Chapter 3 LLM prompting fundamentals 3.1 Setup authorisation 3.2 Understanding how LLMs work 3.3 DIY stats bot 3.4 Advanced prompting", " Chapter 3 LLM prompting fundamentals Start of practical material We’ll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you’ll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you’re not getting the results you expect. Software requirements: VScode with R or Rstudio, ellmer package, API license. 3.1 Setup authorisation First, you need to get an API key from the provider. Login to the provider’s website and follow the instructions. Then, you need to add the key to your .Renviron file: usethis::edit_r_environ() Then type in your key like this: OPENROUTER_API_KEY=\"xxxxxx\" Then restart R. ellmer will automatically find your key so long as you use the recommended envirment variable names. See ?ellmer::chat_openrouter (or chat_xxx where xxx is whatever provider you are using). 3.2 Understanding how LLMs work Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll use the ellmer package to demonstrate some fundamental concepts. First, let’s set up our environment and create a connection to an LLM. library(ellmer) # Initialize a chat with Claude chat &lt;- chat_openrouter( system_prompt = &quot;&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Notice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in Let’s use the ‘system prompt’ to provide it with strong directions. Tip: The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, here’s the system prompt for the chat interface version of anthropic (Claude) chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, # model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Tip: It is generally more effective to tell the LLM what to do rather than what not to do (just like people!). 3.2.1 Token-by-token prediction LLMs don’t understand text as complete sentences or concepts; they predict one token at a time based on the patterns they’ve learned during training. A token is roughly equivalent to a word part, a word, or a common phrase. Let’s see this in action by generating text token by token with a very limited number of tokens: prompt &lt;- &quot;Ecologists like to eat&quot; for(i in 1:5) { single_token_chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 1) ) response &lt;- single_token_chat$chat(prompt) prompt &lt;- paste(prompt, response, sep = &quot;&quot;) cat(&quot;Token&quot;, i, &quot;:&quot;, response, &quot;\\n&quot;) } # Display the complete sequence cat(&quot;Full sequence:&quot;, prompt, &quot;\\n&quot;) This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context. 3.2.2 Temperature effects The “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable. Let’s compare responses with different temperatures: # Create chats with different temperature settings chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 0) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 2) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) At low temperatures, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent. 3.2.3 Comparing model complexity Different models have different capabilities based on their size, training data, and architecture. For example anthropic/claude-3.5-haiku has many fewer parameters than anthropic/claude-3.7-sonnet. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15 For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot. 3.2.4 Understanding context windows LLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million tokens. We’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window. 3.3 DIY stats bot UP TO HERE 3.4 Advanced prompting The point of this section is to show you how github copilot works. Also ellmer has a tools functionality that you can explore on your own. Keep working with our stats bot. Make a tool: Get it to output code that can be parsed (e.g. using markdown). Write an R function that parses and runs the code. Compare different models for accuracy at following the instructions. Show one shot and few shot examples. "],["ethics-and-copyright.html", "Chapter 4 Ethics and copyright 4.1 Model biases 4.2 Energy use 4.3 Copyright 4.4 Managing data privacy", " Chapter 4 Ethics and copyright This chapter addresses important ethical and legal considerations when using LLMs for R programming and data analysis: Managing data privacy (get it to write code for analysis but not show it the data, keeping data in a different directory). 4.1 Model biases Understanding inherent biases in LLMs Recognizing when biases might affect statistical analysis Techniques for mitigating bias in LLM-generated code and analysis 4.2 Energy use Environmental impact of LLM usage Balancing computational efficiency with analytical needs Strategies for reducing the carbon footprint of LLM-powered workflows 4.3 Copyright Legal considerations when using LLM-generated code Understanding the licensing implications of LLM outputs Best practices for attribution and documentation 4.4 Managing data privacy Strategies for using LLMs without exposing sensitive data Techniques for getting LLMs to write code for analysis without showing them the data Keeping data in separate directories and other practical approaches Considerations for research involving human subjects or proprietary information This discussion will help you develop responsible practices for incorporating LLMs into your scientific workflow while respecting ethical, legal, and privacy considerations. "],["github-copilot-for-r.html", "Chapter 5 Github copilot for R", " Chapter 5 Github copilot for R Time: 11:30-12:00pm I’ll show you how you can most effectively use github copilot to plan, code and write up your data analysis and modelling. Software requirements: VScode with R and github copilot license + extension for copilot. This chapter introduces GitHub Copilot as a powerful tool for R programmers: Overview of GitHub Copilot and how it differs from other LLM tools Setting up and configuring Copilot for optimal R programming assistance Understanding Copilot’s strengths and limitations for data analysis tasks Strategies for effectively collaborating with Copilot on R projects We’ll explore how Copilot can accelerate your R development process by: - Suggesting code completions based on context - Generating entire functions and code blocks - Helping with documentation and commenting - Assisting with debugging and error resolution Through practical demonstrations, you’ll learn how to leverage Copilot’s capabilities while maintaining control over your code’s quality and correctness, particularly for statistical analysis and data science applications. "],["best-practices-project-setup.html", "Chapter 6 Best practices project setup 6.1 Project organization 6.2 Version control with Git 6.3 R project setup", " Chapter 6 Best practices project setup How to be organized to maximise LLM effectiveness. Version control. This chapter focuses on establishing an optimal project structure and workflow to get the most out of LLMs in your R projects: 6.1 Project organization Directory structures that facilitate LLM assistance Documentation practices that improve LLM understanding of your code Naming conventions and code organization principles Creating effective README files and project documentation 6.2 Version control with Git Basic Git workflow for R projects How version control enhances collaboration with LLMs Using Git to track changes made with LLM assistance Best practices for committing LLM-generated code 6.3 R project setup Using RStudio projects or VSCode workspaces Setting up reproducible environments with renv or packrat Creating and maintaining package dependency documentation Organizing data, code, and output directories By implementing these best practices, you’ll create a development environment where LLMs can provide more accurate and contextually relevant assistance, leading to more efficient and reliable R programming workflows. "],["inline-code-editing.html", "Chapter 7 Inline code editing 7.1 1. Code completion 7.2 2. Inline code generation 7.3 Understanding inline suggestions 7.4 Guiding Copilot with comments 7.5 Common patterns for R programming 7.6 Troubleshooting and refinement", " Chapter 7 Inline code editing This chapter explores techniques for using GitHub Copilot’s inline code editing capabilities to enhance your R programming workflow: 7.1 1. Code completion This is only option supported in Rstudio (last time I checked). Example: Show how this works. How to get it moving on when its stuck in a loop. Show how to use comments to get it to work more effectively 7.2 2. Inline code generation cmd/cntrl-i . Show how to use this examples: selecting code to fix selecting code to explain Use of /fix, /document etc… as shortcuts to get it to do things. Note options for microphone, @ (shortcuts) and model choices 7.3 Understanding inline suggestions How Copilot generates inline code suggestions Interpreting and evaluating Copilot’s suggestions Accepting, modifying, or rejecting suggestions effectively Keyboard shortcuts and efficiency tips 7.4 Guiding Copilot with comments Writing effective comments to steer Copilot’s suggestions Using natural language to describe your coding intentions Documenting complex statistical procedures for better suggestions Balancing detail and brevity in comments 7.5 Common patterns for R programming Data manipulation with tidyverse Statistical modeling and analysis Data visualization with ggplot2 Working with specialized R packages 7.6 Troubleshooting and refinement Strategies when Copilot provides incorrect or suboptimal suggestions Iterative refinement of code with Copilot’s assistance Handling R-specific syntax and idioms Adapting to Copilot’s learning curve Through practical examples and hands-on exercises, you’ll learn to seamlessly integrate Copilot’s inline suggestions into your R coding process, significantly accelerating your development while maintaining code quality. "],["planning-your-project-with-ask-mode.html", "Chapter 8 Planning your project with Ask mode 8.1 Introduction to Ask mode 8.2 Project planning with Ask mode 8.3 Asking effective questions 8.4 Implementing Ask mode suggestions", " Chapter 8 Planning your project with Ask mode This chapter explores how to effectively use GitHub Copilot’s Ask mode to plan and structure your R projects: Show how to use this. Chain of thought prompting. Attaching references or code files Attaching data files 8.1 Introduction to Ask mode What is Ask mode and how it differs from inline suggestions Accessing and using Ask mode in VSCode Understanding the capabilities and limitations of Ask mode When to use Ask mode vs. other Copilot features 8.2 Project planning with Ask mode Breaking down complex data analysis tasks Creating project roadmaps and development plans Identifying necessary packages and dependencies Structuring your analysis workflow 8.3 Asking effective questions Formulating clear and specific questions Providing sufficient context for accurate responses Iterative questioning strategies Domain-specific questions for statistical analysis 8.4 Implementing Ask mode suggestions Translating high-level advice into concrete code Evaluating and adapting suggested approaches Combining Ask mode with inline coding Documentation based on Ask mode explanations Through practical examples, you’ll learn how to leverage Ask mode as a planning and problem-solving tool that can help you approach complex R programming tasks with greater clarity and structure. "],["creating-your-code-with-edit-mode.html", "Chapter 9 Creating your code with Edit mode 9.1 Understanding Edit mode 9.2 Code generation strategies 9.3 Code transformation techniques 9.4 Quality control and refinement", " Chapter 9 Creating your code with Edit mode This chapter explores how to leverage GitHub Copilot’s Edit mode to efficiently create and modify R code: 9.1 Understanding Edit mode What is Edit mode and how it differs from other Copilot features Accessing and using Edit mode in VSCode Edit mode’s capabilities for code generation and transformation When to use Edit mode in your R workflow 9.2 Code generation strategies Creating functions and code blocks from natural language descriptions Generating data manipulation pipelines Building statistical models and analysis scripts Creating data visualization code 9.3 Code transformation techniques Refactoring existing R code for better performance or readability Converting between base R and tidyverse approaches Translating code between different statistical frameworks Adapting code examples to your specific needs 9.4 Quality control and refinement Evaluating and testing generated code Iterative improvement through Edit mode Handling edge cases and error conditions Ensuring code readability and maintainability Through hands-on examples, you’ll learn to use Edit mode as a powerful tool for both creating new R code and transforming existing code, significantly accelerating your development process while maintaining high-quality, reliable code. "],["automated-workflows-with-agent-mode.html", "Chapter 10 Automated workflows with Agent mode 10.1 Introduction to Agent mode 10.2 Creating automated workflows 10.3 Common R workflow applications 10.4 Troubleshooting and optimization", " Chapter 10 Automated workflows with Agent mode This chapter explores how to use GitHub Copilot’s Agent mode to create automated workflows for R programming and data analysis: 10.1 Introduction to Agent mode What is Agent mode and how it differs from other Copilot features Accessing and using Agent mode in VSCode Understanding the capabilities and limitations of Agent mode When to use Agent mode in your R workflow 10.2 Creating automated workflows Defining tasks for Agent mode to accomplish Breaking down complex analyses into manageable steps Setting up appropriate contexts and environments Monitoring and guiding Agent mode’s progress 10.3 Common R workflow applications Data cleaning and preprocessing Exploratory data analysis Statistical modeling and hypothesis testing Report generation and visualization 10.4 Troubleshooting and optimization Handling errors and unexpected results Providing additional context when needed Refining workflow definitions for better results Balancing automation with human oversight Through practical demonstrations, you’ll learn how to leverage Agent mode to automate repetitive or complex R programming tasks, allowing you to focus on higher-level analysis and interpretation while maintaining control over the quality and correctness of your code. "],["advanced-llm-agents.html", "Chapter 11 Advanced LLM agents 11.1 Roo code 11.2 Introduction to Roo code 11.3 Autonomous R programming with Roo 11.4 Advanced use cases for R 11.5 Best practices and workflow integration 11.6 Evolution of LLM agents 11.7 Agent capabilities for R 11.8 Integration with R workflows 11.9 Practical considerations", " Chapter 11 Advanced LLM agents Time: 3:00-3:30pm Software requirements: VScode with R, Roo code, API license. This chapter introduces advanced LLM agents that go beyond GitHub Copilot’s capabilities: 11.1 Roo code Fully automated workflows. This chapter explores Roo code as a powerful tool for creating fully automated workflows in R: Show roo code example. Talk through token usage, system prompt, editing prompts and roles, model choice, context window Image tool 11.2 Introduction to Roo code What is Roo code and how it differs from other LLM tools Setting up and configuring Roo code in VSCode Understanding Roo’s capabilities and limitations The architecture behind Roo’s automation capabilities 11.3 Autonomous R programming with Roo How Roo interprets task descriptions and requirements Roo’s approach to generating, testing, and refining code Monitoring and interacting with Roo during code generation Evaluating and validating Roo-generated solutions 11.4 Advanced use cases for R End-to-end data analysis pipelines Complex statistical modeling and inference Custom visualization development Package development and documentation 11.5 Best practices and workflow integration Providing effective task descriptions Balancing autonomy with guidance Incorporating Roo into existing development workflows Version control and collaboration with Roo-generated code Through practical demonstrations, you’ll learn how to leverage Roo code to automate sophisticated R programming tasks, potentially saving hours of development time while maintaining high-quality, reliable code. 11.6 Evolution of LLM agents How LLM agents differ from simple code completion tools The spectrum of automation in LLM-assisted programming Understanding agent architectures and capabilities Current state-of-the-art in LLM agents for R programming 11.7 Agent capabilities for R Code generation and modification Reasoning about complex statistical problems Autonomous debugging and error correction Documentation and explanation generation 11.8 Integration with R workflows Connecting agents to your development environment API-based interactions with LLM agents Batch processing vs. interactive assistance Combining multiple agents for complex tasks 11.9 Practical considerations Computational requirements and performance Cost structures and optimization strategies Privacy and security implications Evaluating agent output quality This chapter provides a foundation for understanding more sophisticated LLM agents, preparing you for the next chapter’s focus on Roo code as a specific implementation of advanced agent technology for R programming. "],["cost-and-security.html", "Chapter 12 Cost and security 12.1 Cost considerations 12.2 Security implications 12.3 Best practices for secure usage 12.4 Institutional considerations", " Chapter 12 Cost and security This chapter addresses important practical considerations when using LLMs for R programming: 12.1 Cost considerations Understanding the pricing models of different LLM providers API costs vs. subscription models Strategies for optimizing token usage Balancing cost with capability requirements Budgeting for LLM usage in research and professional contexts 12.2 Security implications Data privacy concerns when using LLMs Understanding what data is sent to LLM providers Risks of exposing sensitive or proprietary information Compliance considerations for different industries and research contexts 12.3 Best practices for secure usage Managing API keys and credentials Sanitizing inputs to remove sensitive information Local vs. cloud-based LLM solutions Auditing and monitoring LLM interactions 12.4 Institutional considerations Developing organizational policies for LLM usage Training researchers and staff on secure practices Documentation and transparency requirements Balancing innovation with risk management This chapter provides practical guidance for managing the financial and security aspects of incorporating LLMs into your R workflow, helping you make informed decisions about when and how to use these powerful tools. "],["conclusion.html", "Chapter 13 Conclusion 13.1 Workshop summary 13.2 The changing landscape of scientific computing 13.3 Developing community standards 13.4 Future directions", " Chapter 13 Conclusion Time: 3:30pm-4:00pm We’ll discuss as a group what LLMs mean for the way we do science, and creating community standards. This chapter synthesizes the key insights from the workshop and explores the broader implications of LLMs for scientific practice: 13.1 Workshop summary Recap of key concepts and techniques covered Integration of different LLM tools and approaches Progression from basic prompting to advanced autonomous agents Practical applications for R programming and data analysis 13.2 The changing landscape of scientific computing How LLMs are transforming research workflows Potential impacts on reproducibility and transparency Changes in skill requirements and education Democratization of advanced programming capabilities 13.3 Developing community standards Ethical considerations for LLM use in scientific research Documentation and reporting practices Peer review in the age of LLM-assisted research Balancing innovation with methodological rigor 13.4 Future directions Emerging trends in LLM technology Potential developments in R-specific LLM tools Opportunities for community contribution and development Preparing for the next generation of AI-assisted data science This concluding discussion encourages critical reflection on how we can harness the power of LLMs while maintaining the integrity and quality of scientific research and analysis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
