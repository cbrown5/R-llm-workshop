[["index.html", "Quality R analysis with large language models Chapter 1 Quality R analysis with large language models 1.1 Summary 1.2 About Chris 1.3 Citation for course 1.4 Course outline 1.5 Software you’ll need for this workshop 1.6 Software licenses 1.7 R packages you’ll need for this workshop 1.8 Data", " Quality R analysis with large language models CJ Brown (c.j.brown@utas.edu.au) 2025-05-28 Chapter 1 Quality R analysis with large language models 1.1 Summary If you are using R you are probably using language model assistants (e.g. ChatGPT) to help you write code, but are you using them in the most effective way? Language models have different biases to humans and so make different types of errors. This 1-day workshop will cover how to use language models to learn R and conduct reliable environmental analyses. We will cover: Pros and cons of different assistants from the interfaces like ChatGPT to advanced coding assistants that can run and test code by themselves and keep going until the analysis is complete (and even written up). Best practice prompting techniques that can dramatically improve LLM performance for complex statistical applications Applying language models to common environmental applications such as GLMs, multivariate statistics and Bayesian statistics Copyright and ethical issues We’ll finish up with a discussion of what large language models mean for analysis and the scientific process. Requirements for interactive workshop: Laptop with R, Rstudio and VScode installed. Please see software instructions below. 1.1.0.1 Who should take this workshop? The workshop is for: anyone who currently uses R, from intermittent users to experienced professionals. The workshop is not suitable for those that need an introduction to R and I’ll assume students know at least what R does and are able to do tasks like read in data and create plots. 1.2 About Chris I’m an Associate Professor of Fisheries Science at University of Tasmania and an Australian Research Council Future Fellow. I specialise in data analysis and modelling, skills I use to better inform environmental decision makers. R takes me many places and I’ve worked with marine ecosystems from tuna fisheries to mangrove forests. I’m an experienced teacher of R. I have taught R to 100s people over the years, from the basics to sophisticated modelling and for everyone from undergraduates to my own supervisors. 1.3 Citation for course If following the prompting advice please consider citing my accompanying article: Citation: Brown (2025). Quality statistics with large language models. Unpublished manuscript To be submitted soon (and updated here) 1.4 Course outline Introduction to LLMs for R 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. Part 1 LLM fundamentals 10-10:30am Tea break: 10:30-10:45 Continue Part 1 LLM fundamentals 10:45-11:30am Part 2 Github copilot for R 11:30-12:00pm Lunch break: 12:00-1:00pm Continue part 2 Github copilot for R 1:00-2:15pm Ethics and copyright 2:15-2:45pm Tea break 2:45-3:00pm Part 3 Advanced coding assistants 3:00-3:30pm Conclusion and discussion of what it means for science 3:30-4:00pm 1.5 Software you’ll need for this workshop Save some time for set-up, there is a bit to it. You may also need ITs help if your computer is locked down. R of course, (4.2.2 or later) VScode It takes a bit of time and IT knowledge and setting up to connect VScode to R. So don’t leave that to the last minute See my installation instructions Once you have VScode, make sure you get these extensions (which can be found by clicking the four squares in the left hand menu bar): GitHub Copilot, GitHub Copilot Chat If you can’t get VScode to work with R you are still welcome to join. Most of the morning session can be done in Rstudio. In the afternoon you’ll need VScode if you want to try what I am teaching. 1.5.1 Optional software RStudio. You can do some of this workshop in Rstudio. But you’ll get more out of it if you use VScode. If you are using Rstudio, make sure you get a Github Copilot account and connect it to Rstudio. 1.5.1.1 Optional VScode extensions Web Search for Copilot (once installed, follow the instructions to set up your API key, I use Tavily because it has a free tier). Roo code extension for VScode. Markdown preview enhanced Let’s you view markdown files in a pane with cmd(cntrl)-shift-V Radian terminal I also recommend installing radian terminal. This makes your terminal for R much cleaner, has autocomplete and seems to help with some common issues. 1.6 Software licenses Github copilot Go to their page to sign-up. The free tier is fine. You can also get free Pro access as a student or professor (requires application). API key and account with LLM provider You will need API (application programming interface) access to an LLM to do all the examples in this workshop. This will allow us to interact with LLMs directly via R code. API access is on a pay per token basis. You will need to create an account with one of the below providers and then buy some credits (USD10 should be sufficient). Here are some popular choices: OpenRouter (recommended as gives you flexible access to lots of models) Anthropic OpenAI Once you have your API key, keep it secret. It’s a password. Be careful not to push it to aa github repo accidently. 1.7 R packages you’ll need for this workshop install.packages(c(\"vegan\", \"ellmer\",\"tidyverse\") INLA for Bayesian computation. Use the link, its not on cran. 1.8 Data We’ll load all data files directly via URL in the workshop notes. So no need to download any data now. Details on data attribution are below. 1.8.1 Benthic cover surveys and fish habitat In this course we’ll be analyzing benthic cover and fish survey data. These data were collected by divers doing standardized surveys on the reefs of Kia, Solomon Islands. These data were first publshed by Hamilton et al. 2017 who showed that logging of forests is causing sedimentation and impact habitats of an important fishery species. In a follow-up study Brown and Hamilton 2018 developed a Bayesian model that estimates the size of the footprint of pollution from logging on reefs. 1.8.2 Rock lobster time-series We’ll analyse time-series data from the Australian Temperate Reef Collaboration and the Reef Life Survey. Original data are available here, and you should access the originals for any research. This data is collected by divers every year who swim standardized transects and count reef fauna. We’ll use a simplified version I created to go with this course. The link is in the notes when we need it. Citation to the dataset. The analysis in the notes is inspired by this study looking at how we forecast reef species change. "],["introduction-to-llms-for-r.html", "Chapter 2 Introduction to LLMs for R", " Chapter 2 Introduction to LLMs for R Time: 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. This chapter provides an overview of: How Large Language Models (LLMs) function and their capabilities Best practices for prompt engineering when working with R Software options available for R users to interact with LLMs (coding assistants) Practical applications of LLMs for R programming and data analysis Ethical considerations when using LLMs for scientific work We’ll explore how LLMs can enhance your R workflow, from code generation to data analysis assistance, while maintaining scientific rigor and reproducibility. "],["llm-prompting-fundamentals.html", "Chapter 3 LLM prompting fundamentals 3.1 Setup authorisation 3.2 Understanding how LLMs work 3.3 DIY stats bot 3.4 Advanced prompting 3.5 Reflection on prompting fundamentals", " Chapter 3 LLM prompting fundamentals Start of practical material We’ll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you’ll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you’re not getting the results you expect. Software requirements: VScode with R or Rstudio, ellmer package, API license. 3.1 Setup authorisation First, you need to get an API key from the provider. Login to the provider’s website and follow the instructions. Then, you need to add the key to your .Renviron file: usethis::edit_r_environ() Then type in your key like this: OPENROUTER_API_KEY=\"xxxxxx\" Then restart R. ellmer will automatically find your key so long as you use the recommended envirment variable names. See ?ellmer::chat_openrouter (or chat_xxx where xxx is whatever provider you are using). 3.2 Understanding how LLMs work Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll use the ellmer package to demonstrate some fundamental concepts. By using ellmer to access a LLM through the API we get as close to the raw LLM as we are able. Later on we will use ‘coding assistants’ (e.g. copilot) which put another layer of software between you and the LLM. First, let’s set up our environment and create a connection to an LLM. library(ellmer) # Initialize a chat with Claude chat &lt;- chat_openrouter( system_prompt = &quot;&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Notice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in Let’s use the ‘system prompt’ to provide it with strong directions. Tip: The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, here’s the system prompt for the chat interface version of anthropic (Claude) chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, # model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Tip: It is generally more effective to tell the LLM what to do rather than what not to do (just like people!). 3.2.1 Token-by-token prediction LLMs don’t understand text as complete sentences or concepts; they predict one token at a time based on the patterns they’ve learned during training. A token is roughly equivalent to a word part, a word, or a common phrase. Let’s see this in action by generating text token by token with a very limited number of tokens: prompt &lt;- &quot;Ecologists like to eat&quot; for(i in 1:5) { single_token_chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 1) ) response &lt;- single_token_chat$chat(prompt) prompt &lt;- paste(prompt, response, sep = &quot;&quot;) cat(&quot;Token&quot;, i, &quot;:&quot;, response, &quot;\\n&quot;) } # Display the complete sequence cat(&quot;Full sequence:&quot;, prompt, &quot;\\n&quot;) This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context. 3.2.2 Temperature effects The “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable. Let’s compare responses with different temperatures: # Create chats with different temperature settings chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 0) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 2) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) At low temperatures, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent. 3.2.3 Comparing model complexity Different models have different capabilities based on their size, training data, and architecture. For example anthropic/claude-3.5-haiku has many fewer parameters than anthropic/claude-3.7-sonnet. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15 For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot. 3.2.4 Understanding context windows LLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million tokens. We’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window. 3.3 DIY stats bot Let’s put together what we’ve learnt so far and built our own chatbot. I’ve provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session. stats_bot &lt;- readr::read_file(url(&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/DIY-stats-bot-system.md&quot;)) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 5000) ) ellmer has a few different options for interacting with chat bots. We’ve seen the ‘chat’ option. We can also have a live_console() or live_browser() (requires installing shinychat) chat. Let’s use one of those options. With live_browser() you’ll also see the browser automatically formats any markdown in the chat. live_browser(chat_stats) # live_console(chat_stats) Here are some suggested questions to start, but feel free to try your own. “Who are you?” “Use stats mode to provide me with some suggestions for how I could make a predictive model of a variable y, where I have a large number of potential explanatory variables.” Tip: How many of you started using “DIY-stats-bot-system.md” without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We’ll see later that LLMs can be given ‘tools’ which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We’ll cover security later. 3.3.1 Improving the stats bot Make a local copy of the stats bot system prompt and try editing it. Try different commands within it and see how your chat bot responds (you’ll have to open a new chat object each time). Here’s some ideas. Try making a chat bot that is a verhment Bayesian that abhors frequentist statistics. You could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models. Try different temperatures. Add your own easter egg. Tip: Adjectives, CAPITALS, *markdown* formatting can all help create emphasis so that your model more closely follows your commands. I used ‘abhors’ and ‘verhment’ above on purpose. 3.4 Advanced prompting LLMs can be given access to tools (github copilot is an example). Tools give the LLMs the ability to use tools to perform tasks on your computer. For example, copilot can edit files. The next step beyond tool use is to create agents. Agents write commands then also run those commands and feed the results back to themselves. Agents are therefore autonomous coders. Tip: LLM “Tools” give the LLM assistant the ability to take actions on your computer. There are a huge range of tools out there, including tools that write files, edit files, perform web searchers and download data. “Agents” are LLMs that return the results of tool use back to themselves. Therefore they can code and improve that code without intervention from humans (though intervention is often needed). We can turn out chatbot into a simple agent. This will help you understand how agents work. What we will do is give a command to format all R code in a specific way. Then we can have an R function that checks every new response for R code, if it detects R code it will write this to a file. 3.4.1 Zero-shot First make a new system message as a .md file, e.g. here: “resources/stats-bot-simple.md” in your project directory. Add to this message: You are an expert in statistial analysis with the R program. Write your response with R code in &lt;code&gt;&lt;/code&gt; tags. (You can also use our chat bot from before, but it might be simpler to make this shorter one) Now give the system message a test. stats_bot_simple &lt;- readr::read_file(&quot;resources/stats-bot-simple.md&quot;) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 400) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data, code to show how to do the regression and summary plots. &quot;) response If it works the response should look something like this: &lt;code&gt; dat &lt;- data.frame(temperature = rnorm(100, mean = 20)) dat$fish_size &lt;- 10 + temperature*2 + rnorm(100) model &lt;- lm(fish_size ~ temperature) &lt;/code&gt; It didn’t work for me when I tried! It kept returning the R code in markdown formatting in inconsistent ways. We want a consistent response so we can parse it easily. Anyway, repeat that a few times to see if it consistently formats the code as we requested. You can also try using “anthropic/claude-3.5-haiku” as your model (its cheaper) to see if that is smart enough to consistently format the code. Tip: What we just tried is called ‘zero-shot prompting’. Zero-shot means we didn’t give the LLM any examples to follow, we assume it knows what to do. 3.4.2 One-shot and multi-shot prompts Giving an example (one-shot) or multiple examples (multi-shot) can improve the precision of the LLMs responses. Let’s update the system prompt to see if we get more reliable answers with some examples: You are an expert in statistial analysis with the R program. Write your response with R code in &lt;code&gt;&lt;/code&gt; tags. ### Examples of formatting R code Single line example &lt;code&gt; dat &lt;- read.csv(&quot;myfile.csv&quot;) &lt;/code&gt; Multiple line example &lt;code&gt; dat &lt;- read.csv(&quot;myfile.csv&quot;) ggplot(dat) + aes(x = x, y = y) + geom_point() &lt;/code&gt; I saved this updated system to a file: “resources/stats-bot-simple-multiple-shot.md”. Now test it out. stats_bot_simple &lt;- readr::read_file(&quot;resources/stats-bot-simple-multiple-shot.md&quot;) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot_simple, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 1000) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;) response That worked for me. Hopefully it works for you too? Test that to see if it gives more reliable examples. We have to be a bit careful with choosing examples as these are for formatting, not for modelling. For instance, we don’t want to inadventantly suggest the LLM always recommend a particular statistical model type. ’ 3.4.3 Making a simple tool Tools are algorithms your assistant can use to perform tasks on your computer or a server. They are what turns a chatbot that can only respond in text into an Assistant that can write code that takes actions. Tip: You might also see the term ‘MCP’ (Model Context Protocol) used in association with LLM assistants. MCP is an interoperable standard for developing tools. This means a tool I write is consistent with other people’s tools. This helps developers and LLMs read and interpret tools. It is also important to allow LLM to use multiple tools at the same time. ’ If the request to use tags works we can then parse the responses and save them as a file, ready to run. First create an R function that can parse responses and search for tags (you can cut and paste my function or try using copilot to make one for you, it’s good at this type of tedious task): write_response &lt;- function(response, filename = &quot;rscript.R&quot;){ # Extract the text between &lt;code&gt; and &lt;/code&gt; tags using regex # Using dotall mode (?s) to match across multiple lines code_pattern &lt;- &quot;(?s)&lt;code&gt;(.*?)&lt;/code&gt;&quot; code_matches &lt;- regmatches(response, gregexpr(code_pattern, response, perl = TRUE)) # Flatten the list and remove the tags if(length(code_matches) &gt; 0 &amp;&amp; length(code_matches[[1]]) &gt; 0) { code_texts &lt;- gsub(&quot;&lt;code&gt;|&lt;/code&gt;&quot;, &quot;&quot;, code_matches[[1]]) # Write all extracted code to the file writeLines(code_texts, filename) cat(&quot;Code written to&quot;, filename, &quot;\\n&quot;) } else { warning(&quot;No code tags found in the response&quot;) } } Now we can set-up our R code to write any suggestions to file: chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 400) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;) write_response(response, filename = &quot;attempt1.R&quot;) Now check the file we just created. If it looks ok then try running it: #CHECK THE FILE before running!!!! source(&quot;attempt1.R&quot;) The source() command will attempt to run the R code that was created. If it works you should see some plots pop up. 3.4.4 Summary of agents and tools We’ve implemented a rudimentary tool and the beginnings of an LLM agent. Going forwards we’ll used specialised software for these tasks. You won’t need to code them yourself. But coding this examples is helpful to understand the fundamentals of how they work. To recap, the basic workflow an agent follows is: Set-up a system prompt with detailed instructions for how the LLM should format responses User asks a question that is sent to the LLM LLM responds and sends response back to user Software on user’s computer attempts to parse and act on the response according to pre-determined rules User’s computers enacts the commands in the response and provides results to user Tools like Copilot Agent mode then go a step further and send the results of step 5 back to the LLM, which then interprets the results and the loop continues (sometimes with and sometimes without direct user approval). If you want to go further with making your own tools, then I suggest you check out ellmer package. It supports tool creation in a structured way. For instance, I made a tool that allows an LLM to download and save ocean data to your computer. 3.5 Reflection on prompting fundamentals The key things I hoped you learnt from this lesson are: Basic LLM jargon, including tokens, temperature, API access and different LLM models. Some different prompt strategies, including role prompting, emphasis, chain of thought and one-shot. The fundamentals of tool use and agents. Now you understand the basics, let’s get into Github Copilot. "],["github-copilot-for-r.html", "Chapter 4 Github copilot for R", " Chapter 4 Github copilot for R Time: 11:30-12:00pm I’ll show you how you can most effectively use github copilot to plan, code and write up your data analysis and modelling. Software requirements: VScode with R and github copilot license + extension for copilot. Github Copilot calls itself an ‘AI programming assistant’ or an ‘AI pair programmer’. I’ll refer to it as an ‘LLM coding assistant’ or just ‘Assistant’. Assistants add a layer of software between you and the LLM. The software is doing some hidden interpretation of what you want to do, as well as trying to save costs. For instance, for most assistants we often don’t get to control (or even see) the system message, the temperature or the number of output tokens. The assistant is also guessing context to include in the prompt, so it can automatically give the LLM more context. At the same time it is managing the LLM’s context window and trying to save on costs. There is no generic name for this type of software (the field is moving to fast to have standardized names). So I’ll refer to them Assistants. In this bucket I’ll also put chatGPT, Claude, Roo Code, Cline and others. Note that Github Copilot (which I’ll call copilot for short) is different to the ‘Copilot’ assistant that is on the web and in the Teams app. This software is also called ‘chatbots’, however, I prefer assistants as the tasks they can do are much broader than just chatting. Tip: You’ll get the most of out Github Copilot if you use Visual Studio Code as your development environment (rather than RStudio). Setting up VScode with R can be a bit fiddly, check out my my installation instructions if you have trouble. Web searching advice is also a good idea if you are stuck. Its worth the effort. Copilot It is developing rapidly, so it is quite likely that when you read this there will be changes and new features. In this section I’ll focus on showing the main ways you can use copilot. Just be aware the implementation may change in future. We’ll look at: Overview VScode for those that are new to this software Best practices for setting up your project directory Inline code editing Ask mode Edit mode Agent mode "],["best-practices-project-setup.html", "Chapter 5 Best practices project setup 5.1 Project organization 5.2 The README.md file 5.3 Example data", " Chapter 5 Best practices project setup This chapter focuses on establishing a project structure and workflow that can get the most out of your LLM assistants. 5.1 Project organization Its helpful to set-up your projects in an organized and modularised way. In my experience most R users write most of their analysis in one long script. Don’t do this. It will be hard for ‘future you’ to navigate. If its hard for a human to navigate, it will also be hard for the assistant. Here’s how I set-up my projects. 5.1.1 General guidance Create a new folder for each new project. Optional but recommended: Initiliaze a git repo in that folder (I use github desktop). Set-up folders and files in an organized way Ideally put the data in this folder also. However, large datasets or sensitive data can be kept in other folders. Keep scripts short and modularized (e.g one for data analysis, one for modelling). Once you have your folder you can make it an Rstudio project (if using Rstudio) or just use ‘open folder’ in vscode. If want to link multiple folders in then use VScode workspaces. If you are not using git (version control), then I recommend you learn. LLM code editing tools can cause you to lose older versions. So best to back them up with proper use of git. 5.1.2 Project directory structure example Here’s an example of a project directory structure. You don’t have to use this strucutre. the important thing is to be organized. my-project/ ├── README.md ├── .gitignore ├── Scripts/ # R code │ ├── 01_data-prep.R │ ├── 02_data-analysis.R │ └── 03_plots.R ├── Shared/ │ ├── Outputs/ │ │ ├── Figures/ │ │ ├── data-prep/ │ │ └── model-objects/ │ ├── Data/ │ └── Manuscripts/ └── Private/ 5.2 The README.md file The README.md is the memory for the project. If you use github it will also be the landing page for your repo, which is handy. Remember you are writing this for you and the LLMs. So think of it like a prompt. Here’s an example of some of the information you might want to include in your readme. # PROJECT TITLE ## Summary ## Aims ## Data methodology ## Tech context - We will use the R program - tidyverse packages for data manipulation - ggplot2 for data visualization Keep your scripts short and modular to facilitate debugging. Don&#39;t complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets. ## Steps As you go tick of the steps below. [ ] Wrangle data [ ] Fit regression [ ] Plot verification [ ] ... ## Data Include meta-data here and file paths. ## Directory structure my-project/ ├── README.md ├── .gitignore ├── Scripts/ # R code │ ├── 01_data-prep.R │ ├── 02_data-analysis.R │ └── 03_plots.R ├── Shared/ │ ├── Outputs/ │ │ ├── Figures/ │ │ ├── data-prep/ │ │ └── model-objects/ │ ├── Data/ │ └── Manuscripts/ └── Private/ 5.3 Example data For the next few chapters we’ll work with some ecological data on benthic marine habitats and fish. 5.3.1 Case-study: Bumphead parrotfish, ‘Topa’ in Solomon Islands Bumphead parrotfish (Bolbometopon muricatum) are an enignmatic tropical fish species. Adults of these species are characterized by a large bump on their forehead that males use to display and fight during breeding. Sex determination for this species is unknown, but it is likely that an individual has the potential to develop into either a male or female at maturity. Adults travel in schools and consume algae by biting off chunks of coral and in the process they literally poo out clean sand. Because of their large size, schooling habit and late age at maturity they are susceptible to overfishing, and many populations are in decline. Their lifecycle is characterized by migration from lagoonal reef as juveniles (see image below) to reef flat and exposed reef habitats as adults. Early stage juveniles are carnivorous and feed on zooplankton, and then transform into herbivores at a young age. Image: Lifecycle of bumphead parrotfish. Image by E. Stump and sourced from Hamilton et al. 2017. Until the mid 2010s the habitat for settling postlarvae and juveniles was a mystery. However, the pattern of migrating from inshore to offshore over their known lifecycle suggests that the earliest benthic lifestages (‘recruits’) stages may occur on nearshore reef habitats. Nearshore reef habitats are susceptible to degradation from poor water quality, raising concerns that this species may also be in decline because of pollution. But the gap in data from the earliest lifestages hinders further exploration of this issue. In this course we’ll be analyzing the first survey that revealed the habitat preferences of early juveniles stages of bumphead parrotfish. These data were analyzed by Hamilton et al. 2017 and Brown and Hamilton 2018. In the 2010s Rick Hamilton (The Nature Conservancy) lead a series of surveys in the nearshore reef habitats of Kia province, Solomon Islands. The aim was to look for the recruitment habitat for juvenile bumphead parrotfish. These surveys were motivated by concern from local communities in Kia that topa (the local name for bumpheads) are in decline. In the surveys, divers swam standardized transects and searched for juvenile bumphead in nearshore habitats, often along the edge of mangroves. All together they surveyed 49 sites across Kia. These surveys were made all the more challenging by the occurrence of crocodiles in mangrove habitat in the region. So these data are incredibly valuable. Logging in the Kia region has caused water quality issues that may impact nearshore coral habitats. During logging, logs are transported from the land onto barges at ‘log ponds’. A log pond is an area of mangroves that is bulldozed to enable transfer of logs to barges. As you can imagine, logponds are very muddy. This damage creates significant sediment runoff which can smother and kill coral habitats. Rick and the team surveyed reefs near logponds and in areas that had no logging. They only ever found bumphead recruits hiding in branching coral species. In this course we will first ask if the occurrence of bumphead recruits is related to the cover of branching coral species. We will then develop a statistical model to analyse the relationship between pollution from logponds and bumphead recruits, and use this model to predict pollution impacts to bumpheads across the Kia region. The data and code for the original analyses are available at my github site. In this course we will use simplified versions of the original data. We’re grateful to Rick Hamilton for providing the data for this course. "],["inline-code-editing.html", "Chapter 6 Inline code editing 6.1 1. Code completion 6.2 2. Using comments 6.3 3. Code completion settings 6.4 4. Inline code generation", " Chapter 6 Inline code editing This chapter explores techniques for using GitHub Copilot’s inline code editing capabilities to enhance your R programming workflow. 6.1 1. Code completion This is only option supported in Rstudio (last time I checked). Assuming you have github copilot set-up you just need to start a new R script (remember to keep it organized and give it a useful name) and start typing. You’ll see suggested code completions appear in grey. Hit tab to complete them. Let’s read in the benthic site data and fish counts: library(tidyverse) library(readr) dat &lt;- read_csv(url(&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/fish-coral-cover-sites.csv&quot;)) head(dat) summary(dat) Now try create a ggplot of secchi (a measure of water clarity, higher values mean clearer water) and pres.topa (count of topa, the bumphead parrotfish). Start typing gg and see what happens. You should a recommendation for a ggplot. But it won’t know the variable names. Tip: Sometimes GC gets stuck in a loop and keeps recommending the same line. To break it out of the loop try typing something new. 6.2 2. Using comments The code completion is using your script and all open scripts in VScode to predict your next line of code. It won’t know the variable names unless you’ve provided that. One way is to include them in the readme.md file and have that open, another is to use comments in the active script (which tends to work more reliably), e.g. # Make a point plot of secchi against pres.topa gg... Should get you the write ggplot. Using variable names in your prompts is more precise and will help the LLM guess the right names. You could also try putting key variable names in comments at the top of your script. Another way to use autocomplete is not to write R at all, just to write comments and fix the R code. Try templating a series of plots like: # Make a point plot of secchi against pres.topa with a stat_smooth # Plot logged (two categories) and pres.topa as a boxplot # Plot CB_cover (branching coral cover) against secchi Now go back through and click under each line to get the suggestions. This strategy is great in data wrangling workflows. As a simple example try make this grouped summary using comments only: dat %&gt;% group_by(logged) %&gt;% summarize(mean_topa = mean(pres.topa), mean_CB = mean(CB_cover)) To make this I might write this series of comments: #group dat by logged #summarize pres.topa and CB_cover If the variable names are documented above you can ofter be lazier and less precise with variable names here. 6.3 3. Code completion settings Click the octocat in the bottom right corner of VScode to fine-tune the settings. You can enable/disable code completions (sometimes they are annoying e.g. when writing a workshop!). You can also enable ‘next edit suggestions’. These are useful if editing an exisiting file. e.g. if you misspelt ‘sechi’ then updated it in one place, it will suggest updates through the script. Hit tab to move through these. The box will also tell you if indexing is available. Indexing allows AI to search your code faster. 6.4 4. Inline code generation In VScode you can also access an inline chat box with cmd/cntrl-i. This chat can chat as well as edit code. You can click anywhere and active this. I find it most useful though to select a section of code and then hit cmd/cntrl-i. This is most useful to - Add new code - Explain code - Fix bugs - Add tests Try select some of your code (e.g. a ggplot) and ask it to explain what the code does. Now try select one of your plots and ask for some style changes (e.g. theme, colours, axes label sizes etc…). Now add a bug into one of your plots. See if the inline chatbox can fix the bug. 6.4.1 Prompt shortcuts Use the / to bring up a list of prompt shortcuts. The most useful in R are /explain, /fix, /tests. Try select some code then use these to see what happens. "],["planning-your-analysis-with-ask-mode.html", "Chapter 7 Planning your analysis with Ask mode 7.1 Stages of analysis 7.2 Ask mode 7.3 The jagged frontier of LLM progress 7.4 How to prompt for better statistical advice 7.5 Planning implementation", " Chapter 7 Planning your analysis with Ask mode 7.1 Stages of analysis There are overall decisions you need to make when developing your analysis: What types of statistics to use. How to implement those statistics in R code. Its worthwhile separting these two decisions, as they are different issues. One is a science question, the other is a programming question. When using Assistants its also worthwhile using different chat sessions to try and find answers. 7.2 Ask mode Ask mode helps you plan analysis and implementation, using context from your project. In VScode click the ‘octocat’ symbol that should be at the top towards the right. This will open the chat window. The chat panel will appear down the bottom of this new sidebar. Confirm that the chatbot is currently set to ‘Ask’ mode. Your current file will automatically be included as context for the prompt. You can drag and drop any other files here as well. Start by asking the chatbot for guidance on a statistical analysis. We are interested in how the abundance of Topa relates to coral cover. For instance you could ask: How can I test the relationship between pres.topa and CB_cover? Evaluate the quality of its response and we will discuss. 7.3 The jagged frontier of LLM progress LLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. Since then AI companies have been optimising their training and development for coding and logic. There are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. Current models get about 50% resolution on this benchmark. Their progress on math and logic is a bit more controversial. It seems like some of the math benchmarks (like AIME annual tests for top 5% highschool students) are saturated as LLMs are scoring close to 100% on these tests.. So newer tests of unsolved maths problems are being developed. However, others are finding that the ability of LLMs on math and logic are overstated, perhaps because the LLMs have been trained on the questions and the answers. Its also clear that AI companies have a strong financial incentive to find ways (real and otherwise) of improving on the benchmarks. Are the moment there is tough competition to be ‘industry leaders’ and grab market share with impressive results on benchmarks. Either way, it does seem that the current areas of progress are programming, math and logic. Evaluations on statistics and the R software are less common. The limited evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive than other benchmarks. An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model, found accuracy at suggesting the correct statistical test of between 8% and 90%. In general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. The results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). The lesson is two-fold. Just because LLMs excel at some tasks doesn’t mean they will excel at others. Second, good prompting strategies pay off. For us in the niche R world there is also another lesson. The LLMs should be good at helping us implement analyses (ie write the R code). However, they are less reliable as statisticians who can guide us on the scientific question of what type of analysis to do. 7.4 How to prompt for better statistical advice The limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that: Include domain knowledge in the prompt Include data or summary data in the prompt Combine domain knowledge with CoT (but CoT on its own doesn’t help) In addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 over GPT-mini. 7.4.1 Guidelines for prompting for statistical advice Attach domain knowledge Try to find quality written advice from recognized researchers to include in your prompts. Always provide context on the data For instance, the model will give better advice for the prompt above if we tell it that pres.topa is integer counts (it will probably then recommend poisson GLM straight away). Likewise, if your replicates are different sites, tell that to the model so it has the opportunity to recommend approaches that are appropriate for spatial analysis. Attach data to your prompts You can attach the whole dataset if its in plain text (e.g. csv). Or write a summary() and/or head() to file and attach that. Combine the above approaches with Chain of Thought Just add ‘use Chain of Thought reasoning’ to your prompt. Its that easy. Double-up on chain of thought with self evaluation After the initial suggest try prompts like “are you sure?”, “Take a deep breath, count to ten and think deeply”, “Evaluate the quality of the options on a 1-5 scale”. Tip: Make a library of reference material for your prompting. If you see vignettes, blogs, or supplemental sections of papers that explain an analysis well, save them as text files to use in prompts. 7.4.2 Improving our initial prompt by attaching data Recall our initial prompt was: How can I statistically test the relationship between pres.topa and CB_cover? Try some of the strategies above (make a new prompt by clicking the + button) and compare the quality of advice. For instance, you can save a data summary like this: write_csv(head(dat), &quot;resources/head-site-level-data.csv&quot;) Then drag and drop it into the ask window and add something like: How can I statistically test the relationship between pres.topa and CB_cover? Here are the first 6 rows of data 7.4.3 Improving our initial prompt by attaching domain knowledge You can further improve the response by attaching a trusted resource. e.g. save this webpage on count models for ecology to your computer. Then you can attach the html file. That turned out to be a bit slow to compute (file too large?). Would be better if we had in plain text (e.g. copy and paste the text to a file, or use an extraction tool to extract text from the html). If you installed the websearch tool (which will likely become default in future) then you could add a prompt like this: How can I statistically test the relationship between pres.topa and CB_cover? Here are the first 6 rows of data. pres.topa is my response and it is count data. Use @websearch to find robust recommendations for ecologists to analyse count data before proceeding with your recomemndations. That worked well for me. I then followed up with: Great. Evaluate the robustness of each suggestoin on a 1-10 scale And it gave me a nice summary suggesting to try overdispersion models first (which is a good suggestion). The absolute best practice would be to give the assistant all the context for your study and observational design. Let’s see how doing that can work in our favour when planning implementation. 7.5 Planning implementation The other main way to use Ask mode is for help in implementing an analysis. Many of our workflows are complex and involve multiple data wrangling steps. To get the best out of GC I recommend creating a detailed README.md file with project context. Let’s try that and use it to plan our project. Save the README.md that his here to a local file. (Remember that we are going to be using this as a prompt, so read it first). Now you can attach it (or open it then click new chat). Given all the context you’ve provided you can just write something simple like: Help me plan R code to implement this analysis. Or Help me plan the workflow and scripts to implement this analysis I did this. It suggested both code (that looked approximatley correct) and the directory structure, sticking to my guideline in the readme about being modular. You should iterative with Ask mode to if there are any refinements you want. Let’s move onto edit mode to see how to put this plan to action. "],["creating-your-code-with-edit-mode.html", "Chapter 8 Creating your code with Edit mode 8.1 Adding a plan to the readme 8.2 Workflows and tips for edit mode", " Chapter 8 Creating your code with Edit mode Edit mode will edit files for you. The best way to learn how is to just see it in action. Open the Chat panel and click the ‘Ask’ button, then select ‘Edit’. 8.1 Adding a plan to the readme Open the README.md. Then type this prompt: Help me plan the implementation of this project. Add the plan to the ## Steps section Click ‘Keep’ if you like what it did. Or you can suggest improvements. Alternatively, accept it for now and then edit it afterwards. Tip: Sometimes you can’t go back once copilot has made edits to a file. So its good practice to use git and commmit changes before and after editing. 8.1.1 Working through your plan Once you’re happy with the plan, you can get copilot to implement it. You can continue the current chat, or start a new chat to do this (depending on the length of the task). Now step through, asking copilot to create each file as you. At this point everyone’s answers will diverge, as there is an element of randomness to the LLM’s responses. We will compare as a class to see if everyone gets to a similar analysis and answer. Tip: We are using the readme.md is copilot’s memory. This means the assitant always has the context it needs across different chat sessions (where it would otherwise forget). So its important to keep the readme updated. Its also useful to help you remember if you come back to the project some months or years later. 8.1.2 Why so much code? Copilot is designed as a programming assistant. We don’t know its system message, but given the main market for this software is professional programmers, we can guess it has a strong emphasis on programming robust code. You might notice that copilot tend to ‘over-engineer’ your R scripts. For instance, it has a tendancy to make an if statement to check if each new package needs installing, before loading it. If you don’t like this style you can add a statement to the readme asking it to keep implementation simple. 8.2 Workflows and tips for edit mode Remember its an assistant, its not doing the project for you. So you need to make sure it stays on track. Left unattended (if you just accept, accept, accept without reading) it can go down rabbit holes. Sometimes it creates superfluous analyses or even incorret statistics. So here’s how I recommend you use it: Use git for version control so you can go back in to older versions. Read the suggested edits before accepting Keep the readme.md updated and keep attaching it to your prompts. This will help keep it focused on the tasks that matter Use a two-step approach to identifying the statistical tests first, then implementing them as R code second. If you conflate these tasks you risk letting copilot guide the stats and getting it wrong. You can use it to help implement multiple different types of statistical tests for experimenting. If you do this, I just suggest you still use a two-step approach: plan a list of stats options first, then get copilot to implement them so you can compare results. NEVER edit the file while copilot is working! To edit files it uses string matching to locate the position to insert the edits. If you change the file it may not find the correct place to insert the new code. Tip: LLMs will tend to suggest the most obvious statistical analyses. If you want to innovate creative new types of analyses you need to work a bit harder. One way to do this is to mix up your prompts to try and get cross-disciplinary pollination. For instance, you could ask it: “Suggest methods I could use for this analysis, taking inspiriation from different disciplines such as medicine, psychology and climate research”. 8.2.1 Suggested workflow for new analyses Here’s a workflow I’ve found works well if I’m doing an analysis that is new to means Read the literature to identify the appropriate analysis for the research question and data. Once I’ve narrowed down the options I look for useful domain knowledge: vignettes, manuals or blogs that have suitable R examples. Start a new folder, setting up the directory and readme as descriped in this workshop. Use copilot to implement the analysis, attaching data summaries and the domain knowledge to get the best prompts. 8.2.2 Suggested workflow for analyses I know well Much the same as above, just less planning and you don’t need to search the literature because you know what you want to do. If you save useful domain knowledge when you see it you will also have the documents on hand to support the assistant. "],["automated-workflows-with-agent-mode.html", "Chapter 9 Automated workflows with Agent mode 9.1 Exploring agent mode 9.2 Summary", " Chapter 9 Automated workflows with Agent mode Agents are LLMs that have tools that allow them to work autonomously. In effect they review the results of tool use (such as writing code and running code), then respond to those results. In Copilot’s chat window you can set it to ‘Agent’ mode to enable these features. After each tool use copilot will ask you to confirm the changes and the next action. At that point you can review its changes, make edits, or continue chatting to suggest refinements. Agent mode has access to the terminal, so it will be using the terminal application to run scripts it creates. We’ll demonstrate in class so you can understand what its doing. Image: Agent mode from https://code.visualstudio.com You can also just accept every suggestion without reading it, also called ‘vibe coding’. However, I don’t recommend doing that, especially when you are starting out. You need to get a feel of how much direction it needs and problems it might create. Without human intervention the algorithms have a tendency to go off task: Have a readme with clear steps that you attach as a prompt is also helpful for Agent mode. It helps it stay on topic. Agent mode also allows installation of additional tools, which we’ll explore later. 9.1 Exploring agent mode Let’s explore Agent mode’s features through some analysis. 9.1.1 Motivating example Bayesian time-series analysis We’ll develop time-series models to forecast rock lobster (Jasus edwardsii) abundance from annual diver surveys. I’ve provided you with summary data. If you want to use this data in your research it is freely available and the original should be downloaded from the AODN portal. We’ll use the INLA package for our time-series models. We’ll fit it for the first part of the data, then we’ll forecast to the last part. In this way we can test the model’s predictions against data that is independent of model fitting (validation). The example is based on my study where I asked how accurately we can forecast species abundance change in dynamic environment. In a rapidly changing environment the models we fit to historical data may no longer make accurate predictions to future, novel, environments. So our current models may overstate the future predictability of ecosystems. In short, the environments we want to predict to in the future have no analogue in contemporary data. This may make accurate prediction more challenging. To explore this idea I developed a new way of validating time-series. I deliberately designed validations that forced the model fitting to be to older data and the forecasting and accuracy evaluation to be on contemporary data. As such, if the environment has changed the parameters the model has learned from the historical data will no longer be relevant in the contemporary environment. We found the new method gave much more pessimistic estimates of model accuracy for species that undergo rapid changes. Whereas for species that have resisted environmental change the new method gave comparable results to traditional methods of validation. In today’s workshop we’ll look at the first step, which is how to fit a model and make forecasts. We chose Bayesian models with INLA because have several advantages over alternatives: Allow for complex heirarchical models in a familiar GLMM framework - we have structuring by time and sites to consider Are computationally fast to run - convenient if you are re-running the model to do cross validation. Automatically handles gaps in time-series - Our data has a gap in 2003 when funding for monitoring wasn’t available Straightforward to model non-normal data - we are using counts. We’ll use INLA to fit auto-regressive order 1 (AR1) models to rock lobster abundance, with a negative binomial distribution. We’ll also use INLA to make forecasts. Another nice thing about INLA for us is that it has an unusual way of implementing predictions. This tends to trip-up copilot, so we’ll see how to overcome that challenge and get copilot to write correct code. 9.1.2 Set-up your project Set-up a new project, including creating a readme following the structure we used before. Here’s the link to the data: library(tidyverse) dat &lt;- readr::read_csv(url(&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/ATRC-RLS-jasus-edwardsii-maria-island.csv&quot;)) You can see the readme.md I used to get started here. I encourage you to write your own to get a feel for how it works and develop your own style. 9.1.3 Prompts I used Once I had the folder and readme set-up here’s the series of prompts I used. I encourage you to explore making your own. I used Claude 4.0 as the model option. I’ve found that GPT occaisonally makes errors with tool use or stuffs up text matching when editing files (meaning it inserts text in the wrong place). I started a new chat session between each of these prompts. This helps manage the context window. I’m relying on updating the readme.md so Copilot has memory (and I get it to update that). Start by documenting the directory structure in the readme.md I&#39;d be most pleased if you can undertake to perform steps 1-2. Document the data variables in the readme when you are done. Tip: There’s no ‘optimal’ prompt, only better prompts. Sometimes the best way to write is the way you are most comfortable writing. You’ll get more out of your brain that way and copilot will end up performing the same. Ahoy you salty sea dog, we&#39;ve scrubbed down steps 1 and 2, time for you to raise the sail on step 3! (Ok so that last prompt definitely doesn’t follow the guidelines of being super clear, but I was bored and it seemed to work ok) It wrote some nice code for step 3, but had some problems with model convergence. At this point I intervened manually and edited the model myself. I didn’t really want it deciding the model structure for me, as I knew what I wanted (below is the model I used FYI). That fixed it and I got it to document the changes then started a new chat. Note that the Agent changed the default fitting algorithm, which I wasn’t pleased with. So always important to check the details. simple_model_formula &lt;- total_lobsters ~ 1 + protection_status + f(site_numeric, model = &quot;iid&quot;) + f(year, model = &quot;ar1&quot;, hyper = ar1_prior) ar1_model &lt;- inla( formula = simple_model_formula, data = train_data, family = &quot;nbinomial&quot;, # Use negative binomial for count data control.predictor = list(compute = TRUE), control.compute = list( dic = TRUE, waic = TRUE, cpo = FALSE, # Disable CPO to help convergence config = FALSE ), verbose = FALSE ) After fixing the model and updated the readme, here’s the next step: Alright cobber, take you best shot at step 4 That worked, which actually I was expecting it not to work based on prior experience. INLA does predictions as part of model fitting, so you can’t predict(model1) like you can with other packages.. I’ve found that often trips up copilot when it tries to predict directly from the model object. It might be that Claude 4.0 (only came out as I was writing this) now ‘knows’ not to make that mistake. I tried again with Claude 3.5 (older version) to see if I could fool that one. However, it avoided the problem by writing a custom fitting function (which would need careful checking). Anyway, the lesson was meant to be to show you how to solve these types of problems by attaching domain knowledge like the FAQ linked above. Copilot agent did have some problems running Rscript on my computer (used to source R files from terminal). So I added this line to the readme to help it: When using Rscript from terminal be sure to put the script in ““, e.g. Rscript \"Scripts/script1.R\" Just step 5 left to go, make me some nice plots using the types of colours that Wes Anderson would choose 9.1.4 Writing up the project? You can keep going from here if you like and get agent mode to write up the results it found as an Rmd file. It will use the tables it generates to (hopefully) make accurate interpretations. Pretty soon Copilot will also have vision capabilities (currently available in preview mode as of 2025-05-27). This means it will be able to interpret the figures it creates as well. We’ll see that in action when we look at Roo Code in a bit. If you do that, as always, don’t take anything for granted. Make sure you check everything and understand the results yourself. 9.1.5 Custom intstructions For heavy agent use you may want to set-up custom instructions. These apply to all prompts in a project. e.g. you could set preference for ggplot2, or tell it how to use Rscript to avoid terminal errors See here for instructions. 9.2 Summary Agent mode can really accelerate your workflow development. But there are some risks. It can also go off track or write excessive amounts of code (over-engineering). Best practices for using Agent mode include: Separate science questions (what stats) from implementation stats (what code) Understand the stats you want to do, don’t just rely on copilot to get it right Checking what it does at is does it, so you can keep it on track Giving strong guidelines e.g. through a project readme file. Keeping the readme updated to guide copilot Report AI use and how it was used in your publications "],["ethics-and-copyright.html", "Chapter 10 Ethics and copyright 10.1 Sustainability 10.2 Model biases 10.3 Rising inequality 10.4 Copyright 10.5 Managing data privacy 10.6 Supplement: Calculations of personal environmental impact from using LLMs", " Chapter 10 Ethics and copyright There’s several fundamental ethical issues we should discuss related to the development of LLMs and AI in general. That LLMs use considerable energy and water resources That many LLMs have probably been trained on copyright data, so there is IP theft That concentration of LLM capabilities in a few big companies may contribute to rising inequality That LLMs have inherent biases and will change the way science is done, possibly for the worse. I’ve developed a little quiz to help you think about your personal ethics. 10.1 Sustainability Training LLMs costs millions of dollars, much of this cost is energy use. Further, the data centres for training and running LLMs need water for cooling. Asking a finished LLM questions uses much less energy, but cumulatively across the globe it adds up to a lot. Here are a few informative statistics I found online: From Forbes: “ChatGPT’s daily power usage is nearly equal to 180,000 U.S. households, each using about twenty-nine kilowatts.” Microsoft emissions have risen 30% since 2020 due to data centers AI prompts use 10x more energy than a traditional google search To put it in context I did some calculations on my personal usage. I estimate the prompting I do through copilot each year will cost about 2.32 kg of C02 and about 1000 litres of water. (this is lower bound, as I also using LLMs for other tasks). To put that in context, flying the 1.5 hours from Halifax to Montreal is about 172kg of emissions. So I’m using approximately 10 less than a short flight. 1000L is equivalent to taking about 22 5-minute showers. Of course, the carbon cost is global, whereas the water cost is localised (Probably to US data centres, so by using this resource I’m really just making the water problem worse for Americans. ) So its not a huge increase in my personal energy use. But cumulatively across the globe it is a lot. More generally, humanities energy use is growing exponential. Despite renewables and so on, ultimately our planet won’t be able to sustain this energy drawdown. LLMs are part of that trend of growing energy use. At some point we need to start using less energy, or the biosphere will become depleted and return to a ‘moon like rock’ in one study’s words. Here’s my personal belief. If we’re smart humanity will use this technology to find ways to make our use of the planet more sustainable and ultimately save water and energy. Just like we should have been using fossil fuels to develop a transition to lasting sustainanle energy use. So you can guess how likely that is to happen… Its the reason I’m teaching this course. I don’t personally think that LLMs make our lives better, or humanity more sustainable. They just raise the bar on the rate of progress. You can bet industries are using this technology to improve their productivity (= greater environmental impacts). I believe as environmental scientists we need to try to keep up. Ultimately we need progress on local to planetary sustainability (environmental scientists) to outpace the development of the industries that are environmentally unsustainable. 10.2 Model biases This is a big one. I recommend everyone read this perspective on the ‘Illusion of Understanding’ Its important that we don’t become too reliant on AI for our work. That’s why I’m teaching and promoting thoughtful use. Some key points: We need to maintain and grow research fields that aren’t convenient to do with AI, not just grow the stuff that’s easy with AI We need to push ourselves as individuals to not ‘be lazy’ and rely on AI too much. There is still great value in human learning. This requires mental energy, for instance, you will know something better if you write it yourself rather than write it with AI. We need to be aware of biases in the content AI generates For statistics these biases are likely to be a preference for well-known methods developed by Western science. So you should still read the literature broadly and avoid using AI, or prompt it in different ways, if you truly want to create novel statitistics (as opposed to using it to do statistics on a study that is otherwise novel data etc…) 10.3 Rising inequality AI development is currently concentrated in the USA and profits for LLM use go to American companies. (USA is itself a country with massive inequality issues!). So the extent to LLMs replaces labour will redirect income and taxes from jobs in countries to American companies. It is likely that the current low cost of LLM use will not continue. Companies are running at a loss in order to gain market share. So be careful how dependent you become on the LLMs and what that budget is replacing in your research budgets. I personally beleive that our own countries should be developing our own LLM products and resources. Even if they are not ‘industry leading’ they can still be highly effective for specific tasks. There are open-source models available that can fill this role. 10.4 Copyright Many LLMs have been trained on pirated books. The extent to which this is recognized by law is still in court. For me personally its frustrating that I spent years developing a statistics blog (which was open-access, but I appreciated attribution), but now that information has been mined by LLMs. Thus AI companies are profiting from our collective knowledge. It is an even worse situation for authors who’s livelihoods and careers depend on their copyrighted works. Copilot does in theory block itself from writing code that might be copyrighted. However, the efficacy of this system is unclear (it seems to just be a command in the system prompt). So be careful. Here are some recommendations for individuals In general you own works you create with an LLM. This also means you have the liability for any works you create (not normally an issue in environmental sciences). e.g. you couldn’t blame the LLM if you had to retract a paper due to incorrect statistics. You should acknolwedge LLM use in academic publications, and what you used it for. Always look for original sources references, e.g. don’t ‘cite’ the LLM for use of a GLM, use a textbook or reputable source (Zuur’s books are good for this!) 10.5 Managing data privacy Any prompt you send to an LLM provider is going to the server of an AI company (e.g. Google). So its important to be mindful of what information you are including in your prompts. The data you send (including text data) will be covered by the privacy policy of the LLM provider. Some services claim to keep your data private (e.g. the Copilot subscription my University has). Public services will tend to retain the right to use any data you enter as prompts. This means if you put your best research ideas into chatGPT, its possible that it will repeat them later to another user who asks similar questions. So be mindful of what you are writing. Before using an LLM to help with data analysis, be sure you understand the IP and ethical considerations involved with that data. For instance, if you have human survey data you may not be allowed to send that to a foreign server, or reveal any information to an LLM. In that case you have three options. 10.5.0.1 Option 1: Locally hosted LLM Use a locally hosted LLM. We won’t cover setting these up in this workshop. Locally hosted LLMs run on your computer. They can be suitable for simpler tasks and if you have a reasonably powerful GPU. Downsides are they do not have the performance of the industry leading LLMs and response times can be slower. 10.5.0.2 Option 2: Keep data seperate from code development. Use the LLM to help generate code to analyse the data, but do not give the LLM the data or the results. I would recommend keeping the data in a different directory altogether (ie not your project directory), so that LLM agents don’t inadvertently access the raw data. You also want to be sure that the LLM isn’t returning results of data analysis to itself (and therefore you reveal private information to the LLM). It can be helpful to generate some simulated data to use for code development, so there is no risk of violating privacy. 10.5.0.3 Option 3: Ignore sensitive folders Some LLM agents can be directed to ignore specific folders. e.g. You could add a command to ignore a folder to copilot custom instructions, Roo Code has a .rooignore file for this. However, remember prompts are not 100% precise (unlike real code), so there’s still the chance the LLM will go in those folders. So be careful, if its really sensitive keep it elsewhere on your computer, and always check its actions before you approve them. 10.6 Supplement: Calculations of personal environmental impact from using LLMs A ChatGPT request uses 2.9 watt-hour. So say that’s similar cost for coding applicatoins (probably more due to the additional context we are loading with every prompt). Then looking at my chat history I had 14 conversations in the last week (not counting in-line editing). Average was 3x requests per conversation, so in a year that equals: 2.9 * 14 * 3 * 52 = 6.33 kW-hours In USA energy cost on Average is 367 grams C02 per kW-hour. (https://www.eia.gov/tools/faqs/faq.php?id=74&amp;t=11) So my conservative estimated yearly usage for coding: 6.33 x 367 = 2.32 kg C02 For comparison flying the 1.5 hours from Halifax to Montreal is about 172kg of emissions. So my personal annual emissions for coding are perhaps about 10x than a short plane flight. Water is used for cooling in data centres: “A single ChatGPT conversation uses about fifty centilitres of water, equivalent to one plastic bottle.” Based on calculations above, this equates to about 1000L per year. That’s equivalent to about 22 x 5-minute showers. "],["advanced-llm-agents.html", "Chapter 11 Advanced LLM agents", " Chapter 11 Advanced LLM agents Time: 3:00-3:30pm Software requirements: VScode with R, Roo code, API license. We’ll take a quick look at Roo Code and its customization options. Roo code is more complex and expensive to use than copilot, but allows significant amounts of customization to make bespoke agents that can help with the scientific process. I’ll use an example with the Benthic Data analysis (benthic-readme.md). Talk through: API access Model options Customizing system message Context window management Cost Vision capabilities "],["cost-and-security.html", "Chapter 12 Cost and security 12.1 Cost considerations 12.2 API security 12.3 Agent security", " Chapter 12 Cost and security This chapter addresses important practical considerations when using LLMs for R programming: 12.1 Cost considerations PIs need to consider cost and impact on research budget e.g. Copilot subscription free for students Tools like Roo Code can be more expensive (pay per use as using API). Still less than a person (currently) e.g. processing 6000 abstracts to extract data for a lit review might cost about USD300 (including cost of developing prompts) Strategies for optimizing token usage Balancing cost with capability requirements AI companies are running at a loss and its quite likely that costs will go up in future. The aim right now is to get us all dependent on the technology, so that we have to keep paying in future (another reason I think its improtant our own countries develop these capaibilites, and that we also need to strive to be capable to work in AI free ways as well. ) 12.2 API security Managing API keys and credentials Sanitizing inputs to remove sensitive information Local vs. cloud-based LLM solutions Auditing and monitoring LLM interactions 12.3 Agent security Can run code on your computer Be careful what it is doing Read prompts before running them "],["conclusion.html", "Chapter 13 Conclusion 13.1 The changing landscape of scientific computing 13.2 Developing community standards 13.3 Future directions 13.4 Recommendations for students and ECRs 13.5 What should supervisors (PIs) do? 13.6 Everyone", " Chapter 13 Conclusion Time: 3:30pm-4:00pm We’ll discuss as a group what LLMs mean for the way we do science, and creating community standards. This chapter synthesizes the key insights from the workshop and explores the broader implications of LLMs for scientific practice: 13.1 The changing landscape of scientific computing How LLMs are transforming research workflows Potential impacts on reproducibility and transparency Changes in skill requirements and education Democratization of advanced programming capabilities 13.2 Developing community standards Ethical considerations for LLM use in scientific research Documentation and reporting practices Peer review in the age of LLM-assisted research Balancing innovation with methodological rigor 13.3 Future directions Emerging trends in LLM technology Potential developments in R-specific LLM tools Opportunities for community contribution and development Preparing for the next generation of AI-assisted data science This concluding discussion encourages critical reflection on how we can harness the power of LLMs while maintaining the integrity and quality of scientific research and analysis. 13.4 Recommendations for students and ECRs Manage your learning. If it is ‘easy’ you aren’t learning as much. Still strive to do things ‘AI free’ as you will learn more that way. Use the AI to aid learning, rather than replace you. e.g. ask it for stats, code or writing advice, rather than asking it just to do the task. 13.5 What should supervisors (PIs) do? Discuss standards with your lab Define boundaries for AI use (e.g. how much for coding, citations etc… ) Consider $ cost and plan for that. Try to avoid over-dependence on AI. For prompting for specific tasks, I suggest its the supervisor who’s writing System prompts that the lab uses. For example, this could be for lit reviews, stats guidelines, writing etc… The students are writing the user prompts to interact with their specific problem. 13.6 Everyone Consider your ethics Develop protocols for AI use, keep a library of your prompts and resources, what works, what didn’t work. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
