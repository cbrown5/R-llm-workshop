[["index.html", "Quality R analysis with large language models Chapter 1 Quality R analysis with large language models 1.1 Summary 1.2 About Chris 1.3 Course outline 1.4 Software you’ll need for this workshop 1.5 Software licenses 1.6 R packages you’ll need for this workshop 1.7 Data", " Quality R analysis with large language models CJ Brown (c.j.brown@utas.edu.au) 2025-05-26 Chapter 1 Quality R analysis with large language models 1.1 Summary If you are using R you are probably using language model assistants (e.g. ChatGPT) to help you write code, but are you using them in the most effective way? Language models have different biases to humans and so make different types of errors. This 1-day workshop will cover how to use language models to learn R and conduct reliable environmental analyses. We will cover: Pros and cons of different assistants from the interfaces like ChatGPT to advanced coding assistants that can run and test code by themselves and keep going until the analysis is complete (and even written up). Best practice prompting techniques that can dramatically improve LLM performance for complex statistical applications Applying language models to common environmental applications such as GLMs, multivariate statistics and Bayesian statistics Copyright and ethical issues We’ll finish up with a discussion of what large language models mean for analysis and the scientific process. Requirements for interactive workshop: Laptop with R, Rstudio and VScode installed. Please see software instructions below. 1.1.0.1 Who should take this workshop? The workshop is for: anyone who currently uses R, from intermittent users to experienced professionals. The workshop is not suitable for those that need an introduction to R and I’ll assume students know at least what R does and are able to do tasks like read in data and create plots. 1.2 About Chris I’m an Associate Professor of Fisheries Science at University of Tasmania and an Australian Research Council Future Fellow. I specialise in data analysis and modelling, skills I use to better inform environmental decision makers. R takes me many places and I’ve worked with marine ecosystems from tuna fisheries to mangrove forests. I’m an experienced teacher of R. I have taught R to 100s people over the years, from the basics to sophisticated modelling and for everyone from undergraduates to my own supervisors. 1.3 Course outline Introduction to LLMs for R 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. Part 1 LLM fundamentals 10-10:30am Tea break: 10:30-10:45 Continue Part 1 LLM fundamentals 10:45-11:30am Part 2 Github copilot for R 11:30-12:00pm Lunch break: 12:00-1:00pm Continue part 2 Github copilot for R 1:00-2:15pm Ethics and copyright 2:15-2:45pm Tea break 2:45-3:00pm Part 3 Advanced coding assistants 3:00-3:30pm Conclusion and discussion of what it means for science 3:30-4:00pm 1.4 Software you’ll need for this workshop Save some time for set-up, there is a bit to it. You may also need ITs help if your computer is locked down. R of course, (4.2.2 or later) VScode It takes a bit of time and IT knowledge and setting up to connect VScode to R. So don’t leave that to the last minute See my installation instructions Once you have VScode, make sure you get these extensions (which can be found by clicking the four squares in the left hand menu bar): GitHub Copilot, GitHub Copilot Chat If you can’t get VScode to work with R you are still welcome to join. Most of the morning session can be done in Rstudio. In the afternoon you’ll need VScode if you want to try what I am teaching. 1.4.1 Optional software RStudio. You can do some of this workshop in Rstudio. But you’ll get more out of it if you use VScode. If you are using Rstudio, make sure you get a Github Copilot account and connect it to Rstudio. 1.4.1.1 Optional VScode extensions Web Search for Copilot (once installed, follow the instructions to set up your API key, I use Tavily because it has a free tier). Roo code extension for VScode. Markdown preview enhanced Let’s you view markdown files in a pane with cmd(cntrl)-shift-V Radian terminal I also recommend installing radian terminal. This makes your terminal for R much cleaner, has autocomplete and seems to help with some common issues. 1.5 Software licenses Github copilot Go to their page to sign-up. The free tier is fine. You can also get free Pro access as a student or professor (requires application). API key and account with LLM provider You will need API (application programming interface) access to an LLM to do all the examples in this workshop. This will allow us to interact with LLMs directly via R code. API access is on a pay per token basis. You will need to create an account with one of the below providers and then buy some credits (USD10 should be sufficient). Here are some popular choices: OpenRouter (recommended as gives you flexible access to lots of models) Anthropic OpenAI Once you have your API key, keep it secret. It’s a password. Be careful not to push it to aa github repo accidently. 1.6 R packages you’ll need for this workshop install.packages(c(\"vegan\", \"ellmer\",\"tidyverse\") INLA for Bayesian computation. Use the link, its not on cran. 1.7 Data We’ll load all data files directly via URL in the workshop notes. So no need to download any data now. Details on data attribution are below. 1.7.1 Benthic cover surveys and fish habitat In this course we’ll be analyzing benthic cover and fish survey data. These data were collected by divers doing standardized surveys on the reefs of Kia, Solomon Islands. These data were first publshed by Hamilton et al. 2017 who showed that logging of forests is causing sedimentation and impact habitats of an important fishery species. In a follow-up study Brown and Hamilton 2018 developed a Bayesian model that estimates the size of the footprint of pollution from logging on reefs. 1.7.2 Rock lobster time-series We’ll analyse time-series data from the Australian Temperate Reef Collaboration and the Reef Life Survey. Original data are available here, and you should access the originals for any research. This data is collected by divers every year who swim standardized transects and count reef fauna. We’ll use a simplified version I created to go with this course. The link is in the notes when we need it. Citation to the dataset. The analysis in the notes is inspired by this study looking at how we forecast reef species change. "],["introduction-to-llms-for-r.html", "Chapter 2 Introduction to LLMs for R", " Chapter 2 Introduction to LLMs for R Time: 9-10am In this presentation I’ll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. This chapter provides an overview of: How Large Language Models (LLMs) function and their capabilities Best practices for prompt engineering when working with R Software options available for R users to interact with LLMs (coding assistants) Practical applications of LLMs for R programming and data analysis Ethical considerations when using LLMs for scientific work We’ll explore how LLMs can enhance your R workflow, from code generation to data analysis assistance, while maintaining scientific rigor and reproducibility. "],["llm-prompting-fundamentals.html", "Chapter 3 LLM prompting fundamentals 3.1 Setup authorisation 3.2 Understanding how LLMs work 3.3 DIY stats bot 3.4 Advanced prompting 3.5 Reflection on prompting fundamentals", " Chapter 3 LLM prompting fundamentals Start of practical material We’ll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you’ll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you’re not getting the results you expect. Software requirements: VScode with R or Rstudio, ellmer package, API license. 3.1 Setup authorisation First, you need to get an API key from the provider. Login to the provider’s website and follow the instructions. Then, you need to add the key to your .Renviron file: usethis::edit_r_environ() Then type in your key like this: OPENROUTER_API_KEY=\"xxxxxx\" Then restart R. ellmer will automatically find your key so long as you use the recommended envirment variable names. See ?ellmer::chat_openrouter (or chat_xxx where xxx is whatever provider you are using). 3.2 Understanding how LLMs work Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll use the ellmer package to demonstrate some fundamental concepts. By using ellmer to access a LLM through the API we get as close to the raw LLM as we are able. Later on we will use ‘coding assistants’ (e.g. copilot) which put another layer of software between you and the LLM. First, let’s set up our environment and create a connection to an LLM. library(ellmer) # Initialize a chat with Claude chat &lt;- chat_openrouter( system_prompt = &quot;&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Notice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in Let’s use the ‘system prompt’ to provide it with strong directions. Tip: The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, here’s the system prompt for the chat interface version of anthropic (Claude) chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, # model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 50) ) chat$chat(&quot;Ecologists like to eat &quot;) Tip: It is generally more effective to tell the LLM what to do rather than what not to do (just like people!). 3.2.1 Token-by-token prediction LLMs don’t understand text as complete sentences or concepts; they predict one token at a time based on the patterns they’ve learned during training. A token is roughly equivalent to a word part, a word, or a common phrase. Let’s see this in action by generating text token by token with a very limited number of tokens: prompt &lt;- &quot;Ecologists like to eat&quot; for(i in 1:5) { single_token_chat &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 1) ) response &lt;- single_token_chat$chat(prompt) prompt &lt;- paste(prompt, response, sep = &quot;&quot;) cat(&quot;Token&quot;, i, &quot;:&quot;, response, &quot;\\n&quot;) } # Display the complete sequence cat(&quot;Full sequence:&quot;, prompt, &quot;\\n&quot;) This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context. 3.2.2 Temperature effects The “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable. Let’s compare responses with different temperatures: # Create chats with different temperature settings chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 0) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) chat_temp &lt;- chat_openrouter( system_prompt = &quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;, model = &quot;anthropic/claude-3.5-haiku&quot;, api_args = list(max_tokens = 50, temperature = 2) ) chat_temp$chat(&quot;Marine ecologists like to eat &quot;) At low temperatures, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent. 3.2.3 Comparing model complexity Different models have different capabilities based on their size, training data, and architecture. For example anthropic/claude-3.5-haiku has many fewer parameters than anthropic/claude-3.7-sonnet. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15 For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot. 3.2.4 Understanding context windows LLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million tokens. We’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window. 3.3 DIY stats bot Let’s put together what we’ve learnt so far and built our own chatbot. I’ve provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session. stats_bot &lt;- readr::read_file(url(&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/DIY-stats-bot-system.md&quot;)) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 5000) ) ellmer has a few different options for interacting with chat bots. We’ve seen the ‘chat’ option. We can also have a live_console() or live_browser() (requires installing shinychat) chat. Let’s use one of those options. With live_browser() you’ll also see the browser automatically formats any markdown in the chat. live_browser(chat_stats) # live_console(chat_stats) Here are some suggested questions to start, but feel free to try your own. “Who are you?” “Use stats mode to provide me with some suggestions for how I could make a predictive model of a variable y, where I have a large number of potential explanatory variables.” Tip: How many of you started using “DIY-stats-bot-system.md” without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We’ll see later that LLMs can be given ‘tools’ which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We’ll cover security later. 3.3.1 Improving the stats bot Make a local copy of the stats bot system prompt and try editing it. Try different commands within it and see how your chat bot responds (you’ll have to open a new chat object each time). Here’s some ideas. Try making a chat bot that is a verhment Bayesian that abhors frequentist statistics. You could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models. Try different temperatures. Add your own easter egg. Tip: Adjectives, CAPITALS, *markdown* formatting can all help create emphasis so that your model more closely follows your commands. I used ‘abhors’ and ‘verhment’ above on purpose. 3.4 Advanced prompting LLMs can be given access to tools (github copilot is an example). Tools give the LLMs the ability to use tools to perform tasks on your computer. For example, copilot can edit files. The next step beyond tool use is to create agents. Agents write commands then also run those commands and feed the results back to themselves. Agents are therefore autonomous coders. Tip: LLM “Tools” give the LLM assistant the ability to take actions on your computer. There are a huge range of tools out there, including tools that write files, edit files, perform web searchers and download data. “Agents” are LLMs that return the results of tool use back to themselves. Therefore they can code and improve that code without intervention from humans (though intervention is often needed). We can turn out chatbot into a simple agent. This will help you understand how agents work. What we will do is give a command to format all R code in a specific way. Then we can have an R function that checks every new response for R code, if it detects R code it will write this to a file. 3.4.1 Zero-shot First make a new system message as a .md file, e.g. here: “resources/stats-bot-simple.md” in your project directory. Add to this message: You are an expert in statistial analysis with the R program. Write your response with R code in &lt;code&gt;&lt;/code&gt; tags. (You can also use our chat bot from before, but it might be simpler to make this shorter one) Now give the system message a test. stats_bot_simple &lt;- readr::read_file(&quot;resources/stats-bot-simple.md&quot;) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 400) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data, code to show how to do the regression and summary plots. &quot;) response If it works the response should look something like this: &lt;code&gt; dat &lt;- data.frame(temperature = rnorm(100, mean = 20)) dat$fish_size &lt;- 10 + temperature*2 + rnorm(100) model &lt;- lm(fish_size ~ temperature) &lt;/code&gt; It didn’t work for me when I tried! It kept returning the R code in markdown formatting in inconsistent ways. We want a consistent response so we can parse it easily. Anyway, repeat that a few times to see if it consistently formats the code as we requested. You can also try using “anthropic/claude-3.5-haiku” as your model (its cheaper) to see if that is smart enough to consistently format the code. Tip: What we just tried is called ‘zero-shot prompting’. Zero-shot means we didn’t give the LLM any examples to follow, we assume it knows what to do. 3.4.2 One-shot and multi-shot prompts Giving an example (one-shot) or multiple examples (multi-shot) can improve the precision of the LLMs responses. Let’s update the system prompt to see if we get more reliable answers with some examples: You are an expert in statistial analysis with the R program. Write your response with R code in &lt;code&gt;&lt;/code&gt; tags. ### Examples of formatting R code Single line example &lt;code&gt; dat &lt;- read.csv(&quot;myfile.csv&quot;) &lt;/code&gt; Multiple line example &lt;code&gt; dat &lt;- read.csv(&quot;myfile.csv&quot;) ggplot(dat) + aes(x = x, y = y) + geom_point() &lt;/code&gt; I saved this updated system to a file: “resources/stats-bot-simple-multiple-shot.md”. Now test it out. stats_bot_simple &lt;- readr::read_file(&quot;resources/stats-bot-simple-multiple-shot.md&quot;) chat_stats &lt;- chat_openrouter( system_prompt = stats_bot_simple, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 1000) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;) response That worked for me. Hopefully it works for you too? Test that to see if it gives more reliable examples. We have to be a bit careful with choosing examples as these are for formatting, not for modelling. For instance, we don’t want to inadventantly suggest the LLM always recommend a particular statistical model type. ’ 3.4.3 Making a simple tool Tools are algorithms your assistant can use to perform tasks on your computer or a server. They are what turns a chatbot that can only respond in text into an Assistant that can write code that takes actions. Tip: You might also see the term ‘MCP’ (Model Context Protocol) used in association with LLM assistants. MCP is an interoperable standard for developing tools. This means a tool I write is consistent with other people’s tools. This helps developers and LLMs read and interpret tools. It is also important to allow LLM to use multiple tools at the same time. ’ If the request to use tags works we can then parse the responses and save them as a file, ready to run. First create an R function that can parse responses and search for tags (you can cut and paste my function or try using copilot to make one for you, it’s good at this type of tedious task): write_response &lt;- function(response, filename = &quot;rscript.R&quot;){ # Extract the text between &lt;code&gt; and &lt;/code&gt; tags using regex # Using dotall mode (?s) to match across multiple lines code_pattern &lt;- &quot;(?s)&lt;code&gt;(.*?)&lt;/code&gt;&quot; code_matches &lt;- regmatches(response, gregexpr(code_pattern, response, perl = TRUE)) # Flatten the list and remove the tags if(length(code_matches) &gt; 0 &amp;&amp; length(code_matches[[1]]) &gt; 0) { code_texts &lt;- gsub(&quot;&lt;code&gt;|&lt;/code&gt;&quot;, &quot;&quot;, code_matches[[1]]) # Write all extracted code to the file writeLines(code_texts, filename) cat(&quot;Code written to&quot;, filename, &quot;\\n&quot;) } else { warning(&quot;No code tags found in the response&quot;) } } Now we can set-up our R code to write any suggestions to file: chat_stats &lt;- chat_openrouter( system_prompt = stats_bot, model = &quot;anthropic/claude-3.7-sonnet&quot;, api_args = list(max_tokens = 400) ) response &lt;- chat_stats$chat(&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;) write_response(response, filename = &quot;attempt1.R&quot;) Now check the file we just created. If it looks ok then try running it: #CHECK THE FILE before running!!!! source(&quot;attempt1.R&quot;) The source() command will attempt to run the R code that was created. If it works you should see some plots pop up. 3.4.4 Summary of agents and tools We’ve implemented a rudimentary tool and the beginnings of an LLM agent. Going forwards we’ll used specialised software for these tasks. You won’t need to code them yourself. But coding this examples is helpful to understand the fundamentals of how they work. To recap, the basic workflow an agent follows is: Set-up a system prompt with detailed instructions for how the LLM should format responses User asks a question that is sent to the LLM LLM responds and sends response back to user Software on user’s computer attempts to parse and act on the response according to pre-determined rules User’s computers enacts the commands in the response and provides results to user Tools like Copilot Agent mode then go a step further and send the results of step 5 back to the LLM, which then interprets the results and the loop continues (sometimes with and sometimes without direct user approval). If you want to go further with making your own tools, then I suggest you check out ellmer package. It supports tool creation in a structured way. For instance, I made a tool that allows an LLM to download and save ocean data to your computer. 3.5 Reflection on prompting fundamentals The key things I hoped you learnt from this lesson are: Basic LLM jargon, including tokens, temperature, API access and different LLM models. Some different prompt strategies, including role prompting, emphasis, chain of thought and one-shot. The fundamentals of tool use and agents. Now you understand the basics, let’s get into Github Copilot. "],["github-copilot-for-r.html", "Chapter 4 Github copilot for R", " Chapter 4 Github copilot for R Time: 11:30-12:00pm I’ll show you how you can most effectively use github copilot to plan, code and write up your data analysis and modelling. Software requirements: VScode with R and github copilot license + extension for copilot. Github Copilot calls itself an ‘AI programming assistant’ or an ‘AI pair programmer’. I’ll refer to it as an ‘LLM coding assistant’ or just ‘Assistant’. Assistants add a layer of software between you and the LLM. The software is doing some hidden interpretation of what you want to do, as well as trying to save costs. For instance, for most assistants we often don’t get to control (or even see) the system message, the temperature or the number of output tokens. The assistant is also guessing context to include in the prompt, so it can automatically give the LLM more context. At the same time it is managing the LLM’s context window and trying to save on costs. There is no generic name for this type of software (the field is moving to fast to have standardized names). So I’ll refer to them Assistants. In this bucket I’ll also put chatGPT, Claude, Roo Code, Cline and others. Note that Github Copilot (which I’ll call copilot for short) is different to the ‘Copilot’ assistant that is on the web and in the Teams app. This software is also called ‘chatbots’, however, I prefer assistants as the tasks they can do are much broader than just chatting. Tip: You’ll get the most of out Github Copilot if you use Visual Studio Code as your development environment (rather than RStudio). Setting up VScode with R can be a bit fiddly, check out my my installation instructions if you have trouble. Web searching advice is also a good idea if you are stuck. Its worth the effort. Copilot It is developing rapidly, so it is quite likely that when you read this there will be changes and new features. In this section I’ll focus on showing the main ways you can use copilot. Just be aware the implementation may change in future. We’ll look at: Overview VScode for those that are new to this software Best practices for setting up your project directory Inline code editing Ask mode Edit mode Agent mode "],["best-practices-project-setup.html", "Chapter 5 Best practices project setup 5.1 Project organization 5.2 The README.md file 5.3 Example data", " Chapter 5 Best practices project setup This chapter focuses on establishing a project structure and workflow that can get the most out of your LLM assistants. 5.1 Project organization Its helpful to set-up your projects in an organized and modularised way. In my experience most R users write most of their analysis in one long script. Don’t do this. It will be hard for ‘future you’ to navigate. If its hard for a human to navigate, it will also be hard for the assistant. Here’s how I set-up my projects. 5.1.1 General guidance Create a new folder for each new project. Optional but recommended: Initiliaze a git repo in that folder (I use github desktop). Set-up folders and files in an organized way Ideally put the data in this folder also. However, large datasets or sensitive data can be kept in other folders. Keep scripts short and modularized (e.g one for data analysis, one for modelling). Once you have your folder you can make it an Rstudio project (if using Rstudio) or just use ‘open folder’ in vscode. If want to link multiple folders in then use VScode workspaces. If you are not using git (version control), then I recommend you learn. LLM code editing tools can cause you to lose older versions. So best to back them up with proper use of git. 5.1.2 Project directory structure example Here’s an example of a project directory structure. You don’t have to use this strucutre. the important thing is to be organized. my-project/ ├── README.md ├── .gitignore ├── Scripts/ # R code │ ├── 01_data-prep.R │ ├── 02_data-analysis.R │ └── 03_plots.R ├── Shared/ │ ├── Outputs/ │ │ ├── Figures/ │ │ ├── data-prep/ │ │ └── model-objects/ │ ├── Data/ │ └── Manuscripts/ └── Private/ 5.2 The README.md file The README.md is the memory for the project. If you use github it will also be the landing page for your repo, which is handy. Remember you are writing this for you and the LLMs. So think of it like a prompt. Here’s an example of some of the information you might want to include in your readme. # PROJECT TITLE ## Summary ## Aims ## Data methodology ## Tech context - We will use the R program - tidyverse packages for data manipulation - ggplot2 for data visualization Keep your scripts short and modular to facilitate debugging. Don&#39;t complete all of the steps below in one script. Finish scripts where it makes sense and save intermediate datasets. ## Steps As you go tick of the steps below. [ ] Wrangle data [ ] Fit regression [ ] Plot verification [ ] ... ## Data Include meta-data here and file paths. ## Directory structure my-project/ ├── README.md ├── .gitignore ├── Scripts/ # R code │ ├── 01_data-prep.R │ ├── 02_data-analysis.R │ └── 03_plots.R ├── Shared/ │ ├── Outputs/ │ │ ├── Figures/ │ │ ├── data-prep/ │ │ └── model-objects/ │ ├── Data/ │ └── Manuscripts/ └── Private/ 5.3 Example data For the next few chapters we’ll work with some ecological data on benthic marine habitats and fish. 5.3.1 Case-study: Bumphead parrotfish, ‘Topa’ in Solomon Islands Bumphead parrotfish (Bolbometopon muricatum) are an enignmatic tropical fish species. Adults of these species are characterized by a large bump on their forehead that males use to display and fight during breeding. Sex determination for this species is unknown, but it is likely that an individual has the potential to develop into either a male or female at maturity. Adults travel in schools and consume algae by biting off chunks of coral and in the process they literally poo out clean sand. Because of their large size, schooling habit and late age at maturity they are susceptible to overfishing, and many populations are in decline. Their lifecycle is characterized by migration from lagoonal reef as juveniles (see image below) to reef flat and exposed reef habitats as adults. Early stage juveniles are carnivorous and feed on zooplankton, and then transform into herbivores at a young age. Image: Lifecycle of bumphead parrotfish. Image by E. Stump and sourced from Hamilton et al. 2017. Until the mid 2010s the habitat for settling postlarvae and juveniles was a mystery. However, the pattern of migrating from inshore to offshore over their known lifecycle suggests that the earliest benthic lifestages (‘recruits’) stages may occur on nearshore reef habitats. Nearshore reef habitats are susceptible to degradation from poor water quality, raising concerns that this species may also be in decline because of pollution. But the gap in data from the earliest lifestages hinders further exploration of this issue. In this course we’ll be analyzing the first survey that revealed the habitat preferences of early juveniles stages of bumphead parrotfish. These data were analyzed by Hamilton et al. 2017 and Brown and Hamilton 2018. In the 2010s Rick Hamilton (The Nature Conservancy) lead a series of surveys in the nearshore reef habitats of Kia province, Solomon Islands. The aim was to look for the recruitment habitat for juvenile bumphead parrotfish. These surveys were motivated by concern from local communities in Kia that topa (the local name for bumpheads) are in decline. In the surveys, divers swam standardized transects and searched for juvenile bumphead in nearshore habitats, often along the edge of mangroves. All together they surveyed 49 sites across Kia. These surveys were made all the more challenging by the occurrence of crocodiles in mangrove habitat in the region. So these data are incredibly valuable. Logging in the Kia region has caused water quality issues that may impact nearshore coral habitats. During logging, logs are transported from the land onto barges at ‘log ponds’. A log pond is an area of mangroves that is bulldozed to enable transfer of logs to barges. As you can imagine, logponds are very muddy. This damage creates significant sediment runoff which can smother and kill coral habitats. Rick and the team surveyed reefs near logponds and in areas that had no logging. They only ever found bumphead recruits hiding in branching coral species. In this course we will first ask if the occurrence of bumphead recruits is related to the cover of branching coral species. We will then develop a statistical model to analyse the relationship between pollution from logponds and bumphead recruits, and use this model to predict pollution impacts to bumpheads across the Kia region. The data and code for the original analyses are available at my github site. In this course we will use simplified versions of the original data. We’re grateful to Rick Hamilton for providing the data for this course. "],["inline-code-editing.html", "Chapter 6 Inline code editing 6.1 1. Code completion 6.2 2. Using comments 6.3 3. Code completion settings 6.4 4. Inline code generation", " Chapter 6 Inline code editing This chapter explores techniques for using GitHub Copilot’s inline code editing capabilities to enhance your R programming workflow. 6.1 1. Code completion This is only option supported in Rstudio (last time I checked). Assuming you have github copilot set-up you just need to start a new R script (remember to keep it organized and give it a useful name) and start typing. You’ll see suggested code completions appear in grey. Hit tab to complete them. Let’s read in the benthic site data and fish counts: library(tidyverse) library(readr) dat &lt;- read_csv(url(&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/fish-coral-cover-sites.csv&quot;)) head(dat) summary(dat) Now try create a ggplot of secchi (a measure of water clarity, higher values mean clearer water) and pres.topa (count of topa, the bumphead parrotfish). Start typing gg and see what happens. You should a recommendation for a ggplot. But it won’t know the variable names. Tip: Sometimes GC gets stuck in a loop and keeps recommending the same line. To break it out of the loop try typing something new. 6.2 2. Using comments The code completion is using your script and all open scripts in VScode to predict your next line of code. It won’t know the variable names unless you’ve provided that. One way is to include them in the readme.md file and have that open, another is to use comments in the active script (which tends to work more reliably), e.g. # Make a point plot of secchi against pres.topa gg... Should get you the write ggplot. Using variable names in your prompts is more precise and will help the LLM guess the right names. You could also try putting key variable names in comments at the top of your script. Another way to use autocomplete is not to write R at all, just to write comments and fix the R code. Try templating a series of plots like: # Make a point plot of secchi against pres.topa with a stat_smooth # Plot logged (two categories) and pres.topa as a boxplot # Plot CB_cover (branching coral cover) against secchi Now go back through and click under each line to get the suggestions. This strategy is great in data wrangling workflows. As a simple example try make this grouped summary using comments only: dat %&gt;% group_by(logged) %&gt;% summarize(mean_topa = mean(pres.topa), mean_CB = mean(CB_cover)) To make this I might write this series of comments: #group dat by logged #summarize pres.topa and CB_cover If the variable names are documented above you can ofter be lazier and less precise with variable names here. 6.3 3. Code completion settings Click the octocat in the bottom right corner of VScode to fine-tune the settings. You can enable/disable code completions (sometimes they are annoying e.g. when writing a workshop!). You can also enable ‘next edit suggestions’. These are useful if editing an exisiting file. e.g. if you misspelt ‘sechi’ then updated it in one place, it will suggest updates through the script. Hit tab to move through these. The box will also tell you if indexing is available. Indexing allows AI to search your code faster. 6.4 4. Inline code generation In VScode you can also access an inline chat box with cmd/cntrl-i. This chat can chat as well as edit code. You can click anywhere and active this. I find it most useful though to select a section of code and then hit cmd/cntrl-i. This is most useful to - Add new code - Explain code - Fix bugs - Add tests Try select some of your code (e.g. a ggplot) and ask it to explain what the code does. Now try select one of your plots and ask for some style changes (e.g. theme, colours, axes label sizes etc…). Now add a bug into one of your plots. See if the inline chatbox can fix the bug. 6.4.1 Prompt shortcuts Use the / to bring up a list of prompt shortcuts. The most useful in R are /explain, /fix, /tests. Try select some code then use these to see what happens. "],["planning-your-analysis-with-ask-mode.html", "Chapter 7 Planning your analysis with Ask mode 7.1 The jagged frontier of LLM progress 7.2 How to prompt for better statistical advice 7.3 Improving our initial prompt 7.4 Planning implementation", " Chapter 7 Planning your analysis with Ask mode Ask mode helps you plan code and analysis, using context from your project. In VScode click the ‘octocat’ symbol that should be at the top towards the right. This will open the chat window. The chat panel will appear down the bottom of this new sidebar. Confirm that the chatbot is currently set to ‘Ask’ mode. Your current file will automatically be included as context for the prompt. You can drag and drop any other files here as well. Start by asking the chatbot for guidance on a statistical analysis. We are interested in how the abundance of Topa relates to coral cover. For instance you could ask: How can I test the relationship between pres.topa and CB_cover? Evaluate the quality of its response and we will discuss. 7.1 The jagged frontier of LLM progress LLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. Since then AI companies have been optimising their training and development for coding and logic. There are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which looks at the ability of LLMs to autonomously create bug fixes. These test different abilities. LLMs started to get so good at maths that they had to develop a new test. This test was a set of 100 maths problems that have never been solved. XXX was getting XXX%, current models now score XXXX% However, evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive. An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model, found accuracy at suggesting the correct statistical test of between 8% and 90%. In general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. The results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). The lesson is two-fold. Just because LLMs excel at some tasks doesn’t mean they will excel at others. Second, good prompting strategies pay off. 7.2 How to prompt for better statistical advice The limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that: Include domain knowledge in the prompt Include data or summary data in the prompt Combine domain knowledge with CoT (but CoT on its own doesn’t help) In addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 over GPT-mini. 7.2.1 Guidelines for prompting for statistical advice Attach domain knowledge Try to find quality written advice from recognized researchers to include in your prompts. Always provide context on the data For instance, the model will give better advice for the prompt above if we tell it that pres.topa is integer counts (it will probably then recommend poisson GLM straight away). Likewise, if your replicates are different sites, tell that to the model so it has the opportunity to recommend approaches that are appropriate for spatial analysis. Attach data to your prompts You can attach the whole dataset if its in plain text (e.g. csv). Or write a summary() and/or head() to file and attach that. Combine the above approaches with Chain of Thought Just add ‘use Chain of Thought reasoning’ to your prompt. Its that easy. Double-up on chain of thought with self evaluation After the initial suggest try prompts like “are you sure?”, “Take a deep breath, count to ten and think deeply”, “Evaluate the quality of the options on a 1-5 scale”. Tip: Make a library of reference material for your prompting. If you see vignettes, blogs, or supplemental sections of papers that explain an analysis well, save them as text files to use in prompts. 7.3 Improving our initial prompt Recall our initial prompt was: How can I test the relationship between pres.topa and CB_cover? Try some of the strategies above (make a new prompt by clicking the + button) and compare the quality of advice. For instance, you can save a data summary like this: write_csv(head(dat), &quot;Shared/Data/site-level-data.csv&quot;) Or try attaching this blog TODO add GLM blog… 7.4 Planning implementation The other main way to use Ask mode is for help in implementing an analysis. Many of our workflows are complex and involve multiple data wrangling steps. To get the best out of GC I recommend creating a detailed README.md file with project context. Let’s try that and use it to plan our project. Save the README.md that his here to a local file. (Remember that we are using this as a prompt) UP TO HERE "],["creating-your-code-with-edit-mode.html", "Chapter 8 Creating your code with Edit mode", " Chapter 8 Creating your code with Edit mode This chapter explores how to leverage GitHub Copilot’s Edit mode to efficiently create and modify R code: Use this to do the MDS. Create some tests as well. "],["automated-workflows-with-agent-mode.html", "Chapter 9 Automated workflows with Agent mode 9.1 Introduction to Agent mode 9.2 Creating automated workflows 9.3 Common R workflow applications 9.4 Troubleshooting and optimization", " Chapter 9 Automated workflows with Agent mode Use this for time-series analysis This chapter explores how to use GitHub Copilot’s Agent mode to create automated workflows for R programming and data analysis: 9.1 Introduction to Agent mode What is Agent mode and how it differs from other Copilot features Accessing and using Agent mode in VSCode Understanding the capabilities and limitations of Agent mode When to use Agent mode in your R workflow 9.2 Creating automated workflows Defining tasks for Agent mode to accomplish Breaking down complex analyses into manageable steps Setting up appropriate contexts and environments Monitoring and guiding Agent mode’s progress 9.3 Common R workflow applications Data cleaning and preprocessing Exploratory data analysis Statistical modeling and hypothesis testing Report generation and visualization 9.4 Troubleshooting and optimization Handling errors and unexpected results Providing additional context when needed Refining workflow definitions for better results Balancing automation with human oversight Through practical demonstrations, you’ll learn how to leverage Agent mode to automate repetitive or complex R programming tasks, allowing you to focus on higher-level analysis and interpretation while maintaining control over the quality and correctness of your code. "],["ethics-and-copyright.html", "Chapter 10 Ethics and copyright 10.1 Model biases 10.2 Energy use 10.3 Copyright 10.4 Managing data privacy", " Chapter 10 Ethics and copyright This chapter overviews important ethical and legal considerations when using LLMs for R programming and data analysis. 10.1 Model biases Understanding inherent biases in LLMs Recognizing when biases might affect statistical analysis Techniques for mitigating bias in LLM-generated code and analysis 10.2 Energy use Environmental impact of LLM usage Balancing computational efficiency with analytical needs Strategies for reducing the carbon footprint of LLM-powered workflows 10.3 Copyright Legal considerations when using LLM-generated code Understanding the licensing implications of LLM outputs Best practices for attribution and documentation In general you own works you create with an LLM. This also means you have the liability for any works you create (not normally an issue in environmental sciences). e.g. you couldn’t blame the LLM if you had to retract a paper due to incorrect statistics. You should acknolwedge LLM use in academic publications, and what you used it for. 10.4 Managing data privacy Any prompt you send to an LLM provider is going to the server of an AI company (e.g. Google). So its important to be mindful of what information you are including in your prompts. The data you send (including text data) will be covered by the privacy policy of the LLM provider. Some services claim to keep your data private (e.g. the Copilot subscription my University has). Public services will tend to retain the right to use any data you enter as prompts. This means if you put your best research ideas into chatGPT, its possible that it will repeat them later to another user who asks similar questions. So be mindful of what you are writing. Before using an LLM to help with data analysis, be sure you understand the IP and ethical considerations involved with that data. For instnace, if you have human survey data you may not be allowed to send that to a foreign server, or reveal any information to an LLM. In that case you have three options. 10.4.0.1 Option 1: Locally hosted LLM Use a locally hosted LLM. We won’t cover setting these up in this workshop. Locally hosted LLMs run on your computer. They can be suitable for simpler tasks and if you have a reasonably powerful GPU. Downsides are they do not have the performance of the industry leading LLMs and response times can be slower. 10.4.0.2 Option 2: Keep data seperate from code development. Use the LLM to help generate code to analyse the data, but do not give the LLM the data or the results. I would recommend keeping the data in a different directory altogether (ie not your project directory), so that LLM agents don’t inadvertently access the raw data. You also want to be sure that the LLM isn’t returning results of data analysis to itself (and therefore you reveal private information to the LLM). It can be helpful to generate some simulated data to use for code development, so there is no risk of violating privacy. 10.4.0.3 Option 3: Ignore sensitive folders Some LLM "],["advanced-llm-agents.html", "Chapter 11 Advanced LLM agents 11.1 Roo code 11.2 Introduction to Roo code 11.3 Autonomous R programming with Roo 11.4 Advanced use cases for R 11.5 Best practices and workflow integration 11.6 Evolution of LLM agents 11.7 Agent capabilities for R 11.8 Integration with R workflows 11.9 Practical considerations", " Chapter 11 Advanced LLM agents Time: 3:00-3:30pm Software requirements: VScode with R, Roo code, API license. Use this with benthic readme. This chapter introduces advanced LLM agents that go beyond GitHub Copilot’s capabilities: 11.1 Roo code Fully automated workflows. This chapter explores Roo code as a powerful tool for creating fully automated workflows in R: Show roo code example. Talk through token usage, system prompt, editing prompts and roles, model choice, context window Image tool 11.2 Introduction to Roo code What is Roo code and how it differs from other LLM tools Setting up and configuring Roo code in VSCode Understanding Roo’s capabilities and limitations The architecture behind Roo’s automation capabilities 11.3 Autonomous R programming with Roo How Roo interprets task descriptions and requirements Roo’s approach to generating, testing, and refining code Monitoring and interacting with Roo during code generation Evaluating and validating Roo-generated solutions 11.4 Advanced use cases for R End-to-end data analysis pipelines Complex statistical modeling and inference Custom visualization development Package development and documentation 11.5 Best practices and workflow integration Providing effective task descriptions Balancing autonomy with guidance Incorporating Roo into existing development workflows Version control and collaboration with Roo-generated code Through practical demonstrations, you’ll learn how to leverage Roo code to automate sophisticated R programming tasks, potentially saving hours of development time while maintaining high-quality, reliable code. 11.6 Evolution of LLM agents How LLM agents differ from simple code completion tools The spectrum of automation in LLM-assisted programming Understanding agent architectures and capabilities Current state-of-the-art in LLM agents for R programming 11.7 Agent capabilities for R Code generation and modification Reasoning about complex statistical problems Autonomous debugging and error correction Documentation and explanation generation 11.8 Integration with R workflows Connecting agents to your development environment API-based interactions with LLM agents Batch processing vs. interactive assistance Combining multiple agents for complex tasks 11.9 Practical considerations Computational requirements and performance Cost structures and optimization strategies Privacy and security implications Evaluating agent output quality This chapter provides a foundation for understanding more sophisticated LLM agents, preparing you for the next chapter’s focus on Roo code as a specific implementation of advanced agent technology for R programming. "],["cost-and-security.html", "Chapter 12 Cost and security 12.1 Cost considerations 12.2 Security implications 12.3 Best practices for secure usage 12.4 Institutional considerations", " Chapter 12 Cost and security This chapter addresses important practical considerations when using LLMs for R programming: 12.1 Cost considerations Understanding the pricing models of different LLM providers API costs vs. subscription models Strategies for optimizing token usage Balancing cost with capability requirements Budgeting for LLM usage in research and professional contexts Costs likely to go up in future once we are all hooked. 12.2 Security implications Data privacy concerns when using LLMs Understanding what data is sent to LLM providers Risks of exposing sensitive or proprietary information Compliance considerations for different industries and research contexts 12.3 Best practices for secure usage Managing API keys and credentials Sanitizing inputs to remove sensitive information Local vs. cloud-based LLM solutions Auditing and monitoring LLM interactions 12.4 Institutional considerations Developing organizational policies for LLM usage Training researchers and staff on secure practices Documentation and transparency requirements Balancing innovation with risk management This chapter provides practical guidance for managing the financial and security aspects of incorporating LLMs into your R workflow, helping you make informed decisions about when and how to use these powerful tools. "],["conclusion.html", "Chapter 13 Conclusion 13.1 Workshop summary 13.2 The changing landscape of scientific computing 13.3 Developing community standards 13.4 Future directions", " Chapter 13 Conclusion Time: 3:30pm-4:00pm We’ll discuss as a group what LLMs mean for the way we do science, and creating community standards. This chapter synthesizes the key insights from the workshop and explores the broader implications of LLMs for scientific practice: 13.1 Workshop summary Recap of key concepts and techniques covered Integration of different LLM tools and approaches Progression from basic prompting to advanced autonomous agents Practical applications for R programming and data analysis 13.2 The changing landscape of scientific computing How LLMs are transforming research workflows Potential impacts on reproducibility and transparency Changes in skill requirements and education Democratization of advanced programming capabilities 13.3 Developing community standards Ethical considerations for LLM use in scientific research Documentation and reporting practices Peer review in the age of LLM-assisted research Balancing innovation with methodological rigor 13.4 Future directions Emerging trends in LLM technology Potential developments in R-specific LLM tools Opportunities for community contribution and development Preparing for the next generation of AI-assisted data science This concluding discussion encourages critical reflection on how we can harness the power of LLMs while maintaining the integrity and quality of scientific research and analysis. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
