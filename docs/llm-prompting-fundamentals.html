<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models</title>
  <meta name="description" content="1-day workshop for using large language models to generate R code." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="1-day workshop for using large language models to generate R code." />
  <meta name="github-repo" content="cbrown5/R-llm-workshop" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models" />
  
  <meta name="twitter:description" content="1-day workshop for using large language models to generate R code." />
  

<meta name="author" content="CJ Brown (c.j.brown@utas.edu.au)" />


<meta name="date" content="2025-05-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-llms-for-r.html"/>
<link rel="next" href="github-copilot-for-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quality R analysis with large language models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Quality R analysis with large language models</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#summary"><i class="fa fa-check"></i><b>1.1</b> Summary</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about-chris"><i class="fa fa-check"></i><b>1.2</b> About Chris</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#course-outline"><i class="fa fa-check"></i><b>1.3</b> Course outline</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#software-youll-need-for-this-workshop"><i class="fa fa-check"></i><b>1.4</b> Software you’ll need for this workshop</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#optional-software"><i class="fa fa-check"></i><b>1.4.1</b> Optional software</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#software-licenses"><i class="fa fa-check"></i><b>1.5</b> Software licenses</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#r-packages-youll-need-for-this-workshop"><i class="fa fa-check"></i><b>1.6</b> R packages you’ll need for this workshop</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data"><i class="fa fa-check"></i><b>1.7</b> Data</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="index.html"><a href="index.html#benthic-cover-surveys-and-fish-habitat"><i class="fa fa-check"></i><b>1.7.1</b> Benthic cover surveys and fish habitat</a></li>
<li class="chapter" data-level="1.7.2" data-path="index.html"><a href="index.html#rock-lobster-time-series"><i class="fa fa-check"></i><b>1.7.2</b> Rock lobster time-series</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-llms-for-r.html"><a href="introduction-to-llms-for-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to LLMs for R</a></li>
<li class="chapter" data-level="3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html"><i class="fa fa-check"></i><b>3</b> LLM prompting fundamentals</a>
<ul>
<li class="chapter" data-level="3.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#setup-authorisation"><i class="fa fa-check"></i><b>3.1</b> Setup authorisation</a></li>
<li class="chapter" data-level="3.2" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#understanding-how-llms-work"><i class="fa fa-check"></i><b>3.2</b> Understanding how LLMs work</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#token-by-token-prediction"><i class="fa fa-check"></i><b>3.2.1</b> Token-by-token prediction</a></li>
<li class="chapter" data-level="3.2.2" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#temperature-effects"><i class="fa fa-check"></i><b>3.2.2</b> Temperature effects</a></li>
<li class="chapter" data-level="3.2.3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#comparing-model-complexity"><i class="fa fa-check"></i><b>3.2.3</b> Comparing model complexity</a></li>
<li class="chapter" data-level="3.2.4" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#understanding-context-windows"><i class="fa fa-check"></i><b>3.2.4</b> Understanding context windows</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#diy-stats-bot"><i class="fa fa-check"></i><b>3.3</b> DIY stats bot</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#improving-the-stats-bot"><i class="fa fa-check"></i><b>3.3.1</b> Improving the stats bot</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#advanced-prompting"><i class="fa fa-check"></i><b>3.4</b> Advanced prompting</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#zero-shot"><i class="fa fa-check"></i><b>3.4.1</b> Zero-shot</a></li>
<li class="chapter" data-level="3.4.2" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#one-shot-and-multi-shot-prompts"><i class="fa fa-check"></i><b>3.4.2</b> One-shot and multi-shot prompts</a></li>
<li class="chapter" data-level="3.4.3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#making-a-simple-tool"><i class="fa fa-check"></i><b>3.4.3</b> Making a simple tool</a></li>
<li class="chapter" data-level="3.4.4" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#summary-of-agents-and-tools"><i class="fa fa-check"></i><b>3.4.4</b> Summary of agents and tools</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#reflection-on-prompting-fundamentals"><i class="fa fa-check"></i><b>3.5</b> Reflection on prompting fundamentals</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="github-copilot-for-r.html"><a href="github-copilot-for-r.html"><i class="fa fa-check"></i><b>4</b> Github copilot for R</a></li>
<li class="chapter" data-level="5" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html"><i class="fa fa-check"></i><b>5</b> Best practices project setup</a>
<ul>
<li class="chapter" data-level="5.1" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#project-organization"><i class="fa fa-check"></i><b>5.1</b> Project organization</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#general-guidance"><i class="fa fa-check"></i><b>5.1.1</b> General guidance</a></li>
<li class="chapter" data-level="5.1.2" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#project-directory-structure-example"><i class="fa fa-check"></i><b>5.1.2</b> Project directory structure example</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#the-readme.md-file"><i class="fa fa-check"></i><b>5.2</b> The README.md file</a></li>
<li class="chapter" data-level="5.3" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#example-data"><i class="fa fa-check"></i><b>5.3</b> Example data</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#case-study-bumphead-parrotfish-topa-in-solomon-islands"><i class="fa fa-check"></i><b>5.3.1</b> Case-study: Bumphead parrotfish, ‘Topa’ in Solomon Islands</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inline-code-editing.html"><a href="inline-code-editing.html"><i class="fa fa-check"></i><b>6</b> Inline code editing</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inline-code-editing.html"><a href="inline-code-editing.html#code-completion"><i class="fa fa-check"></i><b>6.1</b> 1. Code completion</a></li>
<li class="chapter" data-level="6.2" data-path="inline-code-editing.html"><a href="inline-code-editing.html#using-comments"><i class="fa fa-check"></i><b>6.2</b> 2. Using comments</a></li>
<li class="chapter" data-level="6.3" data-path="inline-code-editing.html"><a href="inline-code-editing.html#code-completion-settings"><i class="fa fa-check"></i><b>6.3</b> 3. Code completion settings</a></li>
<li class="chapter" data-level="6.4" data-path="inline-code-editing.html"><a href="inline-code-editing.html#inline-code-generation"><i class="fa fa-check"></i><b>6.4</b> 4. Inline code generation</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inline-code-editing.html"><a href="inline-code-editing.html#prompt-shortcuts"><i class="fa fa-check"></i><b>6.4.1</b> Prompt shortcuts</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html"><i class="fa fa-check"></i><b>7</b> Planning your analysis with Ask mode</a>
<ul>
<li class="chapter" data-level="7.1" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#stages-of-analysis"><i class="fa fa-check"></i><b>7.1</b> Stages of analysis</a></li>
<li class="chapter" data-level="7.2" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#ask-mode"><i class="fa fa-check"></i><b>7.2</b> Ask mode</a></li>
<li class="chapter" data-level="7.3" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#the-jagged-frontier-of-llm-progress"><i class="fa fa-check"></i><b>7.3</b> The jagged frontier of LLM progress</a></li>
<li class="chapter" data-level="7.4" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#how-to-prompt-for-better-statistical-advice"><i class="fa fa-check"></i><b>7.4</b> How to prompt for better statistical advice</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#guidelines-for-prompting-for-statistical-advice"><i class="fa fa-check"></i><b>7.4.1</b> Guidelines for prompting for statistical advice</a></li>
<li class="chapter" data-level="7.4.2" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#improving-our-initial-prompt-by-attaching-data"><i class="fa fa-check"></i><b>7.4.2</b> Improving our initial prompt by attaching data</a></li>
<li class="chapter" data-level="7.4.3" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#improving-our-initial-prompt-by-attaching-domain-knowledge"><i class="fa fa-check"></i><b>7.4.3</b> Improving our initial prompt by attaching domain knowledge</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="planning-your-analysis-with-ask-mode.html"><a href="planning-your-analysis-with-ask-mode.html#planning-implementation"><i class="fa fa-check"></i><b>7.5</b> Planning implementation</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html"><i class="fa fa-check"></i><b>8</b> Creating your code with Edit mode</a>
<ul>
<li class="chapter" data-level="8.1" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#adding-a-plan-to-the-readme"><i class="fa fa-check"></i><b>8.1</b> Adding a plan to the readme</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#working-through-your-plan"><i class="fa fa-check"></i><b>8.1.1</b> Working through your plan</a></li>
<li class="chapter" data-level="8.1.2" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#why-so-much-code"><i class="fa fa-check"></i><b>8.1.2</b> Why so much code?</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#workflows-and-tips-for-edit-mode"><i class="fa fa-check"></i><b>8.2</b> Workflows and tips for edit mode</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#suggested-workflow-for-new-analyses"><i class="fa fa-check"></i><b>8.2.1</b> Suggested workflow for new analyses</a></li>
<li class="chapter" data-level="8.2.2" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#suggested-workflow-for-analyses-i-know-well"><i class="fa fa-check"></i><b>8.2.2</b> Suggested workflow for analyses I know well</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html"><i class="fa fa-check"></i><b>9</b> Automated workflows with Agent mode</a>
<ul>
<li class="chapter" data-level="9.1" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#exploring-agent-mode"><i class="fa fa-check"></i><b>9.1</b> Exploring agent mode</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#motivating-example-bayesian-time-series-analysis"><i class="fa fa-check"></i><b>9.1.1</b> Motivating example Bayesian time-series analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#set-up-your-project"><i class="fa fa-check"></i><b>9.1.2</b> Set-up your project</a></li>
<li class="chapter" data-level="9.1.3" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#prompts-i-used"><i class="fa fa-check"></i><b>9.1.3</b> Prompts I used</a></li>
<li class="chapter" data-level="9.1.4" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#writing-up-the-project"><i class="fa fa-check"></i><b>9.1.4</b> Writing up the project?</a></li>
<li class="chapter" data-level="9.1.5" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#custom-intstructions"><i class="fa fa-check"></i><b>9.1.5</b> Custom intstructions</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#summary-1"><i class="fa fa-check"></i><b>9.2</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html"><i class="fa fa-check"></i><b>10</b> Ethics and copyright</a>
<ul>
<li class="chapter" data-level="10.1" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#model-biases"><i class="fa fa-check"></i><b>10.1</b> Model biases</a></li>
<li class="chapter" data-level="10.2" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#energy-use"><i class="fa fa-check"></i><b>10.2</b> Energy use</a></li>
<li class="chapter" data-level="10.3" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#rising-inequality"><i class="fa fa-check"></i><b>10.3</b> Rising inequality</a></li>
<li class="chapter" data-level="10.4" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#copyright"><i class="fa fa-check"></i><b>10.4</b> Copyright</a></li>
<li class="chapter" data-level="10.5" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#managing-data-privacy"><i class="fa fa-check"></i><b>10.5</b> Managing data privacy</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html"><i class="fa fa-check"></i><b>11</b> Advanced LLM agents</a>
<ul>
<li class="chapter" data-level="11.1" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#roo-code"><i class="fa fa-check"></i><b>11.1</b> Roo code</a></li>
<li class="chapter" data-level="11.2" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#introduction-to-roo-code"><i class="fa fa-check"></i><b>11.2</b> Introduction to Roo code</a></li>
<li class="chapter" data-level="11.3" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#autonomous-r-programming-with-roo"><i class="fa fa-check"></i><b>11.3</b> Autonomous R programming with Roo</a></li>
<li class="chapter" data-level="11.4" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#advanced-use-cases-for-r"><i class="fa fa-check"></i><b>11.4</b> Advanced use cases for R</a></li>
<li class="chapter" data-level="11.5" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#best-practices-and-workflow-integration"><i class="fa fa-check"></i><b>11.5</b> Best practices and workflow integration</a></li>
<li class="chapter" data-level="11.6" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#evolution-of-llm-agents"><i class="fa fa-check"></i><b>11.6</b> Evolution of LLM agents</a></li>
<li class="chapter" data-level="11.7" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#agent-capabilities-for-r"><i class="fa fa-check"></i><b>11.7</b> Agent capabilities for R</a></li>
<li class="chapter" data-level="11.8" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#integration-with-r-workflows"><i class="fa fa-check"></i><b>11.8</b> Integration with R workflows</a></li>
<li class="chapter" data-level="11.9" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#practical-considerations"><i class="fa fa-check"></i><b>11.9</b> Practical considerations</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="cost-and-security.html"><a href="cost-and-security.html"><i class="fa fa-check"></i><b>12</b> Cost and security</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cost-and-security.html"><a href="cost-and-security.html#cost-considerations"><i class="fa fa-check"></i><b>12.1</b> Cost considerations</a></li>
<li class="chapter" data-level="12.2" data-path="cost-and-security.html"><a href="cost-and-security.html#security-implications"><i class="fa fa-check"></i><b>12.2</b> Security implications</a></li>
<li class="chapter" data-level="12.3" data-path="cost-and-security.html"><a href="cost-and-security.html#best-practices-for-secure-usage"><i class="fa fa-check"></i><b>12.3</b> Best practices for secure usage</a></li>
<li class="chapter" data-level="12.4" data-path="cost-and-security.html"><a href="cost-and-security.html#institutional-considerations"><i class="fa fa-check"></i><b>12.4</b> Institutional considerations</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>13</b> Conclusion</a>
<ul>
<li class="chapter" data-level="13.1" data-path="conclusion.html"><a href="conclusion.html#workshop-summary"><i class="fa fa-check"></i><b>13.1</b> Workshop summary</a></li>
<li class="chapter" data-level="13.2" data-path="conclusion.html"><a href="conclusion.html#the-changing-landscape-of-scientific-computing"><i class="fa fa-check"></i><b>13.2</b> The changing landscape of scientific computing</a></li>
<li class="chapter" data-level="13.3" data-path="conclusion.html"><a href="conclusion.html#developing-community-standards"><i class="fa fa-check"></i><b>13.3</b> Developing community standards</a></li>
<li class="chapter" data-level="13.4" data-path="conclusion.html"><a href="conclusion.html#future-directions"><i class="fa fa-check"></i><b>13.4</b> Future directions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quality R analysis with large language models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="llm-prompting-fundamentals" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> LLM prompting fundamentals<a href="llm-prompting-fundamentals.html#llm-prompting-fundamentals" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Start of practical material</em></p>
<p>We’ll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you’ll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you’re not getting the results you expect.</p>
<p><strong>Software requirements:</strong> VScode with R or Rstudio, <a href="https://ellmer.tidyverse.org/">ellmer package</a>, API license.</p>
<div id="setup-authorisation" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Setup authorisation<a href="llm-prompting-fundamentals.html#setup-authorisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>First, you need to get an API key from the provider. Login to the provider’s website and
follow the instructions.</p>
<p>Then, you need to add the key to your <code>.Renviron</code> file:</p>
<p><code>usethis::edit_r_environ()</code></p>
<p>Then type in your key like this:</p>
<p><code>OPENROUTER_API_KEY="xxxxxx"</code></p>
<p>Then restart R. <code>ellmer</code> will automatically find your key so long as you use the recommended envirment variable names.
See <code>?ellmer::chat_openrouter</code> (or <code>chat_xxx</code> where xxx is whatever provider you are using).</p>
</div>
<div id="understanding-how-llms-work" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Understanding how LLMs work<a href="llm-prompting-fundamentals.html#understanding-how-llms-work" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll use the <code>ellmer</code> package to demonstrate some fundamental concepts.</p>
<p>By using <code>ellmer</code> to access a LLM through the API we get as close to the raw LLM as we are able. Later on we will use ‘coding assistants’ (e.g. copilot) which put another layer of software between you and the LLM.</p>
<p>First, let’s set up our environment and create a connection to an LLM.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="llm-prompting-fundamentals.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(ellmer)</span>
<span id="cb1-2"><a href="llm-prompting-fundamentals.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="llm-prompting-fundamentals.html#cb1-3" tabindex="-1"></a><span class="co"># Initialize a chat with Claude</span></span>
<span id="cb1-4"><a href="llm-prompting-fundamentals.html#cb1-4" tabindex="-1"></a>chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb1-5"><a href="llm-prompting-fundamentals.html#cb1-5" tabindex="-1"></a>  <span class="at">system_prompt =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb1-6"><a href="llm-prompting-fundamentals.html#cb1-6" tabindex="-1"></a>  <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb1-7"><a href="llm-prompting-fundamentals.html#cb1-7" tabindex="-1"></a>  <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>)</span>
<span id="cb1-8"><a href="llm-prompting-fundamentals.html#cb1-8" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="llm-prompting-fundamentals.html#cb1-9" tabindex="-1"></a>chat<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Ecologists like to eat &quot;</span>)</span></code></pre></div>
<p>Notice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in Let’s use the ‘system prompt’ to provide it with strong directions.</p>
<div class="tip">
<p><strong>Tip:</strong> The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, <a href="https://www.reddit.com/r/ClaudeAI/comments/1ixapi4/here_is_claude_sonnet_37_full_system_prompt/">here’s the system prompt for the chat interface version of anthropic (Claude)</a></p>
</div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="llm-prompting-fundamentals.html#cb2-1" tabindex="-1"></a>chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb2-2"><a href="llm-prompting-fundamentals.html#cb2-2" tabindex="-1"></a>  <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb2-3"><a href="llm-prompting-fundamentals.html#cb2-3" tabindex="-1"></a>  <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb2-4"><a href="llm-prompting-fundamentals.html#cb2-4" tabindex="-1"></a><span class="co">#   model = &quot;anthropic/claude-3.7-sonnet&quot;,</span></span>
<span id="cb2-5"><a href="llm-prompting-fundamentals.html#cb2-5" tabindex="-1"></a>  <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>)</span>
<span id="cb2-6"><a href="llm-prompting-fundamentals.html#cb2-6" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="llm-prompting-fundamentals.html#cb2-7" tabindex="-1"></a>chat<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Ecologists like to eat &quot;</span>)</span></code></pre></div>
<div class="tip">
<p><strong>Tip:</strong> It is generally more effective to tell the LLM <strong>what to do</strong> rather than <strong>what not to do</strong> (just like people!).</p>
</div>
<div id="token-by-token-prediction" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Token-by-token prediction<a href="llm-prompting-fundamentals.html#token-by-token-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LLMs don’t understand text as complete sentences or concepts; they predict one token at a time based on the patterns they’ve learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.</p>
<p>Let’s see this in action by generating text token by token with a very limited number of tokens:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="llm-prompting-fundamentals.html#cb3-1" tabindex="-1"></a> prompt <span class="ot">&lt;-</span> <span class="st">&quot;Ecologists like to eat&quot;</span></span>
<span id="cb3-2"><a href="llm-prompting-fundamentals.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="llm-prompting-fundamentals.html#cb3-3" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb3-4"><a href="llm-prompting-fundamentals.html#cb3-4" tabindex="-1"></a>        single_token_chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb3-5"><a href="llm-prompting-fundamentals.html#cb3-5" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb3-6"><a href="llm-prompting-fundamentals.html#cb3-6" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb3-7"><a href="llm-prompting-fundamentals.html#cb3-7" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">1</span>)</span>
<span id="cb3-8"><a href="llm-prompting-fundamentals.html#cb3-8" tabindex="-1"></a>    )</span>
<span id="cb3-9"><a href="llm-prompting-fundamentals.html#cb3-9" tabindex="-1"></a>    response <span class="ot">&lt;-</span> single_token_chat<span class="sc">$</span><span class="fu">chat</span>(prompt)</span>
<span id="cb3-10"><a href="llm-prompting-fundamentals.html#cb3-10" tabindex="-1"></a>    prompt <span class="ot">&lt;-</span> <span class="fu">paste</span>(prompt, response, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb3-11"><a href="llm-prompting-fundamentals.html#cb3-11" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Token&quot;</span>, i, <span class="st">&quot;:&quot;</span>, response, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb3-12"><a href="llm-prompting-fundamentals.html#cb3-12" tabindex="-1"></a>}</span>
<span id="cb3-13"><a href="llm-prompting-fundamentals.html#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="llm-prompting-fundamentals.html#cb3-14" tabindex="-1"></a><span class="co"># Display the complete sequence</span></span>
<span id="cb3-15"><a href="llm-prompting-fundamentals.html#cb3-15" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Full sequence:&quot;</span>, prompt, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context.</p>
</div>
<div id="temperature-effects" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Temperature effects<a href="llm-prompting-fundamentals.html#temperature-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.</p>
<p>Let’s compare responses with different temperatures:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="llm-prompting-fundamentals.html#cb4-1" tabindex="-1"></a><span class="co"># Create chats with different temperature settings</span></span>
<span id="cb4-2"><a href="llm-prompting-fundamentals.html#cb4-2" tabindex="-1"></a>chat_temp <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb4-3"><a href="llm-prompting-fundamentals.html#cb4-3" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb4-4"><a href="llm-prompting-fundamentals.html#cb4-4" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb4-5"><a href="llm-prompting-fundamentals.html#cb4-5" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>, <span class="at">temperature =</span> <span class="dv">0</span>)</span>
<span id="cb4-6"><a href="llm-prompting-fundamentals.html#cb4-6" tabindex="-1"></a>    )</span>
<span id="cb4-7"><a href="llm-prompting-fundamentals.html#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="llm-prompting-fundamentals.html#cb4-8" tabindex="-1"></a>chat_temp<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Marine ecologists like to eat &quot;</span>)</span>
<span id="cb4-9"><a href="llm-prompting-fundamentals.html#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="llm-prompting-fundamentals.html#cb4-10" tabindex="-1"></a>chat_temp <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb4-11"><a href="llm-prompting-fundamentals.html#cb4-11" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb4-12"><a href="llm-prompting-fundamentals.html#cb4-12" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb4-13"><a href="llm-prompting-fundamentals.html#cb4-13" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>, <span class="at">temperature =</span> <span class="dv">2</span>)</span>
<span id="cb4-14"><a href="llm-prompting-fundamentals.html#cb4-14" tabindex="-1"></a>    )</span>
<span id="cb4-15"><a href="llm-prompting-fundamentals.html#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="llm-prompting-fundamentals.html#cb4-16" tabindex="-1"></a>chat_temp<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Marine ecologists like to eat &quot;</span>)</span></code></pre></div>
<p>At low temperatures, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.</p>
</div>
<div id="comparing-model-complexity" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Comparing model complexity<a href="llm-prompting-fundamentals.html#comparing-model-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Different models have different capabilities based on their size, training data, and architecture.</p>
<p>For example <code>anthropic/claude-3.5-haiku</code> has many fewer parameters than <code>anthropic/claude-3.7-sonnet</code>. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15</p>
<p>For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot.</p>
</div>
<div id="understanding-context-windows" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Understanding context windows<a href="llm-prompting-fundamentals.html#understanding-context-windows" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million tokens.</p>
<p>We’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window.</p>
</div>
</div>
<div id="diy-stats-bot" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> DIY stats bot<a href="llm-prompting-fundamentals.html#diy-stats-bot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s put together what we’ve learnt so far and built our own chatbot. I’ve provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="llm-prompting-fundamentals.html#cb5-1" tabindex="-1"></a>stats_bot <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_file</span>(<span class="fu">url</span>(<span class="st">&quot;https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/DIY-stats-bot-system.md&quot;</span>))</span>
<span id="cb5-2"><a href="llm-prompting-fundamentals.html#cb5-2" tabindex="-1"></a></span>
<span id="cb5-3"><a href="llm-prompting-fundamentals.html#cb5-3" tabindex="-1"></a>chat_stats <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb5-4"><a href="llm-prompting-fundamentals.html#cb5-4" tabindex="-1"></a>          <span class="at">system_prompt =</span> stats_bot,</span>
<span id="cb5-5"><a href="llm-prompting-fundamentals.html#cb5-5" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.7-sonnet&quot;</span>,</span>
<span id="cb5-6"><a href="llm-prompting-fundamentals.html#cb5-6" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">5000</span>)</span>
<span id="cb5-7"><a href="llm-prompting-fundamentals.html#cb5-7" tabindex="-1"></a>    )</span></code></pre></div>
<p><code>ellmer</code> has a few different options for interacting with chat bots. We’ve seen the ‘chat’ option. We can also have a <code>live_console()</code> or <code>live_browser()</code> (requires installing <code>shinychat</code>) chat. Let’s use one of those options. With <code>live_browser()</code> you’ll also see the browser automatically formats any markdown in the chat.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="llm-prompting-fundamentals.html#cb6-1" tabindex="-1"></a><span class="fu">live_browser</span>(chat_stats)</span>
<span id="cb6-2"><a href="llm-prompting-fundamentals.html#cb6-2" tabindex="-1"></a><span class="co"># live_console(chat_stats)</span></span></code></pre></div>
<p>Here are some suggested questions to start, but feel free to try your own.
“Who are you?”
“Use stats mode to provide me with some suggestions for how I could make a predictive model of a variable y, where I have a large number of potential explanatory variables.”</p>
<div class="tip">
<p><strong>Tip:</strong> How many of you started using “DIY-stats-bot-system.md” without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We’ll see later that LLMs can be given ‘tools’ which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We’ll cover security later.</p>
</div>
<div id="improving-the-stats-bot" class="section level3 hasAnchor" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Improving the stats bot<a href="llm-prompting-fundamentals.html#improving-the-stats-bot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Make a local copy of the stats bot system prompt and try editing it. Try different commands within it and see how your chat bot responds (you’ll have to open a new chat object each time).</p>
<p>Here’s some ideas.</p>
<ul>
<li>Try making a chat bot that is a verhment Bayesian that abhors frequentist statistics.<br />
</li>
<li>You could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models.</li>
<li>Try different temperatures.</li>
<li>Add your own easter egg.</li>
</ul>
<div class="tip">
<p><strong>Tip:</strong> Adjectives, CAPITALS, <code>*markdown*</code> formatting can all help create emphasis so that your model more closely follows your commands. I used ‘abhors’ and ‘verhment’ above on purpose.</p>
</div>
</div>
</div>
<div id="advanced-prompting" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Advanced prompting<a href="llm-prompting-fundamentals.html#advanced-prompting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>LLMs can be given access to tools (github copilot is an example). Tools give the LLMs the ability to use tools to perform tasks on your computer. For example, copilot can edit files.</p>
<p>The next step beyond tool use is to create agents. Agents write commands then also run those commands and feed the results back to themselves. Agents are therefore autonomous coders.</p>
<div class="tip">
<p><strong>Tip:</strong> LLM “Tools” give the LLM assistant the ability to take actions on your computer. There are a huge range of tools out there, including tools that write files, edit files, perform web searchers and download data. “Agents” are LLMs that return the results of tool use back to themselves. Therefore they can code and improve that code without intervention from humans (though intervention is often needed).</p>
</div>
<p>We can turn out chatbot into a simple agent. This will help you understand how agents work. What we will do is give a command to format all R code in a specific way. Then we can have an R function that checks every new response for R code, if it detects R code it will write this to a file.</p>
<div id="zero-shot" class="section level3 hasAnchor" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Zero-shot<a href="llm-prompting-fundamentals.html#zero-shot" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>First make a new system message as a .md file, e.g. here: “resources/stats-bot-simple.md” in your project directory.</p>
<p>Add to this message:</p>
<pre><code>You are an expert in statistial analysis with the R program.  Write your response with R code in &lt;code&gt;&lt;/code&gt; tags.</code></pre>
<p>(You can also use our chat bot from before, but it might be simpler to make this shorter one)</p>
<p>Now give the system message a test.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="llm-prompting-fundamentals.html#cb8-1" tabindex="-1"></a>stats_bot_simple <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_file</span>(<span class="st">&quot;resources/stats-bot-simple.md&quot;</span>)</span>
<span id="cb8-2"><a href="llm-prompting-fundamentals.html#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="llm-prompting-fundamentals.html#cb8-3" tabindex="-1"></a>chat_stats <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb8-4"><a href="llm-prompting-fundamentals.html#cb8-4" tabindex="-1"></a>          <span class="at">system_prompt =</span> stats_bot,</span>
<span id="cb8-5"><a href="llm-prompting-fundamentals.html#cb8-5" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.7-sonnet&quot;</span>,</span>
<span id="cb8-6"><a href="llm-prompting-fundamentals.html#cb8-6" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">400</span>)</span>
<span id="cb8-7"><a href="llm-prompting-fundamentals.html#cb8-7" tabindex="-1"></a>    )</span>
<span id="cb8-8"><a href="llm-prompting-fundamentals.html#cb8-8" tabindex="-1"></a></span>
<span id="cb8-9"><a href="llm-prompting-fundamentals.html#cb8-9" tabindex="-1"></a>response <span class="ot">&lt;-</span> chat_stats<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data, code to show how to do the regression and summary plots. &quot;</span>)</span>
<span id="cb8-10"><a href="llm-prompting-fundamentals.html#cb8-10" tabindex="-1"></a>response</span></code></pre></div>
<p>If it works the response should look something like this:</p>
<pre><code>&lt;code&gt;
dat &lt;- data.frame(temperature = rnorm(100, mean = 20))
dat$fish_size &lt;- 10 + temperature*2 + rnorm(100)
model &lt;- lm(fish_size ~ temperature)
&lt;/code&gt;</code></pre>
<p>It didn’t work for me when I tried! It kept returning the R code in markdown formatting in inconsistent ways. We want a consistent response so we can parse it easily.</p>
<p>Anyway, repeat that a few times to see if it consistently formats the code as we requested. You can also try using “anthropic/claude-3.5-haiku” as your model (its cheaper) to see if that is smart enough to consistently format the code.</p>
<div class="tip">
<p><strong>Tip:</strong> What we just tried is called ‘zero-shot prompting’. Zero-shot means we didn’t give the LLM any examples to follow, we assume it knows what to do.</p>
</div>
</div>
<div id="one-shot-and-multi-shot-prompts" class="section level3 hasAnchor" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> One-shot and multi-shot prompts<a href="llm-prompting-fundamentals.html#one-shot-and-multi-shot-prompts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Giving an example (one-shot) or multiple examples (multi-shot) can improve the precision of the LLMs responses. Let’s update the system prompt to see if we get more reliable answers with some examples:</p>
<pre><code>You are an expert in statistial analysis with the R program.  Write your response with R code in &lt;code&gt;&lt;/code&gt; tags.

### Examples of formatting R code

Single line example

&lt;code&gt; dat &lt;- read.csv(&quot;myfile.csv&quot;) &lt;/code&gt; 

Multiple line example

&lt;code&gt; 
dat &lt;- read.csv(&quot;myfile.csv&quot;) 
ggplot(dat) + 
  aes(x = x, y = y) + 
  geom_point() 
&lt;/code&gt; 
</code></pre>
<p>I saved this updated system to a file: “resources/stats-bot-simple-multiple-shot.md”. Now test it out.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="llm-prompting-fundamentals.html#cb11-1" tabindex="-1"></a>stats_bot_simple <span class="ot">&lt;-</span> readr<span class="sc">::</span><span class="fu">read_file</span>(<span class="st">&quot;resources/stats-bot-simple-multiple-shot.md&quot;</span>)</span>
<span id="cb11-2"><a href="llm-prompting-fundamentals.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="llm-prompting-fundamentals.html#cb11-3" tabindex="-1"></a>chat_stats <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb11-4"><a href="llm-prompting-fundamentals.html#cb11-4" tabindex="-1"></a>          <span class="at">system_prompt =</span> stats_bot_simple,</span>
<span id="cb11-5"><a href="llm-prompting-fundamentals.html#cb11-5" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.7-sonnet&quot;</span>,</span>
<span id="cb11-6"><a href="llm-prompting-fundamentals.html#cb11-6" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">1000</span>)</span>
<span id="cb11-7"><a href="llm-prompting-fundamentals.html#cb11-7" tabindex="-1"></a>    )</span>
<span id="cb11-8"><a href="llm-prompting-fundamentals.html#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="llm-prompting-fundamentals.html#cb11-9" tabindex="-1"></a>response <span class="ot">&lt;-</span> chat_stats<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;</span>)</span>
<span id="cb11-10"><a href="llm-prompting-fundamentals.html#cb11-10" tabindex="-1"></a>response</span></code></pre></div>
<p>That worked for me. Hopefully it works for you too?</p>
<p>Test that to see if it gives more reliable examples. We have to be a bit careful with choosing examples as these are for formatting, not for modelling. For instance, we don’t want to inadventantly suggest the LLM always recommend a particular statistical model type. ’</p>
</div>
<div id="making-a-simple-tool" class="section level3 hasAnchor" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Making a simple tool<a href="llm-prompting-fundamentals.html#making-a-simple-tool" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Tools are algorithms your assistant can use to perform tasks on your computer or a server. They are what turns a chatbot that can only respond in text into an Assistant that can write code that takes actions.</p>
<div class="tip">
<p><strong>Tip:</strong> You might also see the term ‘MCP’ (Model Context Protocol) used in association with LLM assistants. MCP is an interoperable standard for developing tools. This means a tool I write is consistent with other people’s tools. This helps developers and LLMs read and interpret tools. It is also important to allow LLM to use multiple tools at the same time. ’</p>
</div>
<p>If the request to use <code></code> tags works we can then parse the responses and save them as a file, ready to run. First create an R function that can parse responses and search for <code></code> tags (you can cut and paste my function or try using copilot to make one for you, it’s good at this type of tedious task):</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="llm-prompting-fundamentals.html#cb12-1" tabindex="-1"></a>write_response <span class="ot">&lt;-</span> <span class="cf">function</span>(response, <span class="at">filename =</span> <span class="st">&quot;rscript.R&quot;</span>){</span>
<span id="cb12-2"><a href="llm-prompting-fundamentals.html#cb12-2" tabindex="-1"></a>  <span class="co"># Extract the text between &lt;code&gt; and &lt;/code&gt; tags using regex</span></span>
<span id="cb12-3"><a href="llm-prompting-fundamentals.html#cb12-3" tabindex="-1"></a>  <span class="co"># Using dotall mode (?s) to match across multiple lines</span></span>
<span id="cb12-4"><a href="llm-prompting-fundamentals.html#cb12-4" tabindex="-1"></a>  code_pattern <span class="ot">&lt;-</span> <span class="st">&quot;(?s)&lt;code&gt;(.*?)&lt;/code&gt;&quot;</span></span>
<span id="cb12-5"><a href="llm-prompting-fundamentals.html#cb12-5" tabindex="-1"></a>  code_matches <span class="ot">&lt;-</span> <span class="fu">regmatches</span>(response, <span class="fu">gregexpr</span>(code_pattern, response, <span class="at">perl =</span> <span class="cn">TRUE</span>))</span>
<span id="cb12-6"><a href="llm-prompting-fundamentals.html#cb12-6" tabindex="-1"></a>  </span>
<span id="cb12-7"><a href="llm-prompting-fundamentals.html#cb12-7" tabindex="-1"></a>  <span class="co"># Flatten the list and remove the tags</span></span>
<span id="cb12-8"><a href="llm-prompting-fundamentals.html#cb12-8" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">length</span>(code_matches) <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> <span class="fu">length</span>(code_matches[[<span class="dv">1</span>]]) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb12-9"><a href="llm-prompting-fundamentals.html#cb12-9" tabindex="-1"></a>    code_texts <span class="ot">&lt;-</span> <span class="fu">gsub</span>(<span class="st">&quot;&lt;code&gt;|&lt;/code&gt;&quot;</span>, <span class="st">&quot;&quot;</span>, code_matches[[<span class="dv">1</span>]])</span>
<span id="cb12-10"><a href="llm-prompting-fundamentals.html#cb12-10" tabindex="-1"></a>    </span>
<span id="cb12-11"><a href="llm-prompting-fundamentals.html#cb12-11" tabindex="-1"></a>    <span class="co"># Write all extracted code to the file</span></span>
<span id="cb12-12"><a href="llm-prompting-fundamentals.html#cb12-12" tabindex="-1"></a>    <span class="fu">writeLines</span>(code_texts, filename)</span>
<span id="cb12-13"><a href="llm-prompting-fundamentals.html#cb12-13" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Code written to&quot;</span>, filename, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb12-14"><a href="llm-prompting-fundamentals.html#cb12-14" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb12-15"><a href="llm-prompting-fundamentals.html#cb12-15" tabindex="-1"></a>    <span class="fu">warning</span>(<span class="st">&quot;No code tags found in the response&quot;</span>)</span>
<span id="cb12-16"><a href="llm-prompting-fundamentals.html#cb12-16" tabindex="-1"></a>  }</span>
<span id="cb12-17"><a href="llm-prompting-fundamentals.html#cb12-17" tabindex="-1"></a>}</span></code></pre></div>
<p>Now we can set-up our R code to write any suggestions to file:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="llm-prompting-fundamentals.html#cb13-1" tabindex="-1"></a>chat_stats <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb13-2"><a href="llm-prompting-fundamentals.html#cb13-2" tabindex="-1"></a>          <span class="at">system_prompt =</span> stats_bot,</span>
<span id="cb13-3"><a href="llm-prompting-fundamentals.html#cb13-3" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.7-sonnet&quot;</span>,</span>
<span id="cb13-4"><a href="llm-prompting-fundamentals.html#cb13-4" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">400</span>)</span>
<span id="cb13-5"><a href="llm-prompting-fundamentals.html#cb13-5" tabindex="-1"></a>    )</span>
<span id="cb13-6"><a href="llm-prompting-fundamentals.html#cb13-6" tabindex="-1"></a></span>
<span id="cb13-7"><a href="llm-prompting-fundamentals.html#cb13-7" tabindex="-1"></a>response <span class="ot">&lt;-</span> chat_stats<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;What code can I use to regress temperature against fish_size, fish_size and temperature are both continuous real numbers. Include in your response code to simulate some example data and code to show how to do the regression. &quot;</span>)</span>
<span id="cb13-8"><a href="llm-prompting-fundamentals.html#cb13-8" tabindex="-1"></a><span class="fu">write_response</span>(response, <span class="at">filename =</span> <span class="st">&quot;attempt1.R&quot;</span>)</span></code></pre></div>
<p>Now check the file we just created. If it looks ok then try running it:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="llm-prompting-fundamentals.html#cb14-1" tabindex="-1"></a><span class="co">#CHECK THE FILE before running!!!!</span></span>
<span id="cb14-2"><a href="llm-prompting-fundamentals.html#cb14-2" tabindex="-1"></a><span class="fu">source</span>(<span class="st">&quot;attempt1.R&quot;</span>)</span></code></pre></div>
<p>The <code>source()</code> command will attempt to run the R code that was created. If it works you should see some plots pop up.</p>
</div>
<div id="summary-of-agents-and-tools" class="section level3 hasAnchor" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Summary of agents and tools<a href="llm-prompting-fundamentals.html#summary-of-agents-and-tools" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We’ve implemented a rudimentary tool and the beginnings of an LLM agent. Going forwards we’ll used specialised software for these tasks. You won’t need to code them yourself. But coding this examples is helpful to understand the fundamentals of how they work. To recap, the basic workflow an agent follows is:</p>
<ol style="list-style-type: decimal">
<li>Set-up a system prompt with detailed instructions for how the LLM should format responses</li>
<li>User asks a question that is sent to the LLM</li>
<li>LLM responds and sends response back to user</li>
<li>Software on user’s computer attempts to parse and act on the response according to pre-determined rules</li>
<li>User’s computers enacts the commands in the response and provides results to user</li>
</ol>
<p>Tools like Copilot Agent mode then go a step further and send the results of step 5 back to the LLM, which then interprets the results and the loop continues (sometimes with and sometimes without direct user approval).</p>
<p>If you want to go further with making your own tools, then I suggest you check out <code>ellmer</code> package. It supports tool creation in a structured way. For instance, I made a tool that allows an <a href="https://ellmer.tidyverse.org/articles/tool-calling.html">LLM to download and save ocean data to your computer</a>.</p>
</div>
</div>
<div id="reflection-on-prompting-fundamentals" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Reflection on prompting fundamentals<a href="llm-prompting-fundamentals.html#reflection-on-prompting-fundamentals" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The key things I hoped you learnt from this lesson are:</p>
<ul>
<li>Basic LLM jargon, including tokens, temperature, API access and different LLM models.</li>
<li>Some different prompt strategies, including role prompting, emphasis, chain of thought and one-shot.</li>
<li>The fundamentals of tool use and agents.</li>
</ul>
<p>Now you understand the basics, let’s get into Github Copilot.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-llms-for-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="github-copilot-for-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-llm-prompting-fundamentals.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
