<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models</title>
  <meta name="description" content="1-day workshop for using large language models to generate R code." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="1-day workshop for using large language models to generate R code." />
  <meta name="github-repo" content="cbrown5/R-llm-workshop" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 LLM prompting fundamentals | Quality R analysis with large language models" />
  
  <meta name="twitter:description" content="1-day workshop for using large language models to generate R code." />
  

<meta name="author" content="CJ Brown (c.j.brown@utas.edu.au)" />


<meta name="date" content="2025-05-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-llms-for-r.html"/>
<link rel="next" href="ethics-and-copyright.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quality R analysis with large language models</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Quality R analysis with large language models</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#summary"><i class="fa fa-check"></i><b>1.1</b> Summary</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#about-chris"><i class="fa fa-check"></i><b>1.2</b> About Chris</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#course-outline"><i class="fa fa-check"></i><b>1.3</b> Course outline</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#software-youll-need-for-this-workshop"><i class="fa fa-check"></i><b>1.4</b> Software you’ll need for this workshop</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#optional-software"><i class="fa fa-check"></i><b>1.4.1</b> Optional software</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#software-licenses"><i class="fa fa-check"></i><b>1.5</b> Software licenses</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#r-packages-youll-need-for-this-workshop"><i class="fa fa-check"></i><b>1.6</b> R packages you’ll need for this workshop</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#data"><i class="fa fa-check"></i><b>1.7</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-llms-for-r.html"><a href="introduction-to-llms-for-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to LLMs for R</a></li>
<li class="chapter" data-level="3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html"><i class="fa fa-check"></i><b>3</b> LLM prompting fundamentals</a>
<ul>
<li class="chapter" data-level="3.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#setup-authorisation"><i class="fa fa-check"></i><b>3.1</b> Setup authorisation</a></li>
<li class="chapter" data-level="3.2" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#understanding-how-llms-work"><i class="fa fa-check"></i><b>3.2</b> Understanding how LLMs work</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#token-by-token-prediction"><i class="fa fa-check"></i><b>3.2.1</b> Token-by-token prediction</a></li>
<li class="chapter" data-level="3.2.2" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#temperature-effects"><i class="fa fa-check"></i><b>3.2.2</b> Temperature effects</a></li>
<li class="chapter" data-level="3.2.3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#comparing-model-complexity"><i class="fa fa-check"></i><b>3.2.3</b> Comparing model complexity</a></li>
<li class="chapter" data-level="3.2.4" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#understanding-context-windows"><i class="fa fa-check"></i><b>3.2.4</b> Understanding context windows</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#diy-stats-bot"><i class="fa fa-check"></i><b>3.3</b> DIY stats bot</a></li>
<li class="chapter" data-level="3.4" data-path="llm-prompting-fundamentals.html"><a href="llm-prompting-fundamentals.html#advanced-prompting"><i class="fa fa-check"></i><b>3.4</b> Advanced prompting</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html"><i class="fa fa-check"></i><b>4</b> Ethics and copyright</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#model-biases"><i class="fa fa-check"></i><b>4.1</b> Model biases</a></li>
<li class="chapter" data-level="4.2" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#energy-use"><i class="fa fa-check"></i><b>4.2</b> Energy use</a></li>
<li class="chapter" data-level="4.3" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#copyright"><i class="fa fa-check"></i><b>4.3</b> Copyright</a></li>
<li class="chapter" data-level="4.4" data-path="ethics-and-copyright.html"><a href="ethics-and-copyright.html#managing-data-privacy"><i class="fa fa-check"></i><b>4.4</b> Managing data privacy</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="github-copilot-for-r.html"><a href="github-copilot-for-r.html"><i class="fa fa-check"></i><b>5</b> Github copilot for R</a></li>
<li class="chapter" data-level="6" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html"><i class="fa fa-check"></i><b>6</b> Best practices project setup</a>
<ul>
<li class="chapter" data-level="6.1" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#project-organization"><i class="fa fa-check"></i><b>6.1</b> Project organization</a></li>
<li class="chapter" data-level="6.2" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#version-control-with-git"><i class="fa fa-check"></i><b>6.2</b> Version control with Git</a></li>
<li class="chapter" data-level="6.3" data-path="best-practices-project-setup.html"><a href="best-practices-project-setup.html#r-project-setup"><i class="fa fa-check"></i><b>6.3</b> R project setup</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="inline-code-editing.html"><a href="inline-code-editing.html"><i class="fa fa-check"></i><b>7</b> Inline code editing</a>
<ul>
<li class="chapter" data-level="7.1" data-path="inline-code-editing.html"><a href="inline-code-editing.html#code-completion"><i class="fa fa-check"></i><b>7.1</b> 1. Code completion</a></li>
<li class="chapter" data-level="7.2" data-path="inline-code-editing.html"><a href="inline-code-editing.html#inline-code-generation"><i class="fa fa-check"></i><b>7.2</b> 2. Inline code generation</a></li>
<li class="chapter" data-level="7.3" data-path="inline-code-editing.html"><a href="inline-code-editing.html#understanding-inline-suggestions"><i class="fa fa-check"></i><b>7.3</b> Understanding inline suggestions</a></li>
<li class="chapter" data-level="7.4" data-path="inline-code-editing.html"><a href="inline-code-editing.html#guiding-copilot-with-comments"><i class="fa fa-check"></i><b>7.4</b> Guiding Copilot with comments</a></li>
<li class="chapter" data-level="7.5" data-path="inline-code-editing.html"><a href="inline-code-editing.html#common-patterns-for-r-programming"><i class="fa fa-check"></i><b>7.5</b> Common patterns for R programming</a></li>
<li class="chapter" data-level="7.6" data-path="inline-code-editing.html"><a href="inline-code-editing.html#troubleshooting-and-refinement"><i class="fa fa-check"></i><b>7.6</b> Troubleshooting and refinement</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="planning-your-project-with-ask-mode.html"><a href="planning-your-project-with-ask-mode.html"><i class="fa fa-check"></i><b>8</b> Planning your project with Ask mode</a>
<ul>
<li class="chapter" data-level="8.1" data-path="planning-your-project-with-ask-mode.html"><a href="planning-your-project-with-ask-mode.html#introduction-to-ask-mode"><i class="fa fa-check"></i><b>8.1</b> Introduction to Ask mode</a></li>
<li class="chapter" data-level="8.2" data-path="planning-your-project-with-ask-mode.html"><a href="planning-your-project-with-ask-mode.html#project-planning-with-ask-mode"><i class="fa fa-check"></i><b>8.2</b> Project planning with Ask mode</a></li>
<li class="chapter" data-level="8.3" data-path="planning-your-project-with-ask-mode.html"><a href="planning-your-project-with-ask-mode.html#asking-effective-questions"><i class="fa fa-check"></i><b>8.3</b> Asking effective questions</a></li>
<li class="chapter" data-level="8.4" data-path="planning-your-project-with-ask-mode.html"><a href="planning-your-project-with-ask-mode.html#implementing-ask-mode-suggestions"><i class="fa fa-check"></i><b>8.4</b> Implementing Ask mode suggestions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html"><i class="fa fa-check"></i><b>9</b> Creating your code with Edit mode</a>
<ul>
<li class="chapter" data-level="9.1" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#understanding-edit-mode"><i class="fa fa-check"></i><b>9.1</b> Understanding Edit mode</a></li>
<li class="chapter" data-level="9.2" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#code-generation-strategies"><i class="fa fa-check"></i><b>9.2</b> Code generation strategies</a></li>
<li class="chapter" data-level="9.3" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#code-transformation-techniques"><i class="fa fa-check"></i><b>9.3</b> Code transformation techniques</a></li>
<li class="chapter" data-level="9.4" data-path="creating-your-code-with-edit-mode.html"><a href="creating-your-code-with-edit-mode.html#quality-control-and-refinement"><i class="fa fa-check"></i><b>9.4</b> Quality control and refinement</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html"><i class="fa fa-check"></i><b>10</b> Automated workflows with Agent mode</a>
<ul>
<li class="chapter" data-level="10.1" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#introduction-to-agent-mode"><i class="fa fa-check"></i><b>10.1</b> Introduction to Agent mode</a></li>
<li class="chapter" data-level="10.2" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#creating-automated-workflows"><i class="fa fa-check"></i><b>10.2</b> Creating automated workflows</a></li>
<li class="chapter" data-level="10.3" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#common-r-workflow-applications"><i class="fa fa-check"></i><b>10.3</b> Common R workflow applications</a></li>
<li class="chapter" data-level="10.4" data-path="automated-workflows-with-agent-mode.html"><a href="automated-workflows-with-agent-mode.html#troubleshooting-and-optimization"><i class="fa fa-check"></i><b>10.4</b> Troubleshooting and optimization</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html"><i class="fa fa-check"></i><b>11</b> Advanced LLM agents</a>
<ul>
<li class="chapter" data-level="11.1" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#roo-code"><i class="fa fa-check"></i><b>11.1</b> Roo code</a></li>
<li class="chapter" data-level="11.2" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#introduction-to-roo-code"><i class="fa fa-check"></i><b>11.2</b> Introduction to Roo code</a></li>
<li class="chapter" data-level="11.3" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#autonomous-r-programming-with-roo"><i class="fa fa-check"></i><b>11.3</b> Autonomous R programming with Roo</a></li>
<li class="chapter" data-level="11.4" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#advanced-use-cases-for-r"><i class="fa fa-check"></i><b>11.4</b> Advanced use cases for R</a></li>
<li class="chapter" data-level="11.5" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#best-practices-and-workflow-integration"><i class="fa fa-check"></i><b>11.5</b> Best practices and workflow integration</a></li>
<li class="chapter" data-level="11.6" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#evolution-of-llm-agents"><i class="fa fa-check"></i><b>11.6</b> Evolution of LLM agents</a></li>
<li class="chapter" data-level="11.7" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#agent-capabilities-for-r"><i class="fa fa-check"></i><b>11.7</b> Agent capabilities for R</a></li>
<li class="chapter" data-level="11.8" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#integration-with-r-workflows"><i class="fa fa-check"></i><b>11.8</b> Integration with R workflows</a></li>
<li class="chapter" data-level="11.9" data-path="advanced-llm-agents.html"><a href="advanced-llm-agents.html#practical-considerations"><i class="fa fa-check"></i><b>11.9</b> Practical considerations</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="cost-and-security.html"><a href="cost-and-security.html"><i class="fa fa-check"></i><b>12</b> Cost and security</a>
<ul>
<li class="chapter" data-level="12.1" data-path="cost-and-security.html"><a href="cost-and-security.html#cost-considerations"><i class="fa fa-check"></i><b>12.1</b> Cost considerations</a></li>
<li class="chapter" data-level="12.2" data-path="cost-and-security.html"><a href="cost-and-security.html#security-implications"><i class="fa fa-check"></i><b>12.2</b> Security implications</a></li>
<li class="chapter" data-level="12.3" data-path="cost-and-security.html"><a href="cost-and-security.html#best-practices-for-secure-usage"><i class="fa fa-check"></i><b>12.3</b> Best practices for secure usage</a></li>
<li class="chapter" data-level="12.4" data-path="cost-and-security.html"><a href="cost-and-security.html#institutional-considerations"><i class="fa fa-check"></i><b>12.4</b> Institutional considerations</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>13</b> Conclusion</a>
<ul>
<li class="chapter" data-level="13.1" data-path="conclusion.html"><a href="conclusion.html#workshop-summary"><i class="fa fa-check"></i><b>13.1</b> Workshop summary</a></li>
<li class="chapter" data-level="13.2" data-path="conclusion.html"><a href="conclusion.html#the-changing-landscape-of-scientific-computing"><i class="fa fa-check"></i><b>13.2</b> The changing landscape of scientific computing</a></li>
<li class="chapter" data-level="13.3" data-path="conclusion.html"><a href="conclusion.html#developing-community-standards"><i class="fa fa-check"></i><b>13.3</b> Developing community standards</a></li>
<li class="chapter" data-level="13.4" data-path="conclusion.html"><a href="conclusion.html#future-directions"><i class="fa fa-check"></i><b>13.4</b> Future directions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quality R analysis with large language models</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="llm-prompting-fundamentals" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Chapter 3</span> LLM prompting fundamentals<a href="llm-prompting-fundamentals.html#llm-prompting-fundamentals" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Start of practical material</em></p>
<p>We’ll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you’ll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you’re not getting the results you expect.</p>
<p><strong>Software requirements:</strong> VScode with R or Rstudio, <a href="https://ellmer.tidyverse.org/">ellmer package</a>, API license.</p>
<div id="setup-authorisation" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Setup authorisation<a href="llm-prompting-fundamentals.html#setup-authorisation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>First, you need to get an API key from the provider. Login to the provider’s website and
follow the instructions.</p>
<p>Then, you need to add the key to your <code>.Renviron</code> file:</p>
<p><code>usethis::edit_r_environ()</code></p>
<p>Then type in your key like this:</p>
<p><code>OPENROUTER_API_KEY="xxxxxx"</code></p>
<p>Then restart R. <code>ellmer</code> will automatically find your key so long as you use the recommended envirment variable names.
See <code>?ellmer::chat_openrouter</code> (or <code>chat_xxx</code> where xxx is whatever provider you are using).</p>
</div>
<div id="understanding-how-llms-work" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Understanding how LLMs work<a href="llm-prompting-fundamentals.html#understanding-how-llms-work" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we’ll use the <code>ellmer</code> package to demonstrate some fundamental concepts.</p>
<p>First, let’s set up our environment and create a connection to an LLM.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="llm-prompting-fundamentals.html#cb1-1" tabindex="-1"></a><span class="fu">library</span>(ellmer)</span>
<span id="cb1-2"><a href="llm-prompting-fundamentals.html#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="llm-prompting-fundamentals.html#cb1-3" tabindex="-1"></a><span class="co"># Initialize a chat with Claude</span></span>
<span id="cb1-4"><a href="llm-prompting-fundamentals.html#cb1-4" tabindex="-1"></a>chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb1-5"><a href="llm-prompting-fundamentals.html#cb1-5" tabindex="-1"></a>  <span class="at">system_prompt =</span> <span class="st">&quot;&quot;</span>,</span>
<span id="cb1-6"><a href="llm-prompting-fundamentals.html#cb1-6" tabindex="-1"></a>  <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb1-7"><a href="llm-prompting-fundamentals.html#cb1-7" tabindex="-1"></a>  <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>)</span>
<span id="cb1-8"><a href="llm-prompting-fundamentals.html#cb1-8" tabindex="-1"></a>)</span>
<span id="cb1-9"><a href="llm-prompting-fundamentals.html#cb1-9" tabindex="-1"></a>chat<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Ecologists like to eat &quot;</span>)</span></code></pre></div>
<p>Notice that the model doesn’t do what we intend, which is complete the sentence. LLMs have a built in Let’s use the ‘system prompt’ to provide it with strong directions.</p>
<div class="tip">
<p><strong>Tip:</strong> The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, <a href="https://www.reddit.com/r/ClaudeAI/comments/1ixapi4/here_is_claude_sonnet_37_full_system_prompt/">here’s the system prompt for the chat interface version of anthropic (Claude)</a></p>
</div>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="llm-prompting-fundamentals.html#cb2-1" tabindex="-1"></a>chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb2-2"><a href="llm-prompting-fundamentals.html#cb2-2" tabindex="-1"></a>  <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb2-3"><a href="llm-prompting-fundamentals.html#cb2-3" tabindex="-1"></a>  <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb2-4"><a href="llm-prompting-fundamentals.html#cb2-4" tabindex="-1"></a><span class="co">#   model = &quot;anthropic/claude-3.7-sonnet&quot;,</span></span>
<span id="cb2-5"><a href="llm-prompting-fundamentals.html#cb2-5" tabindex="-1"></a>  <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>)</span>
<span id="cb2-6"><a href="llm-prompting-fundamentals.html#cb2-6" tabindex="-1"></a>)</span>
<span id="cb2-7"><a href="llm-prompting-fundamentals.html#cb2-7" tabindex="-1"></a>chat<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Ecologists like to eat &quot;</span>)</span></code></pre></div>
<div class="tip">
<p><strong>Tip:</strong> It is generally more effective to tell the LLM <strong>what to do</strong> rather than <strong>what not to do</strong> (just like people!).</p>
</div>
<div id="token-by-token-prediction" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Token-by-token prediction<a href="llm-prompting-fundamentals.html#token-by-token-prediction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LLMs don’t understand text as complete sentences or concepts; they predict one token at a time based on the patterns they’ve learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.</p>
<p>Let’s see this in action by generating text token by token with a very limited number of tokens:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="llm-prompting-fundamentals.html#cb3-1" tabindex="-1"></a> prompt <span class="ot">&lt;-</span> <span class="st">&quot;Ecologists like to eat&quot;</span></span>
<span id="cb3-2"><a href="llm-prompting-fundamentals.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="llm-prompting-fundamentals.html#cb3-3" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>) {</span>
<span id="cb3-4"><a href="llm-prompting-fundamentals.html#cb3-4" tabindex="-1"></a>        single_token_chat <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb3-5"><a href="llm-prompting-fundamentals.html#cb3-5" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb3-6"><a href="llm-prompting-fundamentals.html#cb3-6" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb3-7"><a href="llm-prompting-fundamentals.html#cb3-7" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">1</span>)</span>
<span id="cb3-8"><a href="llm-prompting-fundamentals.html#cb3-8" tabindex="-1"></a>    )</span>
<span id="cb3-9"><a href="llm-prompting-fundamentals.html#cb3-9" tabindex="-1"></a>    response <span class="ot">&lt;-</span> single_token_chat<span class="sc">$</span><span class="fu">chat</span>(prompt)</span>
<span id="cb3-10"><a href="llm-prompting-fundamentals.html#cb3-10" tabindex="-1"></a>    prompt <span class="ot">&lt;-</span> <span class="fu">paste</span>(prompt, response, <span class="at">sep =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb3-11"><a href="llm-prompting-fundamentals.html#cb3-11" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Token&quot;</span>, i, <span class="st">&quot;:&quot;</span>, response, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb3-12"><a href="llm-prompting-fundamentals.html#cb3-12" tabindex="-1"></a>}</span>
<span id="cb3-13"><a href="llm-prompting-fundamentals.html#cb3-13" tabindex="-1"></a></span>
<span id="cb3-14"><a href="llm-prompting-fundamentals.html#cb3-14" tabindex="-1"></a><span class="co"># Display the complete sequence</span></span>
<span id="cb3-15"><a href="llm-prompting-fundamentals.html#cb3-15" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Full sequence:&quot;</span>, prompt, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context.</p>
</div>
<div id="temperature-effects" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Temperature effects<a href="llm-prompting-fundamentals.html#temperature-effects" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The “temperature” parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.</p>
<p>Let’s compare responses with different temperatures:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="llm-prompting-fundamentals.html#cb4-1" tabindex="-1"></a><span class="co"># Create chats with different temperature settings</span></span>
<span id="cb4-2"><a href="llm-prompting-fundamentals.html#cb4-2" tabindex="-1"></a>chat_temp <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb4-3"><a href="llm-prompting-fundamentals.html#cb4-3" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb4-4"><a href="llm-prompting-fundamentals.html#cb4-4" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb4-5"><a href="llm-prompting-fundamentals.html#cb4-5" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>, <span class="at">temperature =</span> <span class="dv">0</span>)</span>
<span id="cb4-6"><a href="llm-prompting-fundamentals.html#cb4-6" tabindex="-1"></a>    )</span>
<span id="cb4-7"><a href="llm-prompting-fundamentals.html#cb4-7" tabindex="-1"></a></span>
<span id="cb4-8"><a href="llm-prompting-fundamentals.html#cb4-8" tabindex="-1"></a>chat_temp<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Marine ecologists like to eat &quot;</span>)</span>
<span id="cb4-9"><a href="llm-prompting-fundamentals.html#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="llm-prompting-fundamentals.html#cb4-10" tabindex="-1"></a>chat_temp <span class="ot">&lt;-</span> <span class="fu">chat_openrouter</span>(</span>
<span id="cb4-11"><a href="llm-prompting-fundamentals.html#cb4-11" tabindex="-1"></a>          <span class="at">system_prompt =</span> <span class="st">&quot;Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don&#39;t provide any explanation, don&#39;t reiterate the text the user provides&quot;</span>,</span>
<span id="cb4-12"><a href="llm-prompting-fundamentals.html#cb4-12" tabindex="-1"></a>        <span class="at">model =</span> <span class="st">&quot;anthropic/claude-3.5-haiku&quot;</span>,</span>
<span id="cb4-13"><a href="llm-prompting-fundamentals.html#cb4-13" tabindex="-1"></a>        <span class="at">api_args =</span> <span class="fu">list</span>(<span class="at">max_tokens =</span> <span class="dv">50</span>, <span class="at">temperature =</span> <span class="dv">2</span>)</span>
<span id="cb4-14"><a href="llm-prompting-fundamentals.html#cb4-14" tabindex="-1"></a>    )</span>
<span id="cb4-15"><a href="llm-prompting-fundamentals.html#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="llm-prompting-fundamentals.html#cb4-16" tabindex="-1"></a>chat_temp<span class="sc">$</span><span class="fu">chat</span>(<span class="st">&quot;Marine ecologists like to eat &quot;</span>)</span></code></pre></div>
<p>At low temperatures, you’ll notice the model consistently produces similar “safe” completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.</p>
</div>
<div id="comparing-model-complexity" class="section level3 hasAnchor" number="3.2.3">
<h3><span class="header-section-number">3.2.3</span> Comparing model complexity<a href="llm-prompting-fundamentals.html#comparing-model-complexity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Different models have different capabilities based on their size, training data, and architecture.</p>
<p>For example <code>anthropic/claude-3.5-haiku</code> has many fewer parameters than <code>anthropic/claude-3.7-sonnet</code>. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15</p>
<p>For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot.</p>
</div>
<div id="understanding-context-windows" class="section level3 hasAnchor" number="3.2.4">
<h3><span class="header-section-number">3.2.4</span> Understanding context windows<a href="llm-prompting-fundamentals.html#understanding-context-windows" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>LLMs have a limited “context window” - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google’s models have up to 1 million tokens.</p>
<p>We’ll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don’t come close to using up the context window.</p>
</div>
</div>
<div id="diy-stats-bot" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> DIY stats bot<a href="llm-prompting-fundamentals.html#diy-stats-bot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>UP TO HERE</p>
</div>
<div id="advanced-prompting" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Advanced prompting<a href="llm-prompting-fundamentals.html#advanced-prompting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The point of this section is to show you how github copilot works. Also ellmer has a tools functionality that you can explore on your own.</p>
<p>Keep working with our stats bot. Make a tool: Get it to output code that can be parsed (e.g. using markdown).
Write an R function that parses and runs the code.</p>
<p>Compare different models for accuracy at following the instructions.</p>
<p>Show one shot and few shot examples.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-llms-for-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ethics-and-copyright.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-llm-prompting-fundamentals.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
