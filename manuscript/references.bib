@article{fraser2018,
    doi = {10.1371/journal.pone.0200303},
    author = {Fraser, Hannah AND Parker, Tim AND Nakagawa, Shinichi AND Barnett, Ashley AND Fidler, Fiona},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Questionable research practices in ecology and evolution},
    year = {2018},
    month = {07},
    volume = {13},
    url = {https://doi.org/10.1371/journal.pone.0200303},
    pages = {1-16},
    abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
    number = {7},

}

@article{brown2011,
author = {Brown, Christopher J. and Schoeman, David S. and Sydeman, William J. and Brander, Keith and Buckley, Lauren B. and Burrows, Michael and Duarte, Carlos M. and Moore, Pippa J. and Pandolfi, John M. and Poloczanska, Elvira and Venables, William and Richardson, Anthony J.},
title = {Quantitative approaches in climate change ecology},
journal = {Global Change Biology},
volume = {17},
number = {12},
pages = {3697-3713},
doi = {https://doi.org/10.1111/j.1365-2486.2011.02531.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1365-2486.2011.02531.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1365-2486.2011.02531.x},
abstract = {Abstract Contemporary impacts of anthropogenic climate change on ecosystems are increasingly being recognized. Documenting the extent of these impacts requires quantitative tools for analyses of ecological observations to distinguish climate impacts in noisy data and to understand interactions between climate variability and other drivers of change. To assist the development of reliable statistical approaches, we review the marine climate change literature and provide suggestions for quantitative approaches in climate change ecology. We compiled 267 peer-reviewed articles that examined relationships between climate change and marine ecological variables. Of the articles with time series data (n = 186), 75\% used statistics to test for a dependency of ecological variables on climate variables. We identified several common weaknesses in statistical approaches, including marginalizing other important non-climate drivers of change, ignoring temporal and spatial autocorrelation, averaging across spatial patterns and not reporting key metrics. We provide a list of issues that need to be addressed to make inferences more defensible, including the consideration of (i) data limitations and the comparability of data sets; (ii) alternative mechanisms for change; (iii) appropriate response variables; (iv) a suitable model for the process under study; (v) temporal autocorrelation; (vi) spatial autocorrelation and patterns; and (vii) the reporting of rates of change. While the focus of our review was marine studies, these suggestions are equally applicable to terrestrial studies. Consideration of these suggestions will help advance global knowledge of climate impacts and understanding of the processes driving ecological change.},
year = {2011}
}

@article{forstmeier2017,
author = {Forstmeier, Wolfgang and Wagenmakers, Eric-Jan and Parker, Timothy H.},
title = {Detecting and avoiding likely false-positive findings – a practical guide},
journal = {Biological Reviews},
volume = {92},
number = {4},
pages = {1941-1968},
keywords = {confirmation bias, HARKing, hindsight bias, overfitting, P-hacking, power, preregistration, replication, researcher degrees of freedom, Type I error},
doi = {https://doi.org/10.1111/brv.12315},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12315},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/brv.12315},
abstract = {ABSTRACT Recently there has been a growing concern that many published research findings do not hold up in attempts to replicate them. We argue that this problem may originate from a culture of ‘you can publish if you found a significant effect’. This culture creates a systematic bias against the null hypothesis which renders meta-analyses questionable and may even lead to a situation where hypotheses become difficult to falsify. In order to pinpoint the sources of error and possible solutions, we review current scientific practices with regard to their effect on the probability of drawing a false-positive conclusion. We explain why the proportion of published false-positive findings is expected to increase with (i) decreasing sample size, (ii) increasing pursuit of novelty, (iii) various forms of multiple testing and researcher flexibility, and (iv) incorrect P-values, especially due to unaccounted pseudoreplication, i.e. the non-independence of data points (clustered data). We provide examples showing how statistical pitfalls and psychological traps lead to conclusions that are biased and unreliable, and we show how these mistakes can be avoided. Ultimately, we hope to contribute to a culture of ‘you can publish if your study is rigorous’. To this end, we highlight promising strategies towards making science more objective. Specifically, we enthusiastically encourage scientists to preregister their studies (including a priori hypotheses and complete analysis plans), to blind observers to treatment groups during data collection and analysis, and unconditionally to report all results. Also, we advocate reallocating some efforts away from seeking novelty and discovery and towards replicating important research findings of one's own and of others for the benefit of the scientific community as a whole. We believe these efforts will be aided by a shift in evaluation criteria away from the current system which values metrics of ‘impact’ almost exclusively and towards a system which explicitly values indices of scientific rigour.},
year = {2017}
}

@article{warton2016,
author = {Warton, David I. and Lyons, Mitchell and Stoklosa, Jakub and Ives, Anthony R.},
title = {Three points to consider when choosing a LM or GLM test for count data},
journal = {Methods in Ecology and Evolution},
volume = {7},
number = {8},
pages = {882-890},
keywords = {data transformation, generalized linear models, multivariate analysis, power analysis, type I error},
doi = {https://doi.org/10.1111/2041-210X.12552},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.12552},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.12552},
abstract = {Summary The two most common approaches for analysing count data are to use a generalized linear model (GLM), or transform data, and use a linear model (LM). The latter has recently been advocated to more reliably maintain control of type I error rates in tests for no association, while seemingly losing little in power. We make three points on this issue. Point 1 – Choice of statistical model should primarily be made on the grounds of data properties. Choice of testing procedure should be considered and addressed as a separate issue, after model choice. If models with the appropriate data properties nonetheless have statistical problems such as type I error control (i.e. type I error rate greatly exceeds the intended significance level), the best solution is to keep the model but fix the problems. Point 2 – When a test has problems with type I error control, it can usually be corrected, but this may require departure from software default approaches. In particular, resampling is a good solution for small samples that can be easy to implement. Point 3 –Tests based on models that better fit the data (e.g. a negative binomial for overdispersed count data) tend to have better power properties and in some instances have considerably higher power. We illustrate these issues for a 2 × 2 experiment with a count response. This seemingly simple problem becomes hard when the experimental design is unbalanced, and software default procedures using LMs or GLMs can have difficulties, although in both cases the issues can be fixed. We conclude that, when GLMs are thought to fit count data well, and when any necessary steps are taken to correct type I error rates, they should be used rather than LMs. Nonetheless, standard LM tests are often robust and can have good type I error control, so there is an argument for their use for counts when diagnostics are difficult and statistical models are complex, although at some risk of loss of power and interpretability.},
year = {2016}
}

@article{ohara2010,
author = {O’Hara, Robert B. and Kotze, D. Johan},
title = {Do not log-transform count data},
journal = {Methods in Ecology and Evolution},
volume = {1},
number = {2},
pages = {118-122},
keywords = {generalized linear models, linear models, overdispersion, Poisson, transformation},
doi = {https://doi.org/10.1111/j.2041-210X.2010.00021.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2010.00021.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2010.00021.x},
abstract = {Summary 1. Ecological count data (e.g. number of individuals or species) are often log-transformed to satisfy parametric test assumptions. 2. Apart from the fact that generalized linear models are better suited in dealing with count data, a log-transformation of counts has the additional quandary in how to deal with zero observations. With just one zero observation (if this observation represents a sampling unit), the whole data set needs to be fudged by adding a value (usually 1) before transformation. 3. Simulating data from a negative binomial distribution, we compared the outcome of fitting models that were transformed in various ways (log, square root) with results from fitting models using quasi-Poisson and negative binomial models to untransformed count data. 4. We found that the transformations performed poorly, except when the dispersion was small and the mean counts were large. The quasi-Poisson and negative binomial models consistently performed well, with little bias. 5. We recommend that count data should not be analysed by log-transforming it, but instead models based on Poisson and negative binomial distributions should be used.},
year = {2010}
}

@article{arif2022,
author = {Arif, Suchinta and MacNeil, M. Aaron},
title = {Predictive models aren't for causal inference},
journal = {Ecology Letters},
volume = {25},
number = {8},
pages = {1741-1745},
keywords = {back-door criterion, causal inference, directed acyclic graphs (DAGs), model selection, prediction},
doi = {https://doi.org/10.1111/ele.14033},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.14033},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ele.14033},
abstract = {Abstract Ecologists often rely on observational data to understand causal relationships. Although observational causal inference methodologies exist, predictive techniques such as model selection based on information criterion (e.g. AIC) remains a common approach used to understand ecological relationships. However, predictive approaches are not appropriate for drawing causal conclusions. Here, we highlight the distinction between predictive and causal inference and show how predictive techniques can lead to biased causal estimates. Instead, we encourage ecologists to valid causal inference methods such as the backdoor criterion, a graphical rule that can be used to determine causal relationships across observational studies.},
year = {2022}
}


@Article{bolker2024,
AUTHOR = {Bolker, Benjamin M.},
TITLE = {Multimodel Approaches Are Not the Best Way to Understand Multifactorial Systems},
JOURNAL = {Entropy},
VOLUME = {26},
YEAR = {2024},
NUMBER = {6},
ARTICLE-NUMBER = {506},
URL = {https://www.mdpi.com/1099-4300/26/6/506},
PubMedID = {38920515},
ISSN = {1099-4300},
ABSTRACT = {Information-theoretic (IT) and multi-model averaging (MMA) statistical approaches are widely used but suboptimal tools for pursuing a multifactorial approach (also known as the method of multiple working hypotheses) in ecology. (1) Conceptually, IT encourages ecologists to perform tests on sets of artificially simplified models. (2) MMA improves on IT model selection by implementing a simple form of shrinkage estimation (a way to make accurate predictions from a model with many parameters relative to the amount of data, by “shrinking” parameter estimates toward zero). However, other shrinkage estimators such as penalized regression or Bayesian hierarchical models with regularizing priors are more computationally efficient and better supported theoretically. (3) In general, the procedures for extracting confidence intervals from MMA are overconfident, providing overly narrow intervals. If researchers want to use limited data sets to accurately estimate the strength of multiple competing ecological processes along with reliable confidence intervals, the current best approach is to use full (maximal) statistical models (possibly with Bayesian priors) after making principled, a priori decisions about model complexity.},
DOI = {10.3390/e26060506}
}

@article{zuur2010,
author = {Zuur, Alain F. and Ieno, Elena N. and Elphick, Chris S.},
title = {A protocol for data exploration to avoid common statistical problems},
journal = {Methods in Ecology and Evolution},
volume = {1},
number = {1},
pages = {3-14},
keywords = {collinearity, data exploration, independence, transformations, type I and II errors, zero inflation},
doi = {https://doi.org/10.1111/j.2041-210X.2009.00001.x},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/j.2041-210X.2009.00001.x},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2041-210X.2009.00001.x},
abstract = {Summary 1. While teaching statistics to ecologists, the lead authors of this paper have noticed common statistical problems. If a random sample of their work (including scientific papers) produced before doing these courses were selected, half would probably contain violations of the underlying assumptions of the statistical techniques employed. 2. Some violations have little impact on the results or ecological conclusions; yet others increase type I or type II errors, potentially resulting in wrong ecological conclusions. Most of these violations can be avoided by applying better data exploration. These problems are especially troublesome in applied ecology, where management and policy decisions are often at stake. 3. Here, we provide a protocol for data exploration; discuss current tools to detect outliers, heterogeneity of variance, collinearity, dependence of observations, problems with interactions, double zeros in multivariate analysis, zero inflation in generalized linear modelling, and the correct type of relationships between dependent and independent variables; and provide advice on how to address these problems when they arise. We also address misconceptions about normality, and provide advice on data transformations. 4. Data exploration avoids type I and type II errors, among other problems, thereby reducing the chance of making wrong ecological conclusions and poor recommendations. It is therefore essential for good quality management and policy based on statistical analyses.},
year = {2010}
}

@book{mollick2025,
  title     = "Co-Intelligence: Living and Working with AI",
  author    = "Mollick, Ethan",
  year      = 2025,
  publisher = "Penguin Random House",
  address   = "London"
}

@article{zhu2024large,
  title={Are Large Language Models Good Statisticians?},
  author={Zhu, Yizhang and Du, Shiyin and Li, Boyan and Luo, Yuyu and Tang, Nan},
  journal={arXiv preprint arXiv:2406.07815},
  year={2024}
}



@article{gougherty2024,
    author = {Gougherty, Andrew V. and Clipp, Hannah L.},
    title = {Testing the reliability of an AI-based large language model to extract ecological information from the scientific literature},
    journal = {npj Biodiversity},
    volume = {3},
    number = {1},
    pages = {13},
    year = {2024},
    month = {5},
    doi = {10.1038/s44185-024-00043-9},
    url = {https://doi.org/10.1038/s44185-024-00043-9},
    abstract = {Artificial intelligence-based large language models (LLMs) have the potential to substantially improve the efficiency and scale of ecological research, but their propensity for delivering incorrect information raises significant concern about their usefulness in their current state. Here, we formally test how quickly and accurately an LLM performs in comparison to a human reviewer when tasked with extracting various types of ecological data from the scientific literature. We found the LLM was able to extract relevant data over 50 times faster than the reviewer and had very high accuracy (>90%) in extracting discrete and categorical data, but it performed poorly when extracting certain quantitative data. Our case study shows that LLMs offer great potential for generating large ecological databases at unprecedented speed and scale, but additional quality assurance steps are required to ensure data integrity.},
    issn = {2731-4243}
}

@article{spillias2024,
    title = {Human-AI collaboration to identify literature for evidence synthesis},
    author = {Spillias, Scott and Tuohy, Paris and Andreotta, Matthew and Annand-Jones, Ruby and Boschetti, Fabio and Cvitanovic, Christopher and Duggan, Joseph and Fulton, Elisabeth A. and Karcher, Denis B. and Paris, Cécile and Shellock, Rebecca and Trebilco, Rowan},
    journal = {Cell Reports Sustainability},
    volume = {1},
    number = {7},
    year = {2024},
    month = {7},
    publisher = {Elsevier},
    doi = {10.1016/j.crsus.2024.100132},
    issn = {2949-7906},
    url = {https://doi.org/10.1016/j.crsus.2024.100132}
}









@article{
shoemaker2025,
author = {Kevin T. Shoemaker  and Kevin J. Loope },
title = {We need better ways to re-evaluate conservation policies when they’re founded on flawed research},
journal = {Proceedings of the National Academy of Sciences},
volume = {122},
number = {19},
pages = {e2426166122},
year = {2025},
doi = {10.1073/pnas.2426166122},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.2426166122},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2426166122}}

