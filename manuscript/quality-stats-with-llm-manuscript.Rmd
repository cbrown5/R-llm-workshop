--- 
title: "Perspective: Quality statistics with large language models"
author: "CJ Brown (c.j.brown@utas.edu.au)"
date: "`r Sys.Date()`"
url: https://www.seascapemodels.org/R-llm-workshop/
description: |
  Perspective article on prompting
github-repo: cbrown5/R-llm-workshop
bibliography: references.bib  
---

CJ Brown - c.j.brown@utas.edu.au

University of Tasmania

Perspective article

**Citation: Brown (2025). Quality statistics with large language models. Unpublished manuscript**

Pre-print forthcoming soon... 

Aim for 2000-3000 words


## Abstract

Large language models (LLMs) are rapidly transforming scientific workflows, including statistical analyses in environmental sciences. While these AI tools offer impressive capabilities for code generation and analytical guidance, evaluations reveal significant limitations in their statistical reasoning. This perspective addresses the need for effective prompting guidelines to ensure quality statistical analyses when using LLMs. Drawing on empirical evaluations and practical experience, I provide a framework for environmental scientists to leverage these powerful tools while maintaining statistical rigor. Key recommendations include: separating workflows into components that align with LLM strengths and limitations; providing rich context through domain knowledge, data summaries, and research questions; using structured prompting techniques like Chain of Thought reasoning; and maintaining human oversight of critical statistical decisions. By understanding LLM capabilities and employing these prompting strategies, researchers can harness these technologies to improve rather than compromise statistical quality in environmental research. Future research should focus on evaluations of LLMs for  statistical recommendations, development of specialized prompting strategies, and integration of LLMs with traditional statistical approaches.

## Introduction

Large language models (LLMs) are rapidly transforming scientific workflows across disciplines, including environmental sciences. Recent surveys indicate that over XXX% of researchers now incorporate LLMs into their workflows. A recent focus of LLM development has been in the areas of math, reasoning and computer programming. LLMs are making rapid gains in these domains, even showing the ability to out-perform human experts on a range of scientific topics (ref, but  ...). Given their abilities in these domains, LLMs will likely become increasingly  used for the statistical analysis of data. Modern data analysis requires two inter-related skills that depend on logical reasoning: computer programming and statistical analysis. It requires extensive training to develop proficiency in these skills and there is a large gap between scientists working at the forefront of statistical computing and those who are experts in other domains and only need to use statistics and programming irregularly for data analysis. Indeed, our conversations indicate that more than half of our colleagues are using LLMs specifically to provide advice on statistical analyses or to aid scientific programming. The incentive to use these tools is strong: they can almost instantaneously provide advice on statistics and write program code, seemingly removing knowledge barriers regarding statistics and speeding up workflows many times. For instance, with current freely avaiable software (2025-05-30) it is possible to get an LLM to create a sophisticated ordination of an ecological community within 15 minutes (see example in supplemental material). This includes creating a document with figures, methods and interpretation of results. Given the competitiveness of research culture, it is therefore likely that published results are increasingly depending on LLM derived analyses and advice. Therefore, there is a growing imperitive to study the quality of advice from LLMs with regards to environmental statistics and computing, as well as to develop guidelines for robust use patterns. 

Without clear standards for use, LLMs may escalate exisiting issues with the quality of statistical analyses in environmental sciences. LLMs rapidly speed up our abilities to try new statistical methods, giving the unscrupulous research a magnitude more options for models from which to choose the 'ideal' results (p-hacking). Indeed, p-hacking and other forms of bias caused by manipulating analyses after viewing results are prevalent in ecology and evolution and frequently justified by individuals as neccessary for career survival [@fraser2018; @forstmeier2017]. There are also many less nefarious ways LLMs could harm the quality of science. LLMs almost always provide an answer and are typically over-confident of having the right answer. This challenge may be a particular problem in environmental sciences, where complex ecological data often violate standard statistical assumptions and require specialized analytical approaches. It is not clear that current LLMs appreciate these nuances of environmental data. Inexperienced statisticians could be led to use inappropriate analyses, or erronious code. Lack of training in statistics is a widespread problem that undermines the progress of knowledge and application of science to environmental issues. Examples include inappropriate transformations of response variables [@ohara2010] applying methods that assume independent samples to time-series analysis [@brown2011], application of linear regression zero-inflated data (@warton2016), conflating prediction with causality (@arif2022) and inappropriate use of multi-model averages (@bolker2024). 

There are a handful of emerging evaluations of LLMs in the peer-reviewed and pre-print literature. In the environmental literature recent studies have shown LLMs can extract some types of structured data for literature reviews [@gougherty2024] and are effective partners for evidence synthesis [@spillias2024]. These studies speak to the "jagged frontier" of LLM progress [@mollick2025] - they excel in some tasks like logic and semantic searches of documents but lag in other domains such as statistical analysis. One evaluation of the ability of 2024 LLMs to suggest appropriate statistical analyses for questions about an analysis found disappointing results [@zhu2024large]. Accuracy at suggesting the correct statistical tests ranged from just 8% to 90% and was typically less than 40% for anything more sophisticated than basic descriptive statistics. A key result from all of these studies is the importance of prompt writing. For example, accuracy at picking the right statistical test for a problem could double with the right prompting strategy. 

errors in modelling matter for conservation and can stuff things up - @shoemaker2025

This perspective article provides preliminary guidelines for effective prompting that can help improve the quality of LLM responses on scientific computing and statistical analysis. LLM progress and uptake by scientists is currently moving too rapidly for us to wait for formal peer-reviewed evaluations to develop guidelines. Therefore, we draw on diverse sources, including our peer-reviewed studies, extensive practical experience applying LLMs to environmental statistics, experience with teaching, AI industry guidelines and the AI literature more broadly. We conclude by calling for formal evaluations and development of thorough community standards for statistical uses. By understanding LLM capabilities and limitations and employing structured prompting strategies, researchers can harness these technologies to improve rather than compromise statistical quality in environmental research.

## Applications of LLMs to scientific computing and statistics

Large language models present significant opportunities to enhance statistical practice in environmental sciences when used with appropriate guidance and oversight. These AI tools can democratize access to statistical expertise, providing researchers who lack ready access to statistical collaborators with guidance on appropriate methods and implementation strategies. This democratization is particularly valuable in resource-constrained settings or for early-career researchers still developing their statistical networks. For example, researchers in remote field stations or from institutions without dedicated statistical support can leverage LLMs to explore analytical options that might otherwise be inaccessible.

The code generation capabilities of LLMs represent another substantial benefit for statistical practice. These models excel at producing clean, well-documented code that follows contemporary best practices in programming and data analysis. When properly prompted, LLMs can generate R or Python scripts with comprehensive comments, logical structure, and adherence to style guidesâ€”potentially improving computational reproducibility across environmental sciences. This advantage addresses a persistent challenge in the field, where poorly documented or inconsistently structured code has hampered reproducibility efforts.

LLMs also facilitate rapid exploration of alternative analytical approaches, enabling researchers to quickly generate and compare multiple statistical strategies. This capability encourages more robust sensitivity analyses, as researchers can efficiently implement various models to assess how analytical choices influence results. For instance, an ecologist studying species distributions could use an LLM to implement both frequentist and Bayesian approaches to the same question, comparing outcomes and assumptions without investing extensive time in coding each approach from scratch.

The opportunity to run many analyses also creates a risk of accelerating of opportunities for p-hacking. Researchers can write code to explore 10s or 100s of alternatives for solving a statistical issue, creating further opportunities for selective picking the 'best' results. Strong research reporting and research ethics is ultimately needed to combat this issue. 

Documentation and transparencyâ€”critical elements of rigorous scienceâ€”can be substantially improved through LLM assistance. These tools can help generate detailed methodological descriptions, create reproducible workflows with appropriate documentation, and ensure consistent reporting of statistical procedures. By prompting LLMs to document analytical decisions and assumptions explicitly, researchers can enhance the transparency of their statistical workflows, making them more accessible to reviewers and future researchers seeking to build upon their work.

Beyond implementation benefits, LLMs offer valuable opportunities for statistical learning and skill development. When used as interactive tutors rather than black-box solution providers, these models can enhance researchers' statistical understanding by explaining concepts, suggesting relevant literature, and demonstrating proper implementation techniques. This educational function is particularly valuable in environmental sciences, where researchers often come from diverse disciplinary backgrounds with varying levels of statistical training. By engaging with LLMs through carefully structured prompts that request explanations and justifications, researchers can develop deeper statistical intuition alongside practical implementation skills.

However, realizing these benefits requires understanding the fundamental nature of LLMs and their limitations. Unlike traditional statistical software that implements specific algorithms, LLMs generate responses based on patterns learned from training data. This distinction is crucialâ€”LLMs do not "understand" statistics in the way human experts do, but rather predict what text would likely follow in a statistical context. The implications of this prediction-based approach are significant for statistical practice, as these models may confidently suggest inappropriate methods, fail to recognize violations of statistical assumptions, or generate plausible-sounding but incorrect interpretations. 

The challenge, therefore, is to develop prompting strategies that maximize LLMs' strengths while compensating for their weaknesses. Effective prompting requires providing sufficient context about research questions, data characteristics, and analytical constraints to guide the model toward appropriate statistical recommendations. It also involves maintaining critical oversight of model outputs, particularly for decisions requiring deeper statistical understanding such as assumption checking and result interpretation.

The opportunity lies in developing a new kind of statistical workflow that combines human expertise with LLM capabilities. In this workflow, researchers maintain responsibility for critical statistical decisions while using LLMs to implement analyses efficiently, explore options, and enhance documentation. This human-AI partnership represents a middle path between complete automation and traditional manual implementationâ€”leveraging the efficiency and consistency of LLMs while preserving the critical judgment and domain expertise of human researchers. The key to this workflow is effective promptingâ€”providing LLMs with the context, constraints, and guidance needed to generate high-quality statistical advice and code that advances rather than compromises statistical rigor in environmental research.

## LLM Overview

To develop effective prompting strategies, it's essential to understand how LLMs function. At their core, LLMs are prediction engines that generate text one token at a time based on patterns learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.

This token-by-token prediction process has important implications for statistical guidance. LLMs don't reason about statistics from first principles; they predict what text would likely follow in a statistical context based on their training data. This means their statistical advice reflects patterns in existing literature and codeâ€”including both best practices and common mistakes.

Several key parameters influence LLM behavior:

1. **Temperature**: Controls randomness in token prediction. Lower temperatures (closer to 0) make responses more deterministic and conservative, while higher temperatures (closer to 2) increase creativity but potentially reduce reliability. For statistical applications, lower temperatures typically produce more consistent and conventional recommendations.

2. **Context window**: The amount of text an LLM can consider when generating a response. Current LLMs have context windows typically in the range from 100,000 to 1,000,000 tokens. Larger context windows allow for including more detailed information about data, research questions, and statistical requirements.

3. **Model complexity**: Different models have varying capabilities based on their size, training data, and architecture. More complex models (e.g., Claude-3.7-Sonnet vs. Claude-3.5-Haiku) generally provide more nuanced statistical guidance but at higher computational and financial cost.

4. **System prompt**: Sets the overall context and constraints for the LLM. This "behind-the-scenes" instruction shapes how the model responds to user queries and can significantly impact statistical advice quality.

5. **AI assistant, AI programmer pair** 

6. **Tools and MCP** 

7. **Agents** 

Understanding these parameters allows researchers to optimize LLM interactions for statistical applications. For example, using lower temperatures for statistical recommendations increases consistency, while larger context windows enable including more detailed information about data characteristics and research questions.

The distinction between different types of LLM interfaces is also important. Basic chat interfaces (like ChatGPT) provide limited control over these parameters, while API access and specialized coding assistants (like GitHub Copilot) offer more customization options. For statistical applications, interfaces that allow inclusion of data summaries, code context, and specialized system prompts will produce better results.

## Prompting Guidelines Best Practices

Effective prompting can dramatically improve the quality of statistical guidance from LLMs. Based on empirical evaluations and practical experience, the following guidelines provide a framework for environmental scientists seeking to leverage LLMs for statistical applications.

### Recognizing Different Steps in Workflows

A critical first step is separating statistical workflows into distinct components that align with LLM strengths and limitations:

1. **Statistical approach selection**: Determining appropriate statistical methods for research questions
2. **Implementation planning**: Designing the analytical workflow and code structure
3. **Code generation**: Writing the actual code to implement analyses
4. **Interpretation guidance**: Understanding and reporting results

LLMs perform differently across these components. They excel at code generation and implementation planning but are less reliable for selecting appropriate statistical approaches or interpreting complex results. This uneven performance profile suggests a workflow where:

- Researchers maintain primary responsibility for statistical approach selection, potentially using LLMs to explore options but validating choices against literature and expert knowledge
- LLMs take a more central role in implementation planning and code generation, with researchers providing clear constraints on the workflow
- Interpretation remains a collaborative process where LLMs can suggest standard interpretations but researchers critically evaluate these suggestions

This separation of responsibilities helps prevent overreliance on LLMs for decisions that require deeper statistical understanding while leveraging their strengths in code generation and documentation.

### Statistical Advice

When seeking statistical guidance from LLMs, several prompting strategies can significantly improve response quality.Whereas, we have found that many other prompting strategies commonly spruiked by tech commenators are less effective for elicting good  statistical advice. 

To elicit better statistical advice the most effective approaches are to be clear about your hypotheses, attach domain knowledge, include context about the data and data collection process. Below we provide several illustrative examples. 

**Example 1: Be clear and provide domain knowledge** 

This example is written from the point of view of a researcher who knows their hypothesis and data, but is unclear about what methods they should use: 

```
How can I statistically test the dependence of fish abundance on coral cover? 
Abundances are count data representing, and the cover variable is proportional data. Use @websearch to find 
robust recommendations for ecologists to analyze count data before proceeding 
with your recommendations.
```

In this prompt we made use of several strategies. First we were clear about the hypothesis (we think pres.topa should depend on CB_cover, not the other way around). We specificed the variable names as they appear in the data (e.g. `pres.topa`), this mean the LLM is more likely to write accurate code. We gave context about the data, by telling the LLM we had count data the response is more likely to include count appropriate methods (e.g. Poisson GLM). Finally, we asked the LLM to use a web search to find domain knowledge in our field. 

**Example 2: Provide context on experimental/observational design** 

Extending the previous example we could also provide context about survey design: 

```
I need to analyze the dependence of fish abundance (integer counts, 
zero-inflated) on coral cover (continuous percentage). Sites are spatially 
clustered within regions. What statistical approaches would be appropriate?
```

In this prompt by including the context that sites are spatial replicates the LLM is more likely to recommend methods that account for spatial structure in the data. 

**Example 3: Attach data** 

```
How can I statistically test the depdence of `pres.topa` on `CB_cover`? 
Here are the first 6 rows of data:
[data table]
```

Here we have specifically named the variables as they appear in the dataset (e.g. `pres.topa`). We have also dragged and dropped the first 6 rows of the data into the prompt (this is possible with many assistants, e.g. github copilot). Often (but not always) the LLM will appropriately recognize the response as a count variable if the data is attached. 

**Example 3: Use Chain of Thought reasoning with reputable source attached**

```
Using Chain of Thought reasoning, what statistical approach would be most 
appropriate for analyzing the relationship between fish abundance (count data) 
and coral cover (continuous)? We have attached a guideline for analysis of count data in ecology: [attach reference]
```

Chain of Thought reasoning is a common and easy prompting strategy that can dramatically improve responses in many domains, particularly reasoning. This works because LLMs 'think' by writing (a trait they share with good researchers). However, it does not improve statistical advice on its own (@zhu2024large). If used on its own, CoT will cause the LLM to elaborate on the same incorrect suggestions. In this prompt we dragged and dropped a reference (e.g. a blog from a reputable source, or a vignette from a popular ecological modelling R package). Therefore, the LLM could reason accurately with its improved domain knowledge. 

**Example 4: Request self-evaluation and multiple options** 

```
Evaluate the robustness of each suggestion on a 1-10 scale and explain 
the strengths and limitations of each approach.
```

or 

```
What are three different statistical approaches I could use for this analysis? 
For each, explain the assumptions, advantages, and limitations.
```

These suggestions is related to CoT, it just promotes further thinking. Once again, best used in a chain of prompts when you have attached a reputable reference. Note that most LLM assistants have the imperitive to provide options baked in by the providers, so it is usually not neccessary to explictly ask for options, just ask to think about them. 


### Code Implementation Advice

UP TO HERE

For implementing statistical analyses in R, different prompting strategies apply:

1. **Create a detailed project README**: Document the project context, research questions, data structure, and analytical approach in a README.md file that can be attached to prompts:

```
Help me plan R code to implement this analysis based on the project context 
in the README.
```

See the example in Supplemental file 1. 

2. **Use a two-step approach**: First plan the analysis structure, then implement specific components:

```
First, outline the overall workflow for analyzing the relationship between 
fish abundance and coral cover using a negative binomial GLM. Then, we'll 
implement each step.
```

3. **Provide implementation constraints**: Specify packages, coding style, and other requirements:

```
Implement this analysis using the tidyverse ecosystem and INLA for Bayesian 
modeling. Follow tidyverse style guidelines and prioritize code readability.
```

4. **Request modular code**: Ask for code organized into logical functions and scripts:

```
Create modular scripts for this analysis with separate files for data 
preparation, model fitting, diagnostics, and visualization.
```

5. **Include verification steps**: Request code that validates assumptions and checks model fit:

```
Include diagnostic checks for overdispersion, zero-inflation, and spatial 
autocorrelation in the model implementation.
```

6. **Iteratively refine**: Start with a basic implementation and progressively add complexity:

```
Let's start with a simple negative binomial GLM for fish abundance. Once 
that's working, we'll extend it to account for spatial clustering.
```

These strategies help ensure that LLM-generated code is not only syntactically correct but also statistically appropriate and well-structured. By providing clear constraints and expectations, researchers can guide LLMs toward implementations that follow best practices in both programming and statistical analysis.

## Discussion and Conclusion

Large language models represent both opportunity and challenge for statistical practice in environmental sciences. When used thoughtfully with effective prompting strategies, they can enhance analytical workflows, improve code quality, and potentially address longstanding issues in statistical implementation. However, uncritical reliance on LLMs risks perpetuating or even amplifying existing problems in statistical practice.

The prompting guidelines presented in this perspective provide a framework for leveraging LLMs while maintaining statistical rigor. By separating workflows into components that align with LLM strengths and limitations, providing appropriate context and constraints, and maintaining human oversight of critical decisions, researchers can harness these powerful tools while mitigating their risks.

Several key principles emerge from this analysis:

1. **Maintain critical thinking**: LLMs should complement rather than replace statistical expertise. Researchers must critically evaluate LLM suggestions against domain knowledge and statistical principles.

2. **Provide rich context**: The quality of LLM statistical guidance improves dramatically when provided with detailed information about research questions, data characteristics, and analytical constraints.

3. **Leverage strengths, compensate for weaknesses**: Use LLMs primarily for tasks where they excel (code generation, implementation planning) while maintaining human oversight of tasks requiring deeper statistical understanding (method selection, assumption checking, interpretation).

4. **Document LLM use**: Transparency about LLM use in research workflows is essential for reproducibility and evaluation. Publications should clearly describe how LLMs were used and what prompting strategies were employed.

5. **Develop LLM literacy**: As these tools become increasingly integrated into research workflows, developing "LLM literacy"â€”understanding how these models work, their limitations, and effective interaction strategiesâ€”becomes an essential skill for environmental scientists.

The rapid evolution of LLM capabilities suggests that their role in statistical workflows will only increase. Current models already show impressive performance in code generation and implementation planning, and future models may address some of the limitations identified in statistical reasoning. However, the fundamental nature of LLMs as prediction engines rather than reasoning systems means that human oversight will remain essential for ensuring statistical quality.

### Research Needs

Several critical research needs emerge from this analysis:

1. **Evaluations of LLM statistical performance**: Systematic assessments across diverse environmental data types and analytical challenges would help identify specific strengths and weaknesses.

2. **Development of specialized prompting strategies**: Domain-specific prompting templates and guidelines could improve consistency and quality of statistical implementations.

3. **Integration of LLMs with traditional statistical software**: Hybrid systems that combine LLM flexibility with the algorithmic reliability of traditional statistical packages could leverage the strengths of both approaches.

4. **Educational approaches for LLM-assisted statistics**: New pedagogical strategies are needed to develop statistical understanding in an era where code implementation is increasingly automated.

5. **Ethical frameworks for LLM use in research**: Guidelines for appropriate attribution, transparency, and responsibility when using LLM-assisted analyses in published research.

By addressing these research needs and adopting thoughtful prompting strategies, environmental scientists can harness the power of large language models to enhance rather than compromise statistical quality. The future of environmental statistics likely lies not in choosing between human expertise and artificial intelligence, but in developing effective partnerships that leverage the unique strengths of each.

## Acknowledgements

## References


## Figures

```{r risk-framework, fig.width=5, fig.height=3, dpi=300, echo = FALSE}

library(DiagrammeR)

# Create a decision tree diagram for statistical risk mitigation
DiagrammeR::grViz("
digraph risk_framework {
  # Graph settings
  graph [rankdir = TD, fontname = 'Arial', nodesep = 0.8, ranksep = 0.5]
  node [shape = rectangle, fontname = 'Arial', fontsize = 12, style = 'filled', 
        fillcolor = 'white', width = 3, height = 0.8, margin = 0.2]
  edge [fontname = 'Arial', fontsize = 10]

  # Decision nodes
  task_complexity [label = 'Assess Task Complexity', fillcolor = '#e6f3ff', shape = diamond]
  error_consequence_low [label = 'Assess Consequence of Errors\n(Low Complexity)', fillcolor = '#e6f3ff', shape = diamond]
  error_consequence_high [label = 'Assess Consequence of Errors\n(High Complexity)', fillcolor = '#e6f3ff', shape = diamond]
  
  # Risk levels and mitigation strategies
  low_risk [label = 'Low Risk\n- Use standard prompts\n- Basic verification', fillcolor = '#ccffcc']
  medium_risk1 [label = 'Medium Risk\n- Provide rich context\n- Use Chain of Thought\n- Compare multiple approaches', fillcolor = '#ffffcc']
  medium_risk2 [label = 'Medium Risk\n- Expert review required\n- Include domain knowledge\n- Request self-evaluation', fillcolor = '#ffffcc']
  high_risk [label = 'High Risk\n- Use LLM for code only\n- Human decision-making\n- Peer review essential\n- Extensive validation', fillcolor = '#ffcccc']
  
  # Connections
  task_complexity -> error_consequence_low [label = 'Low\n(Descriptive stats,\nbasic plotting,\nsimple tests)']
  task_complexity -> error_consequence_high [label = 'High\n(Complex models,\nmultivariate analysis,\nspecialized methods)']
  
  error_consequence_low -> low_risk [label = 'Low\n(Exploratory,\nnon-critical)']
  error_consequence_low -> medium_risk1 [label = 'High\n(Publication,\ndecision-making)']
  
  error_consequence_high -> medium_risk2 [label = 'Low\n(Exploratory,\nnon-critical)']
  error_consequence_high -> high_risk [label = 'High\n(Publication,\ndecision-making)']
  
  # Additional guidance nodes
  subgraph cluster_guidance {
    label = 'General Risk Mitigation Guidelines'
    style = filled
    fillcolor = '#f0f0f0'
    node [shape = box, margin = 0.1, fontsize = 10, width = 2.5]
    
    guidance1 [label = 'Always document LLM use\nand prompting strategies']
    guidance2 [label = 'Validate against established\nstatistical literature']
    guidance3 [label = 'Consider reproducibility\nand transparency']
    
    guidance1 -> guidance2 -> guidance3 [style = invis]
  }
}
")
```

**Figure XXX**
Decision tree showing how to identify and mitigate risks in LLM statistical advice based on task complexity and consequence of errors.


```{r workflow-diagram, fig.width=5, fig.height=3, dpi=300, echo = FALSE}
library(DiagrammeR)

DiagrammeR::grViz("
digraph workflow {
  # Graph settings
  graph [rankdir = TD, fontname = 'Arial', nodesep = 0.8, ranksep = 0.8]
  node [shape = rectangle, fontname = 'Arial', fontsize = 12, style = 'filled', 
        fillcolor = 'white', margin = 0.2]
  edge [fontname = 'Arial', fontsize = 10]

  # Main workflow steps
  step1 [label = 'Step 1: Statistical Approach Selection', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step2 [label = 'Step 2: Implementation Planning', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step3 [label = 'Step 3: Code Generation', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step4 [label = 'Step 4: Interpretation Guidance', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  
  # Recommendations for each step
  rec1 [label = 'Recommendations:\\nâ€¢ Explore multiple analytical approaches\\nâ€¢ Provide domain knowledge and relevant literature\\nâ€¢ Request step-by-step reasoning with Chain of Thought\\nâ€¢ Compare statistical approaches with alternatives\\nâ€¢ Prompt for cross-disciplinary perspectives\\nâ€¢ Verify against statistical textbooks and guidelines', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec2 [label = 'Recommendations:\\nâ€¢ Create a detailed README with analysis context\\nâ€¢ Define clear research questions and hypotheses\\nâ€¢ Specify data characteristics and structure\\nâ€¢ Outline analytical constraints and assumptions\\nâ€¢ Plan workflow before implementation\\nâ€¢ Establish validation criteria in advance', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec3 [label = 'Recommendations:\\nâ€¢ Use a two-step approach: plan then implement\\nâ€¢ Provide implementation constraints (packages, style)\\nâ€¢ Request modular, well-documented code\\nâ€¢ Include verification and diagnostic steps\\nâ€¢ Iteratively refine from simple to complex models\\nâ€¢ Add comments explaining statistical decisions', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec4 [label = 'Recommendations:\\nâ€¢ Never blindly trust LLM interpretations\\nâ€¢ Request multiple visualization options\\nâ€¢ Ask for limitations and assumptions of results\\nâ€¢ Prompt for alternative interpretations\\nâ€¢ Request guidance on reporting standards\\nâ€¢ Maintain human oversight of conclusions', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]

  # Connections between steps
  step1 -> step2 -> step3 -> step4 [weight = 5]
  
  # Connect steps to their recommendations
  step1 -> rec1 [dir = none, style = dashed]
  step2 -> rec2 [dir = none, style = dashed]  
  step3 -> rec3 [dir = none, style = dashed]
  step4 -> rec4 [dir = none, style = dashed]
  
  # Human oversight element
  {rank = same; human [label = 'Human Oversight\\nand Critical Evaluation', 
                      shape = 'oval', fillcolor = '#ffffcc', 
                      style = 'filled,dashed', width = 3]}
                      
  human -> step1 [dir = both, style = dashed, constraint = false]
  human -> step2 [dir = both, style = dashed, constraint = false]
  human -> step3 [dir = both, style = dashed, constraint = false]
  human -> step4 [dir = both, style = dashed, constraint = false]
}
")
```

**Figure 2**
Recommended workflow for using LLMs in statistical analysis, showing the four key steps alongside specific recommendations for effectively leveraging LLMs at each stage while maintaining scientific rigor.


