--- 
title: "Perspective: Quality statistics with large language models"
author: "CJ Brown (c.j.brown@utas.edu.au)"
date: "`r Sys.Date()`"
url: https://www.seascapemodels.org/R-llm-workshop/
description: |
  Perspective article on prompting
github-repo: cbrown5/R-llm-workshop
bibliography: references.bib  
---

CJ Brown - c.j.brown@utas.edu.au

University of Tasmania

Perspective article

**Citation: Brown (2025). Quality statistics with large language models. Unpublished manuscript**

Pre-print forthcoming soon... 

Aim for 2000-3000 words


## Abstract

Large language models (LLMs) are rapidly transforming scientific workflows, including statistical analyses in environmental sciences. While these AI tools offer impressive capabilities for code generation and analytical guidance, evaluations reveal significant limitations in their statistical reasoning. This perspective addresses the need for effective prompting guidelines to ensure quality statistical analyses when using LLMs. Drawing on empirical evaluations and practical experience, I provide a framework for environmental scientists to leverage these powerful tools while maintaining statistical rigor. Key recommendations include: separating workflows into components that align with LLM strengths and limitations; providing rich context through domain knowledge, data summaries, and research questions; using structured prompting techniques like Chain of Thought reasoning; and maintaining human oversight of critical statistical decisions. By understanding LLM capabilities and employing these prompting strategies, researchers can harness these technologies to improve rather than compromise statistical quality in environmental research. Future research should focus on evaluations of LLMs for  statistical recommendations, development of specialized prompting strategies, and integration of LLMs with traditional statistical approaches.

## Introduction

Large language models (LLMs) are rapidly transforming scientific workflows, with profound implications for statistical analysis in environmental sciences. Recent surveys indicate that over XXX% of researchers now incorporate LLMs into their workflows, with many specifically using them for statistical advice and code generation. The appeal is clear: LLMs almost instantly generate statistical code and analyses that would traditionally require extensive training and time to develop. For example, researchers can now produce a complete ecological ordination analysis—including code, visualizations, and interpretations—in under 15 minutes (see example in supplemental material).

However, this efficiency comes with significant risks. Recent evaluations reveal concerning limitations in LLMs' statistical reasoning abilities. One study found that accuracy at suggesting appropriate statistical tests was typically below 40% for anything beyond basic descriptive statistics [@zhu2024large]. Crucially, the quality of statistical advice from LLMs depends heavily on how questions are framed—effective prompting strategies can double the accuracy of recommendations.

This perspective article addresses the urgent need for guidelines on using LLMs for statistical analysis in environmental research. As LLM adoption outpaces formal evaluation, we cannot wait for comprehensive peer-reviewed assessments before establishing best practices. Drawing on empirical evaluations, practical experience, and broader AI literature, we provide a framework for leveraging these powerful tools while maintaining statistical rigor. By understanding LLM capabilities and limitations and employing structured prompting strategies, researchers can harness these technologies to enhance rather than compromise statistical quality in environmental research. The guidelines presented here aim to help environmental scientists navigate this rapidly evolving landscape responsibly and effectively.

## Challenges for statistical analysis quality in environmental sciences

Statistical analysis in environmental sciences faces numerous challenges that predate the emergence of LLMs but may be exacerbated by their use. Environmental data often violate standard statistical assumptions, requiring specialized analytical approaches that may not be well-represented in LLM training data. 

Lack of statistical training among environmental scientists has long undermined research quality and application. Common misapplications include inappropriate transformations of response variables [@ohara2010], applying methods that assume independent samples to time-series analysis [@brown2011], using linear regression for zero-inflated data [@warton2016], conflating prediction with causality [@arif2022], and inappropriate use of multi-model averages [@bolker2024]. These errors are not merely academic concerns—they can lead to misinformed policy actions with real consequences for conservation outcomes [@shoemaker2025].

P-hacking and other forms of bias caused by manipulating analyses after viewing results are already prevalent in ecology and evolution, often justified by researchers as necessary for career survival [@fraser2018; @forstmeier2017]. Without clear guidelines, LLMs could dramatically accelerate this problem by enabling researchers to rapidly generate and select from numerous analytical approaches.

Modern data analysis requires two interrelated skills: computer programming and statistical reasoning. There exists a substantial gap between specialists at the forefront of statistical computing and domain experts who use statistics irregularly. LLMs appear to bridge this gap by democratizing access to statistical expertise, but this accessibility comes with significant risks if users lack the background to evaluate the appropriateness of LLM-generated analyses.

## Applications of LLMs to scientific computing and statistics

Large language models present significant opportunities to enhance statistical practice in environmental sciences when used with appropriate guidance and oversight. These AI tools can democratize access to statistical expertise, providing researchers who lack ready access to statistical collaborators with guidance on appropriate methods and implementation strategies. This democratization is particularly valuable in resource-constrained settings or for early-career researchers still developing their statistical networks. For example, researchers in remote field stations or from institutions without dedicated statistical support can leverage LLMs to explore analytical options that might otherwise be inaccessible.

The code generation capabilities of LLMs represent another substantial benefit for statistical practice. These models excel at producing clean, well-documented code that follows contemporary best practices in programming and data analysis. When properly prompted, LLMs can generate R or Python scripts with comprehensive comments, logical structure, and adherence to style guides—potentially improving computational reproducibility across environmental sciences. This advantage addresses a persistent challenge in the field, where poorly documented or inconsistently structured code has hampered reproducibility efforts.

LLMs also facilitate rapid exploration of alternative analytical approaches, enabling researchers to quickly generate and compare multiple statistical strategies. This capability encourages more robust sensitivity analyses, as researchers can efficiently implement various models to assess how analytical choices influence results. For instance, an ecologist studying species distributions could use an LLM to implement both frequentist and Bayesian approaches to the same question, comparing outcomes and assumptions without investing extensive time in coding each approach from scratch.

Documentation and transparency—critical elements of rigorous science—can be substantially improved through LLM assistance. These tools can help generate detailed methodological descriptions, create reproducible workflows with appropriate documentation, and ensure consistent reporting of statistical procedures. By prompting LLMs to document analytical decisions and assumptions explicitly, researchers can enhance the transparency of their statistical workflows, making them more accessible to reviewers and future researchers seeking to build upon their work.

Beyond implementation benefits, LLMs offer valuable opportunities for statistical learning and skill development. When used as interactive tutors rather than black-box solution providers, these models can enhance researchers' statistical understanding by explaining concepts, suggesting relevant literature, and demonstrating proper implementation techniques. This educational function is particularly valuable in environmental sciences, where researchers often come from diverse disciplinary backgrounds with varying levels of statistical training. By engaging with LLMs through carefully structured prompts that request explanations and justifications, researchers can develop deeper statistical intuition alongside practical implementation skills.

However, realizing these benefits requires understanding the fundamental nature of LLMs and their limitations. Unlike traditional statistical software that implements specific algorithms, LLMs generate responses based on patterns learned from training data. This distinction is crucial—LLMs do not "understand" statistics in the way human experts do, but rather predict what text would likely follow in a statistical context. The implications of this prediction-based approach are significant for statistical practice, as these models may confidently suggest inappropriate methods, fail to recognize violations of statistical assumptions, or generate plausible-sounding but incorrect interpretations. 

The challenge, therefore, is to develop prompting strategies that maximize LLMs' strengths while compensating for their weaknesses. Effective prompting requires providing sufficient context about research questions, data characteristics, and analytical constraints to guide the model toward appropriate statistical recommendations. It also involves maintaining critical oversight of model outputs, particularly for decisions requiring deeper statistical understanding such as assumption checking and result interpretation.

The opportunity lies in developing a new kind of statistical workflow that combines human expertise with LLM capabilities. In this workflow, researchers maintain responsibility for critical statistical decisions while using LLMs to implement analyses efficiently, explore options, and enhance documentation. This human-AI partnership represents a middle path between complete automation and traditional manual implementation—leveraging the efficiency and consistency of LLMs while preserving the critical judgment and domain expertise of human researchers. The key to this workflow is effective prompting—providing LLMs with the context, constraints, and guidance needed to generate high-quality statistical advice and code that advances rather than compromises statistical rigor in environmental research.

## Risks of LLM use for environmental statistics

LLMs present several specific risks for statistical practice in environmental sciences that require careful consideration and mitigation strategies.

First, LLMs may amplify existing problems with statistical quality. By dramatically accelerating the ability to try multiple analytical approaches, LLMs could enable unprecedented levels of p-hacking and selective reporting. Researchers can now write code to explore tens or hundreds of alternatives for solving a statistical issue in minutes, creating far more opportunities to cherry-pick favorable results. While this capability could support robust sensitivity analyses, it also creates a significant risk of selective reporting that undermines scientific integrity. Strong research reporting standards and ethics are ultimately needed to combat this issue.

Second, LLMs exhibit overconfidence in their statistical recommendations. They almost always provide an answer, typically with high apparent certainty, even when their suggestions are inappropriate or incorrect. This characteristic is particularly problematic in environmental sciences, where data often have complex structures requiring specialized approaches. Current LLMs may not adequately recognize or account for these nuances of environmental data.

Third, LLMs lack true statistical understanding. Unlike human experts who reason from first principles, LLMs generate responses based on patterns in their training data. This fundamental limitation means they may confidently suggest inappropriate methods, fail to recognize violations of statistical assumptions, or generate plausible-sounding but incorrect interpretations.

Fourth, inexperienced users may be particularly vulnerable to these risks. Without sufficient statistical background to critically evaluate LLM suggestions, researchers might implement inappropriate analyses or misinterpret results. The apparent authority and confidence of LLM responses can create a false sense of security, potentially leading to erroneous conclusions that influence scientific understanding and policy decisions.

Finally, there is a risk of statistical deskilling in the research community. If researchers increasingly rely on LLMs for statistical decisions without developing their own understanding, the collective statistical literacy of the field could decline over time. This would create a dangerous dependency on tools that, while powerful, lack true statistical reasoning capabilities.

These risks highlight the need for thoughtful guidelines on LLM use in statistical workflows—guidelines that leverage LLM strengths while implementing appropriate safeguards against their limitations.

## LLM Overview

To develop effective prompting strategies, it's essential to understand how LLMs function. At their core, LLMs are prediction engines that generate text one token at a time based on patterns learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.

This token-by-token prediction process has important implications for statistical guidance. LLMs don't reason about statistics from first principles; they predict what text would likely follow in a statistical context based on their training data. This means their statistical advice reflects patterns in existing literature and code—including both best practices and common mistakes.

Several key parameters influence LLM behavior:

1. **Temperature**: Controls randomness in token prediction. Lower temperatures (closer to 0) make responses more deterministic and conservative, while higher temperatures (closer to 2) increase creativity but potentially reduce reliability. For statistical applications, lower temperatures typically produce more consistent and conventional recommendations.

2. **Context window**: The amount of text an LLM can consider when generating a response. Current LLMs have context windows typically in the range from 100,000 to 1,000,000 tokens. Larger context windows allow for including more detailed information about data, research questions, and statistical requirements.

3. **Model complexity**: Different models have varying capabilities based on their size, training data, and architecture. More complex models (e.g., Claude-4.0-Opus vs. Claude-4.0-Sonnet) generally provide more nuanced statistical guidance but at higher computational and financial cost.

4. **System prompt**: Sets the overall context and constraints for the LLM. This "behind-the-scenes" instruction shapes how the model responds to user queries and can significantly impact statistical advice quality.

5. **AI assistant, AI programmer pair** 

6. **Tools and MCP** 

7. **Agents** 

Understanding these parameters allows researchers to optimize LLM interactions for statistical applications. For example, using lower temperatures for statistical recommendations increases consistency, while larger context windows enable including more detailed information about data characteristics and research questions.

The distinction between different types of LLM interfaces is also important. Basic chat interfaces (like ChatGPT) provide limited control over these parameters, while API access and specialized coding assistants (like GitHub Copilot) offer more customization options. For statistical applications, interfaces that allow inclusion of data summaries, code context, and specialized system prompts will produce better results.

## Prompting Guidelines Best Practices

Effective prompting can dramatically improve the quality of statistical guidance from LLMs. Based on empirical evaluations and practical experience, the following guidelines provide a framework for environmental scientists seeking to leverage LLMs for statistical applications.

### Recognizing Different Steps in Workflows

A critical first step is separating statistical workflows into distinct components that align with LLM strengths and limitations:

1. **Statistical approach selection**: Determining appropriate statistical methods for research questions
2. **Implementation planning**: Designing the analytical workflow and code structure
3. **Code generation**: Writing the actual code to implement analyses
4. **Interpretation guidance**: Understanding and reporting results

LLMs perform differently across these components. They excel at code generation and implementation planning but are less reliable for selecting appropriate statistical approaches or interpreting complex results. This uneven performance profile suggests a workflow where:

- Researchers maintain primary responsibility for statistical approach selection, potentially using LLMs to explore options but validating choices against literature and expert knowledge
- LLMs take a more central role in implementation planning and code generation, with researchers providing clear constraints on the workflow
- Interpretation remains a collaborative process where LLMs can suggest standard interpretations but researchers critically evaluate these suggestions

This separation of responsibilities helps prevent overreliance on LLMs for decisions that require deeper statistical understanding while leveraging their strengths in code generation and documentation.

### Statistical Advice

When seeking statistical guidance from LLMs, several prompting strategies can significantly improve response quality.Whereas, we have found that many other prompting strategies commonly spruiked by tech commenators are less effective for elicting good  statistical advice. 

To elicit better statistical advice the most effective approaches are to be clear about your hypotheses, attach domain knowledge, include context about the data and data collection process. Below we provide several illustrative examples. 

**Example 1: Be clear and provide domain knowledge** 

This example is written from the point of view of a researcher who knows their hypothesis and data, but is unclear about what methods they should use: 

```
How can I statistically test the dependence of fish abundance on coral cover? 
Abundances are count data representing, and the cover variable is proportional data. Use @websearch to find 
robust recommendations for ecologists to analyze count data before proceeding 
with your recommendations.
```

In this prompt we made use of several strategies. First we were clear about the hypothesis (we think pres.topa should depend on CB_cover, not the other way around). We specificed the variable names as they appear in the data (e.g. `pres.topa`), this mean the LLM is more likely to write accurate code. We gave context about the data, by telling the LLM we had count data the response is more likely to include count appropriate methods (e.g. Poisson GLM). Finally, we asked the LLM to use a web search to find domain knowledge in our field. 

**Example 2: Provide context on experimental/observational design** 

Extending the previous example we could also provide context about survey design: 

```
I need to analyze the dependence of fish abundance (integer counts, 
zero-inflated) on coral cover (continuous percentage). Sites are spatially 
clustered within regions. What statistical approaches would be appropriate?
```

In this prompt by including the context that sites are spatial replicates the LLM is more likely to recommend methods that account for spatial structure in the data. 

**Example 3: Attach data** 

```
How can I statistically test the depdence of `pres.topa` on `CB_cover`? 
Here are the first 6 rows of data:
[data table]
```

Here we have specifically named the variables as they appear in the dataset (e.g. `pres.topa`). We have also dragged and dropped the first 6 rows of the data into the prompt (this is possible with many assistants, e.g. github copilot). Often (but not always) the LLM will appropriately recognize the response as a count variable if the data is attached. 

**Example 3: Use Chain of Thought reasoning with reputable source attached**

```
Using Chain of Thought reasoning, what statistical approach would be most 
appropriate for analyzing the relationship between fish abundance (count data) 
and coral cover (continuous)? We have attached a guideline for analysis of count data in ecology: [attach reference]
```

Chain of Thought reasoning is a common and easy prompting strategy that can dramatically improve responses in many domains, particularly reasoning. This works because LLMs 'think' by writing (a trait they share with good researchers). However, it does not improve statistical advice on its own (@zhu2024large). If used on its own, CoT will cause the LLM to elaborate on the same incorrect suggestions. In this prompt we dragged and dropped a reference (e.g. a blog from a reputable source, or a vignette from a popular ecological modelling R package). Therefore, the LLM could reason accurately with its improved domain knowledge. 

**Example 4: Request self-evaluation and multiple options** 

```
Evaluate the robustness of each suggestion on a 1-10 scale and explain 
the strengths and limitations of each approach.
```

or 

```
What are three different statistical approaches I could use for this analysis? 
For each, explain the assumptions, advantages, and limitations.
```

These suggestions is related to CoT, it just promotes further thinking. Once again, best used in a chain of prompts when you have attached a reputable reference. Note that most LLM assistants have the imperitive to provide options baked in by the providers, so it is usually not neccessary to explictly ask for options, just ask to think about them. 


### Code Implementation Advice

UP TO HERE

For implementing statistical analyses in R, different prompting strategies apply:

1. **Create a detailed project README**: Document the project context, research questions, data structure, and analytical approach in a README.md file that can be attached to prompts:

```
Help me plan R code to implement this analysis based on the project context 
in the README.
```

See the example in Supplemental file 1. 

2. **Use a two-step approach**: First plan the analysis structure, then implement specific components:

```
First, outline the overall workflow for analyzing the relationship between 
fish abundance and coral cover using a negative binomial GLM. Then, we'll 
implement each step.
```

3. **Provide implementation constraints**: Specify packages, coding style, and other requirements:

```
Implement this analysis using the tidyverse ecosystem and INLA for Bayesian 
modeling. Follow tidyverse style guidelines and prioritize code readability.
```

4. **Request modular code**: Ask for code organized into logical functions and scripts:

```
Create modular scripts for this analysis with separate files for data
preparation, model fitting, diagnostics, and visualization.
```

Modular code organization significantly enhances reproducibility and maintainability of statistical analyses, particularly for complex environmental data. When prompting LLMs, explicitly request that code be structured into discrete, logical components with clear separation of concerns. This approach yields several benefits: it facilitates easier troubleshooting as errors can be isolated to specific modules; enables collaborative workflows where different team members can work on separate components; improves readability by breaking complex analyses into conceptually coherent units; and supports iterative development where individual components can be refined independently. For environmental analyses that often involve multiple stages—data cleaning, transformation, model fitting, diagnostics, and visualization—this modular structure is particularly valuable. LLMs excel at generating this type of organized code when explicitly prompted, but may default to monolithic scripts if not specifically directed. Request separate functions or files for each major analytical step, with clear documentation of inputs, outputs, and dependencies between components. This structure not only improves immediate code quality but also enhances long-term project sustainability and knowledge transfer.

5. **Include verification steps**: Request code that validates assumptions and checks model fit:

```
Include diagnostic checks for overdispersion, zero-inflation, and spatial 
autocorrelation in the model implementation.
```

6. **Iteratively refine**: Start with a basic implementation and progressively add complexity:

```
Let's start with a simple negative binomial GLM for fish abundance. Once 
that's working, we'll extend it to account for spatial clustering.
```

These strategies help ensure that LLM-generated code is not only syntactically correct but also statistically appropriate and well-structured. By providing clear constraints and expectations, researchers can guide LLMs toward implementations that follow best practices in both programming and statistical analysis.

## Discussion and Conclusion

Large language models represent both opportunity and challenge for statistical practice in environmental sciences. When used thoughtfully with effective prompting strategies, they can enhance analytical workflows, improve code quality, and potentially address longstanding issues in statistical implementation. However, uncritical reliance on LLMs risks perpetuating or even amplifying existing problems in statistical practice.

The prompting guidelines presented in this perspective provide a framework for leveraging LLMs while maintaining statistical rigor. By separating workflows into components that align with LLM strengths and limitations, providing appropriate context and constraints, and maintaining human oversight of critical decisions, researchers can harness these powerful tools while mitigating their risks.

Several key principles emerge from this analysis:

1. **Maintain critical thinking**: LLMs should complement rather than replace statistical expertise. Researchers must critically evaluate LLM suggestions against domain knowledge and statistical principles.

2. **Provide rich context**: The quality of LLM statistical guidance improves dramatically when provided with detailed information about research questions, data characteristics, and analytical constraints.

3. **Leverage strengths, compensate for weaknesses**: Use LLMs primarily for tasks where they excel (code generation, implementation planning) while maintaining human oversight of tasks requiring deeper statistical understanding (method selection, assumption checking, interpretation).

4. **Document LLM use**: Transparency about LLM use in research workflows is essential for reproducibility and evaluation. Publications should clearly describe how LLMs were used and what prompting strategies were employed.

5. **Develop LLM literacy**: As these tools become increasingly integrated into research workflows, developing "LLM literacy"—understanding how these models work, their limitations, and effective interaction strategies—becomes an essential skill for environmental scientists.

The rapid evolution of LLM capabilities suggests that their role in statistical workflows will only increase. Current models already show impressive performance in code generation and implementation planning, and future models may address some of the limitations identified in statistical reasoning. However, the fundamental nature of LLMs as prediction engines rather than reasoning systems means that human oversight will remain essential for ensuring statistical quality.

### Research Needs

Several critical research needs emerge from this analysis:

1. **Evaluations of LLM statistical performance**: Systematic assessments across diverse environmental data types and analytical challenges would help identify specific strengths and weaknesses.

2. **Development of specialized prompting strategies**: Domain-specific prompting templates and guidelines could improve consistency and quality of statistical implementations.

3. **Integration of LLMs with traditional statistical software**: Hybrid systems that combine LLM flexibility with the algorithmic reliability of traditional statistical packages could leverage the strengths of both approaches.

4. **Educational approaches for LLM-assisted statistics**: New pedagogical strategies are needed to develop statistical understanding in an era where code implementation is increasingly automated.

5. **Ethical frameworks for LLM use in research**: Guidelines for appropriate attribution, transparency, and responsibility when using LLM-assisted analyses in published research.

By addressing these research needs and adopting thoughtful prompting strategies, environmental scientists can harness the power of large language models to enhance rather than compromise statistical quality. The future of environmental statistics likely lies not in choosing between human expertise and artificial intelligence, but in developing effective partnerships that leverage the unique strengths of each.

## Acknowledgements

## References


## Figures

```{r risk-framework, fig.width=5, fig.height=3, dpi=300, echo = FALSE}

library(DiagrammeR)

# Create a decision tree diagram for statistical risk mitigation
DiagrammeR::grViz("
digraph risk_framework {
  # Graph settings
  graph [rankdir = TD, fontname = 'Arial', nodesep = 0.8, ranksep = 0.5]
  node [shape = rectangle, fontname = 'Arial', fontsize = 12, style = 'filled', 
        fillcolor = 'white', width = 3, height = 0.8, margin = 0.2]
  edge [fontname = 'Arial', fontsize = 10]

  # Decision nodes
  task_complexity [label = 'Assess Task Complexity', fillcolor = '#e6f3ff', shape = diamond]
  error_consequence_low [label = 'Assess Consequence of Errors\n(Low Complexity)', fillcolor = '#e6f3ff', shape = diamond]
  error_consequence_high [label = 'Assess Consequence of Errors\n(High Complexity)', fillcolor = '#e6f3ff', shape = diamond]
  
  # Risk levels and mitigation strategies
  low_risk [label = 'Low Risk\n- Use standard prompts\n- Basic verification', fillcolor = '#ccffcc']
  medium_risk1 [label = 'Medium Risk\n- Provide rich context\n- Use Chain of Thought\n- Compare multiple approaches', fillcolor = '#ffffcc']
  medium_risk2 [label = 'Medium Risk\n- Expert review required\n- Include domain knowledge\n- Request self-evaluation', fillcolor = '#ffffcc']
  high_risk [label = 'High Risk\n- Use LLM for code only\n- Human decision-making\n- Peer review essential\n- Extensive validation', fillcolor = '#ffcccc']
  
  # Connections
  task_complexity -> error_consequence_low [label = 'Low\n(Descriptive stats,\nbasic plotting,\nsimple tests)']
  task_complexity -> error_consequence_high [label = 'High\n(Complex models,\nmultivariate analysis,\nspecialized methods)']
  
  error_consequence_low -> low_risk [label = 'Low\n(Exploratory,\nnon-critical)']
  error_consequence_low -> medium_risk1 [label = 'High\n(Publication,\ndecision-making)']
  
  error_consequence_high -> medium_risk2 [label = 'Low\n(Exploratory,\nnon-critical)']
  error_consequence_high -> high_risk [label = 'High\n(Publication,\ndecision-making)']
  
  # Additional guidance nodes
  subgraph cluster_guidance {
    label = 'General Risk Mitigation Guidelines'
    style = filled
    fillcolor = '#f0f0f0'
    node [shape = box, margin = 0.1, fontsize = 10, width = 2.5]
    
    guidance1 [label = 'Always document LLM use\nand prompting strategies']
    guidance2 [label = 'Validate against established\nstatistical literature']
    guidance3 [label = 'Consider reproducibility\nand transparency']
    
    guidance1 -> guidance2 -> guidance3 [style = invis]
  }
}
")
```

**Figure XXX**
Decision tree showing how to identify and mitigate risks in LLM statistical advice based on task complexity and consequence of errors.


```{r workflow-diagram, fig.width=5, fig.height=3, dpi=300, echo = FALSE}
library(DiagrammeR)

DiagrammeR::grViz("
digraph workflow {
  # Graph settings
  graph [rankdir = TD, fontname = 'Arial', nodesep = 0.8, ranksep = 0.8]
  node [shape = rectangle, fontname = 'Arial', fontsize = 12, style = 'filled', 
        fillcolor = 'white', margin = 0.2]
  edge [fontname = 'Arial', fontsize = 10]

  # Main workflow steps
  step1 [label = 'Step 1: Statistical Approach Selection', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step2 [label = 'Step 2: Implementation Planning', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step3 [label = 'Step 3: Code Generation', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  step4 [label = 'Step 4: Interpretation Guidance', fillcolor = '#e6f3ff', width = 3, height = 0.8]
  
  # Recommendations for each step
  rec1 [label = 'Recommendations:\\n• Explore multiple analytical approaches\\n• Provide domain knowledge and relevant literature\\n• Request step-by-step reasoning with Chain of Thought\\n• Compare statistical approaches with alternatives\\n• Prompt for cross-disciplinary perspectives\\n• Verify against statistical textbooks and guidelines', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec2 [label = 'Recommendations:\\n• Create a detailed README with analysis context\\n• Define clear research questions and hypotheses\\n• Specify data characteristics and structure\\n• Outline analytical constraints and assumptions\\n• Plan workflow before implementation\\n• Establish validation criteria in advance', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec3 [label = 'Recommendations:\\n• Use a two-step approach: plan then implement\\n• Provide implementation constraints (packages, style)\\n• Request modular, well-documented code\\n• Include verification and diagnostic steps\\n• Iteratively refine from simple to complex models\\n• Add comments explaining statistical decisions', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]
        
  rec4 [label = 'Recommendations:\\n• Never blindly trust LLM interpretations\\n• Request multiple visualization options\\n• Ask for limitations and assumptions of results\\n• Prompt for alternative interpretations\\n• Request guidance on reporting standards\\n• Maintain human oversight of conclusions', 
        shape = 'box', fillcolor = '#f0f8ff', width = 6]

  # Connections between steps
  step1 -> step2 -> step3 -> step4 [weight = 5]
  
  # Connect steps to their recommendations
  step1 -> rec1 [dir = none, style = dashed]
  step2 -> rec2 [dir = none, style = dashed]  
  step3 -> rec3 [dir = none, style = dashed]
  step4 -> rec4 [dir = none, style = dashed]
  
  # Human oversight element
  {rank = same; human [label = 'Human Oversight\\nand Critical Evaluation', 
                      shape = 'oval', fillcolor = '#ffffcc', 
                      style = 'filled,dashed', width = 3]}
                      
  human -> step1 [dir = both, style = dashed, constraint = false]
  human -> step2 [dir = both, style = dashed, constraint = false]
  human -> step3 [dir = both, style = dashed, constraint = false]
  human -> step4 [dir = both, style = dashed, constraint = false]
}
")
```

**Figure 2**
Recommended workflow for using LLMs in statistical analysis, showing the four key steps alongside specific recommendations for effectively leveraging LLMs at each stage while maintaining scientific rigor.


