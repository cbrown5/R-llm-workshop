# LLM prompting fundamentals

*Start of practical material*

We'll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you'll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you're not getting the results you expect.

**Software requirements:** VScode with R or Rstudio, [ellmer package](https://ellmer.tidyverse.org/), API license.

## Setup authorisation

First, you need to get an API key from the provider. Login to the provider's website and
follow the instructions. 

Then, you need to add the key to your `.Renviron` file: 

`usethis::edit_r_environ()`

Then type in  your key like this:

`OPENROUTER_API_KEY="xxxxxx"`

Then restart R. `ellmer` will automatically find your key so long as you use the recommended envirment variable names. 
See `?ellmer::chat_openrouter` (or `chat_xxx` where xxx is whatever provider you are using). 

## Understanding how LLMs work

Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we'll use the `ellmer` package to demonstrate some fundamental concepts.

First, let's set up our environment and create a connection to an LLM. 

```{r eval=TRUE, echo=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

```{r eval=FALSE}
library(ellmer)

# Initialize a chat with Claude
chat <- chat_openrouter(
  system_prompt = "",
  model = "anthropic/claude-3.5-haiku",
  api_args = list(max_tokens = 50)
)
chat$chat("Ecologists like to eat ")

```

Notice that the model doesn't do what we intend, which is complete the sentence. LLMs have a built in Let's use the 'system prompt' to provide it with strong directions. 

::: {.tip}
**Tip:** The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, [here's the system prompt for the chat interface version of anthropic (Claude)](https://www.reddit.com/r/ClaudeAI/comments/1ixapi4/here_is_claude_sonnet_37_full_system_prompt/)
:::

```{r setup-ellmer, eval=FALSE}
chat <- chat_openrouter(
  system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
  model = "anthropic/claude-3.5-haiku",
#   model = "anthropic/claude-3.7-sonnet",
  api_args = list(max_tokens = 50)
)
chat$chat("Ecologists like to eat ")

```

::: {.tip}
**Tip:** It is generally more effective to tell the LLM **what to do** rather than **what not to do** (just like people!). 
:::

### Token-by-token prediction

LLMs don't understand text as complete sentences or concepts; they predict one token at a time based on the patterns they've learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.

Let's see this in action by generating text token by token with a very limited number of tokens:

```{r token-prediction, eval=FALSE}

 prompt <- "Ecologists like to eat"

for(i in 1:5) {
        single_token_chat <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 1)
    )
    response <- single_token_chat$chat(prompt)
    prompt <- paste(prompt, response, sep = "")
    cat("Token", i, ":", response, "\n")
}

# Display the complete sequence
cat("Full sequence:", prompt, "\n")
```

This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context.

### Temperature effects

The "temperature" parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.

Let's compare responses with different temperatures:

```{r temperature-comparison, eval=FALSE}
# Create chats with different temperature settings
chat_temp <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 50, temperature = 0)
    )

chat_temp$chat("Marine ecologists like to eat ")

chat_temp <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 50, temperature = 2)
    )

chat_temp$chat("Marine ecologists like to eat ")

```

At low temperatures, you'll notice the model consistently produces similar "safe" completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.

### Comparing model complexity

Different models have different capabilities based on their size, training data, and architecture. 

For example `anthropic/claude-3.5-haiku` has many fewer parameters than `anthropic/claude-3.7-sonnet`. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15

For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot. 

### Understanding context windows

LLMs have a limited "context window" - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google's models have up to 1 million tokens. 

We'll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don't come close to using up the context window. 

## DIY stats bot 

Let's put together what we've learnt so far and built our own chatbot. I've provided you with a detailed system prompt that implements a chat bot that specialises in helping with statistics. First, we read the bot markdown file from github, then we can use it in our chat session. 

```{r, eval=FALSE}

stats_bot <- readr::read_file(url("https://raw.githubusercontent.com/cbrown5/R-llm-workshop/refs/heads/main/resources/DIY-stats-bot-system.md"))

chat_stats <- chat_openrouter(
          system_prompt = stats_bot,
        model = "anthropic/claude-3.7-sonnet",
        api_args = list(max_tokens = 5000)
    )

```

`ellmer` has a few different options for interacting with chat bots. We've seen the 'chat' option. We can also have a `live_console()` or `live_browser()` (requires installing `shinychat`) chat. Let's use one of those options. With `live_browser()` you'll also see the browser automatically formats any markdown in the chat. 

```{r, eval=FALSE}
live_browser(chat_stats)
# live_console(chat_stats)
```

Here are some suggested questions to start, but feel free to try your own. 
"Who are you?"
"Use stats mode to provide me with some suggestions for how I could make a predictive model of a variable y, where I have a large number of potential explanatory variables. "


::: {.tip}
**Tip:** How many of you started using my "DIY-stats-bot-system.md" without first reading it? Did you find the easter egg in my prompt? For security you should ALWAYS read prompts before you start running them through LLM chats. We'll see later that LLMs can be given 'tools' which allow them to run code on your computer. Its easy to see how a malicious prompt could mis-use these tools. We'll cover security later. 
:::

### Improving the stats bot

Make a local copy of the stats bot system prompt and try editing it. Try different commands within it and see how your chat bot responds (you'll have to open a new chat object each time). 

Here's some ideas. 

- Try making a chat bot that is a verhment Bayesian that abhors frequentist statistics.   
- You could provide it with more mode-specific instructions. For instance, try to get the chatbot to suggest appropriate figures for verifying statistical models. 
- Try different temperatures. 
- Add your own easter egg. 

::: {.tip}
**Tip:** Adjectives, CAPITALS, `*markdown*` formatting can all help create emphasis so that your model more closely follows your commands. I used 'abhors' and 'verhment' above on purpose. 
:::

## Advanced prompting


UP TO HERE, add one shot/ few shot examples

Then make a tool: Get it to output code that can be parsed (e.g. using markdown). 

Write an R function that parses and runs the code. 

Compare different models for accuracy at following the instructions. 

Show one shot and few shot examples.

