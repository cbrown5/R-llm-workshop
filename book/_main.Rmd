--- 
title: "Quality R analysis with large language models"
author: "CJ Brown (c.j.brown@utas.edu.au)"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
url: https://www.seascapemodels.org/R-llm-workshop/
description: |
  1-day workshop for using large language models to generate R code.
link-citations: yes
github-repo: cbrown5/R-llm-workshop
---

# Summary

If you are using R you are probably using language models (e.g. ChatGPT) to help you write code, but are you using them in the most effective way? Language models have different biases to humans and so make different types of errors. This 1-day workshop will cover how to use language models to learn R and conduct reliable environmental analyses. We will cover:  

- Pros and cons of different tools from the simple interfaces like ChatGPT to advanced tools that can run and test code by themselves and keep going until the analysis is complete (and even written up).  
- Best practice prompting techniques that can dramatically improve model performance for complex statistical applications  
- Applying language models to common environmental applications such as GLMs, multivariate statistics and Bayesian statistics
- Copyright and ethical issues  

We'll finish up with a discussion of what large language models mean for analysis and the scientific process.

Requirements for interactive workshop: Laptop with R, Rstudio and VScode installed. Please see software instructions below. 


#### Who should take this workshop?
The workshop is for: anyone who currently uses R, from intermittent users to 
experienced professionals. The workshop is not suitable for those that need an introduction to R and I'll assume students know at least what R does and are able to do tasks like read in data and create plots.


## About Chris 

I'm an Associate Professor of Fisheries Science at University of Tasmania and an Australian Research Council Future Fellow. I specialise in data analysis and modelling, skills I use to better inform environmental decision makers. R takes me many places and I've worked with marine ecosystems from tuna fisheries to mangrove forests. I'm an experienced teacher of R. I have taught R to 100s people over the years, from the basics to sophisticated modelling and for everyone from undergraduates to my own supervisors.

## Course outline

**Introduction to LLMs for R** 

9-10am
In this presentation I'll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics. 

**Part 1 LLM fundamentals** 

10-10:30am 

**Tea break: 10:30-10:45**

**Continue Part 1 LLM fundamentals** 


**Ethics and copyright**
11:30-12:00 

**Lunch break: 12:00-1:00pm**

**Part 2 Github copilot for R**

1:00pm-2:45pm

**Tea break 2:45-3:00pm**

**Part 3 Advanced LLM agents** 

3:00-3:30pm

**Conclusion and discussion of what it means for science**

3:30pm-4:00pm

## Software you'll need for this workshop

Save some time for set-up, there is a bit to it. You may also need ITs help if your computer is locked down. 

**R** of course, (4.2.2 or later)

**VScode** 
It takes a bit of time and IT knowledge and setting up to connect VScode to R. So don't leave that to the last minute [See my installation instructions](https://www.seascapemodels.org/rstats/2025/02/07/setting-up-vscode-r-cline.html)

Once you have VScode, make sure you get these extensions (which can be found by clicking the four squares in the left hand menu bar): 

**GitHub Copilot, GitHub Copilot Chat**

If you can't get VScode to work with R you are still welcome to join. Most of the morning session can be done in Rstudio. In the afternoon you'll need VScode if you want to try what I am teaching. 

### Optional software 

**RStudio**. You can do some of this workshop in Rstudio. But you'll get more out of it if you use VScode. 
If you are using Rstudio, make sure you get a Github Copilot account and connect it to Rstudio. 

#### Optional VScode extensions

**Web Search for Copilot** (once installed, follow the instructions to set up your API key, I use Tavily because it has a free tier). 

**Roo code** extension for VScode. 

**Markdown preview enhanced** Let's  you view markdown files in a pane with cmd(cntrl)-shift-V

**Radian terminal** I also recommend installing [radian terminal](https://github.com/randy3k/radian). This makes your terminal for R much cleaner, has autocomplete and seems to help with some common issues. 

## Software licenses

**Github copilot** Go to their page to [sign-up](https://github.com/features/copilot). The free tier is fine. You can also get free Pro access as a student or professor (requires application). 

**API key and account with LLM provider** 

You will need API (application programming interface) access to an LLM to do all the examples in this workshop. This will allow us to interact with LLMs directly via R code. API access is on a pay per token basis. You will need to create an account with one of the below providers and then buy some credits (USD10 should be sufficient). 

Here are some popular choices: 

- [OpenRouter](https://openrouter.ai/sign-up) (recommended as gives you flexible access to lots of models)
- [Anthropic](https://console.anthropic.com/login?returnTo=%2F%3F)
- [OpenAI](https://platform.openai.com/api-keys)

Once you have your API key, keep it secret. It's a password. Be careful not to push it to aa github repo accidently. 

## R packages you'll need for this workshop

`install.packages(c("vegan", "ellmer","tidyverse")`

[INLA for Bayesian computation](https://www.r-inla.org/download-install). Use the link, its not on cran. 

## Data 

Not yet complete...

We'll work with some benthic cover data, direct links to csv files are here: 

text files for 2 papers. 
time-series data? 



<!--chapter:end:index.md-->

# Introduction to LLMs for R

**Time:** 9-10am

In this presentation I'll cover how LLMs work, best practices prompt engineering, software, applications for R users and ethics.

This chapter provides an overview of:

- How Large Language Models (LLMs) function and their capabilities
- Best practices for prompt engineering when working with R
- Software options available for R users to interact with LLMs
- Practical applications of LLMs for R programming and data analysis
- Ethical considerations when using LLMs for scientific work

We'll explore how LLMs can enhance your R workflow, from code generation to data analysis assistance, while maintaining scientific rigor and reproducibility.

<!--chapter:end:01-introduction.Rmd-->

# Part 1: LLM prompting fundamentals

*Start of practical material*

We'll cover LLM prompting theory through practical examples. This section will give you a deep understanding of how chatbots work and teach you some advanced prompting skills. By understanding these technical foundations, you'll be better equipped to leverage LLMs effectively for your R programming and data analysis tasks, and to troubleshoot when you're not getting the results you expect.

**Software requirements:** VScode with R or Rstudio, [ellmer package](https://ellmer.tidyverse.org/), API license.

## Setup authorisation

First, you need to get an API key from the provider. Login to the provider's website and
follow the instructions. 

Then, you need to add the key to your `.Renviron` file: 

`usethis::edit_r_environ()`

Then type in  your key like this:

`OPENROUTER_API_KEY="xxxxxx"`

Then restart R. `ellmer` will automatically find your key so long as you use the recommended envirment variable names. 
See `?ellmer::chat_openrouter` (or `chat_xxx` where xxx is whatever provider you are using). 

## Understanding how LLMs work

Large Language Models (LLMs) operate by predicting the next token in a sequence, one token at a time. To understand how this works in practice, we'll use the `ellmer` package to demonstrate some fundamental concepts.

First, let's set up our environment and create a connection to an LLM. 

```{r setup-ellmer, eval=FALSE}
library(ellmer)

# Initialize a chat with Claude
chat <- chat_openrouter(
  system_prompt = "",
  model = "anthropic/claude-3.5-haiku",
  api_args = list(max_tokens = 50)
)
chat$chat("Ecologists like to eat ")

```

Notice that the model doesn't do what we intend, which is complete the sentence. LLMs have a built in Let's use the 'system prompt' to provide it with strong directions. 

::: {.tip}
**Tip:** The system prompt sets the overall context for a chat. It is meant to be a stronger directive than the user prompt. In most chat interfaces (e.g. copilot) you are interacting with the user prompt. The provider has provided the system prompt, [here's the system prompt for the chat interface version of anthropic (Claude)](https://www.reddit.com/r/ClaudeAI/comments/1ixapi4/here_is_claude_sonnet_37_full_system_prompt/)
:::

```{r setup-ellmer, eval=FALSE}
chat <- chat_openrouter(
  system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
  model = "anthropic/claude-3.5-haiku",
#   model = "anthropic/claude-3.7-sonnet",
  api_args = list(max_tokens = 50)
)
chat$chat("Ecologists like to eat ")

```

::: {.tip}
**Tip:** It is generally more effective to tell the LLM **what to do** rather than **what not to do** (just like people!). 
:::

### Token-by-token prediction

LLMs don't understand text as complete sentences or concepts; they predict one token at a time based on the patterns they've learned during training. A token is roughly equivalent to a word part, a word, or a common phrase.

Let's see this in action by generating text token by token with a very limited number of tokens:

```{r token-prediction, eval=FALSE}

 prompt <- "Ecologists like to eat"

for(i in 1:5) {
        single_token_chat <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 1)
    )
    response <- single_token_chat$chat(prompt)
    prompt <- paste(prompt, response, sep = "")
    cat("Token", i, ":", response, "\n")
}

# Display the complete sequence
cat("Full sequence:", prompt, "\n")
```

This shows how the model builds its response incrementally (actually its not quite how streams of predictions are implemented, but it gives you the right idea). Each token is predicted based on the probability distribution of possible next tokens given the context.

### Temperature effects

The "temperature" parameter controls the randomness of token predictions. Lower temperatures (closer to 0) make the model more deterministic, while higher temperatures (closer to 2) make it more creative and unpredictable.

Let's compare responses with different temperatures:

```{r temperature-comparison, eval=FALSE}
# Create chats with different temperature settings
chat_temp <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 50, temperature = 0)
    )

chat_temp$chat("Marine ecologists like to eat ")

chat_temp <- chat_openrouter(
          system_prompt = "Complete the sentences the user providers you. Continue from where the user left off. Provide one answer only. Don't provide any explanation, don't reiterate the text the user provides",
        model = "anthropic/claude-3.5-haiku",
        api_args = list(max_tokens = 50, temperature = 2)
    )

chat_temp$chat("Marine ecologists like to eat ")

```

At low temperatures, you'll notice the model consistently produces similar "safe" completions that focus on the most probable next tokens. As temperature increases, the responses become more varied and potentially more creative, but possibly less coherent.

### Comparing model complexity

Different models have different capabilities based on their size, training data, and architecture. 

For example `anthropic/claude-3.5-haiku` has many fewer parameters than `anthropic/claude-3.7-sonnet`. This means that the latter model is more complex and can handle more nuanced tasks. However, haiku is significantly cheaper to run. Haiku is 80c per million input tokens vs $3 for Sonnet. Output tokens are $4 vs $15

For the kind of simple tasks we are doing here, both give similar results. We will compare models later in the workshop when we use github copilot. 

### Understanding context windows

LLMs have a limited "context window" - the amount of text they can consider when generating a response. This affects their ability to maintain coherence over long conversations. For most LLMs this is about 100-200K tokens, which includes input and output. However, Google's models have up to 1 million tokens. 

We'll come back to the context window when we explore more advanced tools with longer prompts. These simple prompts don't come close to using up the context window. 

## DIY stats bot 

UP TO HERE

## Advanced prompting

The point of this section is to show you how github copilot works. Also ellmer has a tools functionality that you can explore on your own. 

Keep working with our stats bot. Make a tool: Get it to output code that can be parsed (e.g. using markdown). 
Write an R function that parses and runs the code. 

Compare different models for accuracy at following the instructions. 

Show one shot and few shot examples.


<!--chapter:end:02-llm-prompting-fundamentals.Rmd-->

# Ethics and copyright

This chapter addresses important ethical and legal considerations when using LLMs for R programming and data analysis:


Managing data privacy (get it to write code for analysis but not show it the data, keeping data in a different directory). 


## Model biases

- Understanding inherent biases in LLMs
- Recognizing when biases might affect statistical analysis
- Techniques for mitigating bias in LLM-generated code and analysis

## Energy use

- Environmental impact of LLM usage
- Balancing computational efficiency with analytical needs
- Strategies for reducing the carbon footprint of LLM-powered workflows

## Copyright

- Legal considerations when using LLM-generated code
- Understanding the licensing implications of LLM outputs
- Best practices for attribution and documentation

## Managing data privacy

- Strategies for using LLMs without exposing sensitive data
- Techniques for getting LLMs to write code for analysis without showing them the data
- Keeping data in separate directories and other practical approaches
- Considerations for research involving human subjects or proprietary information

This discussion will help you develop responsible practices for incorporating LLMs into your scientific workflow while respecting ethical, legal, and privacy considerations.

<!--chapter:end:03-ethics-copyright.Rmd-->

# Part 2: Github copilot for R

**Time:** 11:30-12:00pm

I'll show you how you can most effectively use github copilot to plan, code and write up your data analysis and modelling.

**Software requirements:** VScode with R and github copilot license + extension for copilot.

This chapter introduces GitHub Copilot as a powerful tool for R programmers:

- Overview of GitHub Copilot and how it differs from other LLM tools
- Setting up and configuring Copilot for optimal R programming assistance
- Understanding Copilot's strengths and limitations for data analysis tasks
- Strategies for effectively collaborating with Copilot on R projects

We'll explore how Copilot can accelerate your R development process by:
- Suggesting code completions based on context
- Generating entire functions and code blocks
- Helping with documentation and commenting
- Assisting with debugging and error resolution

Through practical demonstrations, you'll learn how to leverage Copilot's capabilities while maintaining control over your code's quality and correctness, particularly for statistical analysis and data science applications.

<!--chapter:end:04-github-copilot.Rmd-->

# Best practices project setup

How to be organized to maximise LLM effectiveness. Version control.

This chapter focuses on establishing an optimal project structure and workflow to get the most out of LLMs in your R projects:

## Project organization

- Directory structures that facilitate LLM assistance
- Documentation practices that improve LLM understanding of your code
- Naming conventions and code organization principles
- Creating effective README files and project documentation

## Version control with Git

- Basic Git workflow for R projects
- How version control enhances collaboration with LLMs
- Using Git to track changes made with LLM assistance
- Best practices for committing LLM-generated code

## R project setup

- Using RStudio projects or VSCode workspaces
- Setting up reproducible environments with renv or packrat
- Creating and maintaining package dependency documentation
- Organizing data, code, and output directories

By implementing these best practices, you'll create a development environment where LLMs can provide more accurate and contextually relevant assistance, leading to more efficient and reliable R programming workflows.

<!--chapter:end:05-best-practices-setup.Rmd-->

# Inline code editing

This chapter explores techniques for using GitHub Copilot's inline code editing capabilities to enhance your R programming workflow:

## 1. Code completion

This is only option supported in Rstudio (last time I checked). 

Example: 

Show how this works. How to get it moving on when its stuck in a loop. 
Show how to use comments to get it to work more effectively 

## 2. Inline code generation

cmd/cntrl-i . Show how to use this 
examples: 
selecting code to fix 
selecting code to explain 
Use of /fix, /document etc... as shortcuts to get it to do things.

Note options for microphone, @ (shortcuts) and model choices


## Understanding inline suggestions

- How Copilot generates inline code suggestions
- Interpreting and evaluating Copilot's suggestions
- Accepting, modifying, or rejecting suggestions effectively
- Keyboard shortcuts and efficiency tips

## Guiding Copilot with comments

- Writing effective comments to steer Copilot's suggestions
- Using natural language to describe your coding intentions
- Documenting complex statistical procedures for better suggestions
- Balancing detail and brevity in comments

## Common patterns for R programming

- Data manipulation with tidyverse
- Statistical modeling and analysis
- Data visualization with ggplot2
- Working with specialized R packages

## Troubleshooting and refinement

- Strategies when Copilot provides incorrect or suboptimal suggestions
- Iterative refinement of code with Copilot's assistance
- Handling R-specific syntax and idioms
- Adapting to Copilot's learning curve

Through practical examples and hands-on exercises, you'll learn to seamlessly integrate Copilot's inline suggestions into your R coding process, significantly accelerating your development while maintaining code quality.

<!--chapter:end:06-inline-code-editing.Rmd-->

# Planning your project with Ask mode

This chapter explores how to effectively use GitHub Copilot's Ask mode to plan and structure your R projects:

Show how to use this. 
Chain of thought prompting. 
Attaching references or code files 
Attaching data files 

## Introduction to Ask mode

- What is Ask mode and how it differs from inline suggestions
- Accessing and using Ask mode in VSCode
- Understanding the capabilities and limitations of Ask mode
- When to use Ask mode vs. other Copilot features

## Project planning with Ask mode

- Breaking down complex data analysis tasks
- Creating project roadmaps and development plans
- Identifying necessary packages and dependencies
- Structuring your analysis workflow

## Asking effective questions

- Formulating clear and specific questions
- Providing sufficient context for accurate responses
- Iterative questioning strategies
- Domain-specific questions for statistical analysis

## Implementing Ask mode suggestions

- Translating high-level advice into concrete code
- Evaluating and adapting suggested approaches
- Combining Ask mode with inline coding
- Documentation based on Ask mode explanations

Through practical examples, you'll learn how to leverage Ask mode as a planning and problem-solving tool that can help you approach complex R programming tasks with greater clarity and structure.

<!--chapter:end:07-ask-mode.Rmd-->

# Creating your code with Edit mode

This chapter explores how to leverage GitHub Copilot's Edit mode to efficiently create and modify R code:

## Understanding Edit mode

- What is Edit mode and how it differs from other Copilot features
- Accessing and using Edit mode in VSCode
- Edit mode's capabilities for code generation and transformation
- When to use Edit mode in your R workflow

## Code generation strategies

- Creating functions and code blocks from natural language descriptions
- Generating data manipulation pipelines
- Building statistical models and analysis scripts
- Creating data visualization code

## Code transformation techniques

- Refactoring existing R code for better performance or readability
- Converting between base R and tidyverse approaches
- Translating code between different statistical frameworks
- Adapting code examples to your specific needs

## Quality control and refinement

- Evaluating and testing generated code
- Iterative improvement through Edit mode
- Handling edge cases and error conditions
- Ensuring code readability and maintainability

Through hands-on examples, you'll learn to use Edit mode as a powerful tool for both creating new R code and transforming existing code, significantly accelerating your development process while maintaining high-quality, reliable code.

<!--chapter:end:08-edit-mode.Rmd-->

# Automated workflows with Agent mode

This chapter explores how to use GitHub Copilot's Agent mode to create automated workflows for R programming and data analysis:

## Introduction to Agent mode

- What is Agent mode and how it differs from other Copilot features
- Accessing and using Agent mode in VSCode
- Understanding the capabilities and limitations of Agent mode
- When to use Agent mode in your R workflow

## Creating automated workflows

- Defining tasks for Agent mode to accomplish
- Breaking down complex analyses into manageable steps
- Setting up appropriate contexts and environments
- Monitoring and guiding Agent mode's progress

## Common R workflow applications

- Data cleaning and preprocessing
- Exploratory data analysis
- Statistical modeling and hypothesis testing
- Report generation and visualization

## Troubleshooting and optimization

- Handling errors and unexpected results
- Providing additional context when needed
- Refining workflow definitions for better results
- Balancing automation with human oversight

Through practical demonstrations, you'll learn how to leverage Agent mode to automate repetitive or complex R programming tasks, allowing you to focus on higher-level analysis and interpretation while maintaining control over the quality and correctness of your code.

<!--chapter:end:09-agent-mode.Rmd-->

# Part 3: Advanced LLM agents

**Time:** 3:00-3:30pm



**Software requirements:** VScode with R, Roo code, API license.



This chapter introduces advanced LLM agents that go beyond GitHub Copilot's capabilities:


## Roo code

Fully automated workflows.

This chapter explores Roo code as a powerful tool for creating fully automated workflows in R:


Show roo code example. 
Talk through token usage, system prompt, editing prompts and roles, model choice, context window 

Image tool 


## Introduction to Roo code

- What is Roo code and how it differs from other LLM tools
- Setting up and configuring Roo code in VSCode
- Understanding Roo's capabilities and limitations
- The architecture behind Roo's automation capabilities

## Autonomous R programming with Roo

- How Roo interprets task descriptions and requirements
- Roo's approach to generating, testing, and refining code
- Monitoring and interacting with Roo during code generation
- Evaluating and validating Roo-generated solutions

## Advanced use cases for R

- End-to-end data analysis pipelines
- Complex statistical modeling and inference
- Custom visualization development
- Package development and documentation

## Best practices and workflow integration

- Providing effective task descriptions
- Balancing autonomy with guidance
- Incorporating Roo into existing development workflows
- Version control and collaboration with Roo-generated code

Through practical demonstrations, you'll learn how to leverage Roo code to automate sophisticated R programming tasks, potentially saving hours of development time while maintaining high-quality, reliable code.

## Evolution of LLM agents

- How LLM agents differ from simple code completion tools
- The spectrum of automation in LLM-assisted programming
- Understanding agent architectures and capabilities
- Current state-of-the-art in LLM agents for R programming

## Agent capabilities for R

- Code generation and modification
- Reasoning about complex statistical problems
- Autonomous debugging and error correction
- Documentation and explanation generation

## Integration with R workflows

- Connecting agents to your development environment
- API-based interactions with LLM agents
- Batch processing vs. interactive assistance
- Combining multiple agents for complex tasks

## Practical considerations

- Computational requirements and performance
- Cost structures and optimization strategies
- Privacy and security implications
- Evaluating agent output quality

This chapter provides a foundation for understanding more sophisticated LLM agents, preparing you for the next chapter's focus on Roo code as a specific implementation of advanced agent technology for R programming.

<!--chapter:end:10-advanced-llm-agents.Rmd-->

# Cost and security

This chapter addresses important practical considerations when using LLMs for R programming:

## Cost considerations

- Understanding the pricing models of different LLM providers
- API costs vs. subscription models
- Strategies for optimizing token usage
- Balancing cost with capability requirements
- Budgeting for LLM usage in research and professional contexts

## Security implications

- Data privacy concerns when using LLMs
- Understanding what data is sent to LLM providers
- Risks of exposing sensitive or proprietary information
- Compliance considerations for different industries and research contexts

## Best practices for secure usage

- Managing API keys and credentials
- Sanitizing inputs to remove sensitive information
- Local vs. cloud-based LLM solutions
- Auditing and monitoring LLM interactions

## Institutional considerations

- Developing organizational policies for LLM usage
- Training researchers and staff on secure practices
- Documentation and transparency requirements
- Balancing innovation with risk management

This chapter provides practical guidance for managing the financial and security aspects of incorporating LLMs into your R workflow, helping you make informed decisions about when and how to use these powerful tools.

<!--chapter:end:11-cost-security.Rmd-->

# Conclusion

**Time:** 3:30pm-4:00pm

We'll discuss as a group what LLMs mean for the way we do science, and creating community standards.

This chapter synthesizes the key insights from the workshop and explores the broader implications of LLMs for scientific practice:

## Workshop summary

- Recap of key concepts and techniques covered
- Integration of different LLM tools and approaches
- Progression from basic prompting to advanced autonomous agents
- Practical applications for R programming and data analysis

## The changing landscape of scientific computing

- How LLMs are transforming research workflows
- Potential impacts on reproducibility and transparency
- Changes in skill requirements and education
- Democratization of advanced programming capabilities

## Developing community standards

- Ethical considerations for LLM use in scientific research
- Documentation and reporting practices
- Peer review in the age of LLM-assisted research
- Balancing innovation with methodological rigor

## Future directions

- Emerging trends in LLM technology
- Potential developments in R-specific LLM tools
- Opportunities for community contribution and development
- Preparing for the next generation of AI-assisted data science

This concluding discussion encourages critical reflection on how we can harness the power of LLMs while maintaining the integrity and quality of scientific research and analysis.

<!--chapter:end:12-conclusion.Rmd-->

