# Planning your analysis with Ask mode

Ask mode helps you plan code and analysis, using context from your project. 

In VScode click the 'octocat' symbol that should be at the top towards the right. This will open the chat window. 

The chat panel will appear down the bottom of this new sidebar. Confirm that the chatbot is currently set to 'Ask' mode. 

Your current file will automatically be included as context for the prompt. You can drag and drop any other files here as well. 

Start by asking the chatbot for guidance on a statistical analysis. We are interested in how the abundance of Topa relates to coral cover. For instance you could ask:

```
How can I test the relationship between pres.topa and CB_cover?
```

Evaluate the quality of its response and we will discuss. 

## The jagged frontier of LLM progress

LLMs were created to write text. But it soon became apparent that they excel at writing programming code in many different languages. 

Since then AI companies have been optimising their training and development for coding and logic. 

There are a series of standardized tests that are used to compare quality of LLMs. Common evaluation tests are the SWE benchmark which TODO add here, also add other evaluations, AINSE maths test... 

These test different abilities. LLMs started to get so good at maths that they had to develop a new test. 

This test was a set of 100 maths problems that have never been solved. XXX was getting XXX%, current models now score XXXX%

However, evaluations of LLMs on their ability to identify the correct statistical procedure are less impressive. An evaluation (published 2025) of several models, including GPT-4 as the most up-to-date model, found accuracy at suggesting the correct statistical test of between 8% and 90%. 

In general LLMs were good at choosing descriptive statistics (accuracy of up to 90% for GPT-4). Whereas when choosing inferential tests accuracy was much less impressive - GPT-4 scored between 20% and 43% accuracy on questions for which a contingency table was the correct answer. 

The results also indicate the improvements that can be gained through better prompts (i.e. doubling in accuracy for GPT 4). 

The lesson is two-fold. Just because LLMs excel at some tasks doesn't mean they will excel at others. Second, good prompting strategies pay off. 

## How to prompt for better statistical advice

The limited number of evaluations of LLMs for statistics have found the biggest improvements for prompts that:

- Include domain knowledge in the prompt
- Include data or summary data in the prompt
- Combine domain knowledge with CoT (but CoT on its own doesn't help)

In addition, larger and more up-to-date models tend to be better. e.g. try Claude 4.0 over GPT-mini.  

### Guidelines for prompting for statistical advice

**Attach domain knowledge** Try to find quality written advice from recognized researchers to include in your prompts. 

**Always provide context on the data** For instance, the model will give better advice for the prompt above if we tell it that `pres.topa` is integer counts (it will probably then recommend poisson GLM straight away). Likewise, if your replicates are different sites, tell that to the model so it has the opportunity to recommend approaches that are appropriate for spatial analysis. 

**Attach data to your prompts** You can attach the whole dataset if its in plain text (e.g. csv). Or write a `summary()` and/or `head()` to file and attach that. 

**Combine the above approaches with Chain of Thought** Just add 'use Chain of Thought reasoning' to your prompt. Its that easy. 

**Double-up on chain of thought with self evaluation** After the initial suggest try prompts like "are you sure?", "Take a deep breath, count to ten and think deeply", "Evaluate the quality of the options on a 1-5 scale". 

::: {.tip}
**Tip:** Make a library of reference material for your prompting. If you see vignettes, blogs, or supplemental sections of papers that explain an analysis well, save them as text files to use in prompts. 
:::


## Improving our initial prompt 

Recall our initial prompt was: 

```
How can I test the relationship between pres.topa and CB_cover?
```

Try some of the strategies above (make a new prompt by clicking the  + button) and compare the quality of advice. 

For instance, you can save a data summary like this: 

```{r}
write_csv(head(dat), "Shared/Data/site-level-data.csv")
```

Or try attaching this blog TODO add GLM blog... 

## Planning implementation 

The other main way to use Ask mode is for help in implementing an analysis. Many of our workflows are complex and involve multiple data wrangling steps. 

To get the best out of GC I recommend creating a detailed README.md file with project context. Let's try that and use it to plan our project. 

Save the [README.md that his here to a local file](). (Remember that we are using this as a prompt)
UP TO HERE